{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 오차 역전법\n",
    "\n",
    "    신경망의 가중치 매개변수의 기울기는 수치 미분을 사용해 구했다.\n",
    "    수치 미분은 단순하고 구현ㅅ하기도 쉽지만 계산 시간이 오래 걸린다는 게 단점이다.\n",
    "    이번장에서는 가중치 매개변수의 기울기를 효율적으로 계산하는 오차역전법을 배워보자\n",
    "\n",
    "\n",
    "    오차 역전파법을 이해하는 방법으로는 두가지가 존재한다.\n",
    "    첫번째는 수식을 통한 이해이고 다른 한가지는 계산 그래프를 통한 방법이다.\n",
    "    일반적으로는 수식을 통해 이해하지만 본 교재에서는 그래프를 통하여 이해하도록 하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-1. 계산 그래프\n",
    "    계산 그래프란 계산 과정을 그래프로 나타낸 것이다.\n",
    "    그리고 이는 복수의 노드(node)와 예자(edge)로 나타내어진다.\n",
    "\n",
    "5-1.1. 계산 그래프로 풀다\n",
    "    계산 과정을 노드와 화살표로 표현한다.\n",
    "    노드는 원으로 표기하고 원안에 연산 내용을 적는다.\n",
    "    각 노드의 계산 결과는 왼쪽에서 오른쪽으로 전해지게 된다.\n",
    "\n",
    "    계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행합니다.\n",
    "\n",
    "    1. 계산 그래프를 구성한다.\n",
    "    2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.\n",
    "\n",
    "    여기서 왼쪽에서 오른쪽으로 진행하는 단계를 순전파라고 한다.\n",
    "    당연히 오른쪽에서 왼쪽으로 전파가 가능할 것이다.\n",
    "    그리고 우리는 그것을 역전파라고 부른다.\n",
    "\n",
    "5-1.2. 국소적 계산\n",
    "\n",
    "    계산 그래프의 특징은 국소적 계산을 전파함으로써 최종 결과를 얻는다는 점에 있다.\n",
    "    국소적이란 ' 자신과 직접 관계된 작은 범위' 라는 뜻이다.\n",
    "\n",
    "    즉, 아무리 복잡한 계산임에도 불구하고 주변의 관계는 상관할 필요 없이\n",
    "    각 단계에서 수행하는 일만 수행하여 결과를 전달 함으로써 전체 를 구성하는 복잡한 계산을 해낼 수 있다는 것이다.\n",
    "\n",
    "5-1.3. 이점\n",
    "\n",
    "    이러한 계산 프래프는 3가지 이점을 준다.\n",
    "    1. 국소적 계산\n",
    "    2. 계산 중간 결과 보관\n",
    "    3. 역전파를 통해 미분을 효율적으로 계산 할 수 있다.\n",
    "    \n",
    "    계산 그래프의 역전파를 설명하기 위해 문제 1을 다시 꺼내보자.\n",
    "    문제 1은 사과를 2개 사서 소비세를 포함한 최종 금액을 구하는 것이었다.\n",
    "    여기서 만약 사과 가격이 오르면 최종금액에 어떤 영향을 미치는지 알고 싶다고 하자.\n",
    "    사과 가격에 대한 지불 금액의 미분을 구하는 문제에 해당한다.\n",
    "\n",
    "    그림 5-5같이 역전파는 순전파와는 반대 방향으로 화살표를 그립니다. 이전파는 국소적 미분을 전달하고 그 미분 값은 화살표 아래에 적습니다.\n",
    "    이 예에서 역전파는 오른쪽에서 왼쪽으로 '1 ->1.1 ->2.2' 순으로 미분값을 전달한다.\n",
    "    이 결과로부터 사과 가격에 대한 지불 금액의 미분값은 2.2라고 할수 있다.\n",
    "    즉 사과 가격이 1원 오르면 최종 금액은 2.2원이 오른 다른 뜻이다.\n",
    "    (정확히는 사과 값이 아주 조금 오르면 최종 금액은 그 아주 작은 값의 2.2배만큼 오른 다는 뜻이다.)\n",
    "\n",
    "\n",
    "5-2. 연쇄 법칙\n",
    "    이번 장에서는 연쇄법칙을 설명하고 그것이 계산 그래프 상의 역전파와 같다는 사시릉ㄹ 밝힐 것이다.\n",
    "\n",
    "    그림 5-6을 통하여 y = f(x)라는 계산의 역전파를 확인해보자.\n",
    "\n",
    "    역전파의 계산 절차는 신호 E에 노드의 국소적 미분 dy/dx를 곱한 후에 노드로 전달 하는 것이다.\n",
    "    여기서 말하는 국소적 미분이란 순전파때의 y = f(x) 계산의 미분을 구한다는 것이다.\n",
    "\n",
    "    결국 핵심은 이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이다.\n",
    "    왜 그런 일이 가능한가는 연쇄법칙의 원리로 설명 할 수 있다.\n",
    "\n",
    "5-2.1. 연쇄법칙이란?\n",
    "    먼저 합성 함수 이야기부터 시작해야 한다.\n",
    "    합성 함수란 여러 함수로 구서오딘 함수이다.\n",
    "    예를 들어 z = (x+y)^2이라는 식은 식 5.1처럼 두 개의 식으로 구성된다\n",
    "\n",
    "    z = t^2\n",
    "    t = x + y\n",
    "\n",
    "    연쇄 법칙은 합성 함수의 미분에 대한 성질이다. 다음과 같이 정의된다.\n",
    "\n",
    "    ' 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다. '\n",
    "\n",
    "    위 식을 예시로 설명해보자.\n",
    "    x에대한 z의 미분은 t에 대한 z의 미분과 x에대한 t의 미분으로 나타낼 수 있다.\n",
    "    만약 우리가 dz/dx가 우하고 싶다고 하자.\n",
    "    그러면 우리는 dz/dt와 dt/dx를 구하면 된다\n",
    "\n",
    "    dz/dt는 2t가 되고 dt/dx는 1이 나온다.\n",
    "    그럼 둘을 곱하면 2t가 나오고 이것이 답이 된다.\n",
    "\n",
    "5-2.2. 연쇄 법칙과 계산 그래프\n",
    "\n",
    "    그림 5.4의 연쇄  법칙을 계산 그래프로 나타내어보자. 그림 5.7처럼 표현 될것이다.\n",
    "\n",
    "    결국 전체적인 결과는 dz/dx가 되지만 이러한 결과를 위한 각각의 역전파가 수행하는 일은\n",
    "    연쇄법칙의 원리와 동일하라는 것을 알 수 있다.\n",
    "\n",
    "\n",
    "5-3. 역전파\n",
    "    앞 절에서는 계산 그래프의 역전파가 연쇄법칙에 따라 진행되는 모습을 설명했다. \n",
    "    이번절 에서는 + x등의 연산을 예로 들어 역전파의 구조를 설명한다.\n",
    "\n",
    "5-3.1. 덧셈 노드의 역전파\n",
    "    먼저 덧셈 노드의 역전파이다. 여기에서는 z = x+y라는 식을 대상으로 역전파를 살펴보자.\n",
    "    일단 다음과 같이 해석이 가능하다.\n",
    "    dz/dx = 1\n",
    "    dz/dy = 1\n",
    "\n",
    "    식 5.5에서와 같이 둘 모두 1이 된다. 이는 계산 그래프 5-9에서 확인 할 수 있다.\n",
    "    즉 덧셈노드는 결국 1을 곱하기만 할 뿐이므로 입력된 값을 그대로 다음 노드로 보내게 된다.\n",
    "\n",
    "5-3.2. 곱셈 노드의 역전파\n",
    "\n",
    "    이어서 곱셈 노드의 역전파를 이해해보자.\n",
    "    z= xy라는 식을 생각 해보자.\n",
    "    이식의 미분은 다음과 같을 것이다.\n",
    "\n",
    "    dz/dx = y\n",
    "    dz/dy = x\n",
    "\n",
    "    곰셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보낸다.\n",
    "    서로 바꾼 값이란 그림 5-12 처럼 순전파 때 x 였던 역전파에서는 y, 순전파 때 y 였다면 역전파에서는 x가 되는 것이다.\n",
    "    구체적인 예를 한가지 보자.\n",
    "    10 x 5 = 50이란 계산이 있다.\n",
    "    역전파 때 상류에서 1.3이 흘러들어온다면\n",
    "    10으로는 1.3 *5 = 6.5의 값이\n",
    "    5으로는  1.3.10 = 13의 값이 올라가게 되는 것이다.\n",
    "\n",
    "    덧셈의 역전파에서는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값은 필요하지 않지만,\n",
    "    곱셈의 역전파는 순방향 입력 신호의 값이 필요하다. 그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를\n",
    "    변수에 저장해두곤 한다.\n",
    "\n",
    "5-3.3. 사과 쇼핑의 예\n",
    "\n",
    "사과 쇼핑의 예를 다시 살펴보자.\n",
    "이 문제에서는 사과의 가격, 사과의 개수, 소비세 라는 세변수 각각이 최종 결과에 어떻게 영향을 주냐를 풀고자 한다.\n",
    "이는 '사과 가격에 대한 지불금액의 미분'\n",
    "    '사과 개수에 대한 지불 금액의 미분'\n",
    "    '소비세에 대한 지불 금액의 미분' 으로 구하는것에 해당한다.\n",
    "\n",
    "이를 계산 그래프의 역전파를 활용하여 풀면 그림 5-14와 같이 진행된다.\n",
    "\n",
    "마지막으로 사과와 귤 쇼핑의 역전파를 풀어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4. 단순한 계층 구현하기\n",
    "    이제 지금까지 보아온 사과 쇼핑 예를 파이썬으로 구현합니다.\n",
    "    여기에서는 계산 그래프의 \n",
    "    곱셈 노드를 'MulLayter'\n",
    "    덧셈 노드를 'AddLayer'라는 이름으로 구현한다.\n",
    "\n",
    "5.4.1. 곱셈 계층\n",
    "    모든 계층은 forward()은 순전파, backward()은 역전파를 처리합니다.\n",
    "    그럼 먼저 곱셈 계층을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx,dy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__init__()에서는 인스턴스 변수인 x와 y를 초기화한다.\n",
    "이 두 변ㄴ수는 순전파 시의 입력값을 유지하기 위해서 사용한다.\n",
    "forward()에서는 x와 y를 인수로 받고 두 값을 곱해서 반환한다.\n",
    "반면 backward()에서는 상류에서 넘어온 미분에 순전파 때의 값을 서로 바꿔 곱한 후 하류로 흘린다.\n",
    "이상이 MulLayterdlek.\n",
    "\n",
    "이를 이용하여 사과 쇼핑을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "#계층들\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "price = mul_tax_layer.forward(apple_price,tax)\n",
    "print(price)\n",
    "\n",
    "#역전파\n",
    "\n",
    "dprice = 1\n",
    "dapple_price,dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple,dapple_num,dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-4.2. 덧셈 계층\n",
    "    이어서 덧셈 노드인 계층을 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        out = x+y\n",
    "        return out        \n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        \n",
    "        return dx,dy\n",
    "    \n",
    "apple= 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "#계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파\n",
    "price1 = mul_apple_layer.forward(apple_num,apple)\n",
    "price2 = mul_orange_layer.forward(orange,orange_num)\n",
    "price_sum = add_apple_orange_layer.forward(price1,price2)\n",
    "total = mul_tax_layer.forward(price_sum,tax)\n",
    "print(total)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#역전파\n",
    "dprice = 1\n",
    "dtotal,dtax = mul_tax_layer.backward(dprice)\n",
    "dprice1,dprice2 = add_apple_orange_layer.backward(dtotal)\n",
    "dapple_num,dapple = mul_apple_layer.backward(dprice1)\n",
    "dorange,dorange_num = mul_orange_layer.backward(dprice2)\n",
    "print(dapple_num,dapple,dorange,dorange_num,dtax)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-5.활성화 함수 계층 구현하기\n",
    "    이제 계산 그래프를 신경망에 적용해보자.\n",
    "    여기에서는 신경망을 구성하는 층 각각을 클래스 하나로 구현한다.\n",
    "    우선은 ReLU와 Sigmoid 계층을 구현해보자.\n",
    "\n",
    "5-5.1. ReLU 계층\n",
    "    활성화 함수로 사용되는 ReLU의 수식은 다음과 같다.식 5.7\n",
    "    x에 대한 y의 미분은 5.8처럼 구한다.\n",
    "\n",
    "    순전파 때 \n",
    "    x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다.\n",
    "    x가 0보다 작거나 같으면 신호가 흐르지 않는다(0을 보낸다)\n",
    "    \n",
    "    구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0          # false를 충족하는 원소는 0으로 바꾸어버리라는 의미\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout \n",
    "        \n",
    "        return dx   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n",
      "[[1. 0.]\n",
      " [0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0,-0.5],[-2.0,3.0]])\n",
    "mask = (x<=0)\n",
    "print(mask)\n",
    "x1 = x.copy()\n",
    "x1[mask] = 0\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 통하여 모르는 부분을 이해하도록 하자.\n",
    "\n",
    "5-5.2. Sigmoid 계층\n",
    "    시그모이드의 계산 그래프를 그리면 식 5.19와 같다.\n",
    "\n",
    "    시그모이드의 역전파는 총 4개의 노드를 거친다.\n",
    "    x,exp,+,/ 차근차근 역전파를 구해보자.\n",
    "\n",
    "    1단계 /노드\n",
    "        y = 1/x을 미분하면 - 1/x^2이다.\n",
    "        즉 -y^2을 곱해서 하류로 전달한다.\n",
    "\n",
    "    2단계 +노드\n",
    "        달라지는 점 없이 상류의 값을 하류로 전달한다.\n",
    "    \n",
    "    3단계 exp 노드\n",
    "        y = exp(x)를 미분하면 exp(x)가 나온다.\n",
    "        상류의 값에 순전파 때의 출력을 곱해 하류로 전달한다.\n",
    "        이 예시에서는 -x를 넣는다.\n",
    "\n",
    "    4단계 x노드\n",
    "        바꿔 곱해서 상류의 흐름을 보낸다.\n",
    "        본 예시에서는 -1을 사용하도록 한다.\n",
    "\n",
    "    전체 결과를 정리하면 다음과 같이 표현 가능하다\n",
    "\n",
    "    dL/dy *y(1-y)\n",
    "    즉, sigmoid계층의 역전파는 순전파의 출력만으로 계산할 수 있다.\n",
    "    \n",
    "    이제 구현하여 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self,x):\n",
    "        out = 1/(1+np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout*(1.0-self.out)*self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-6. Affine/Softmax 계층 구현하기\n",
    "\n",
    "5-6.1. Affine 계층\n",
    "    신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬곱을 사용했다.\n",
    "    다차원 배열에서의 dot 연산과 +연산의 역전파에 관하여 생각해보아야 한다.\n",
    "    \n",
    "    affine계층은은 그림 5-24를 확인해보자.\n",
    "    두가지의 행렬을 dot연산 한뒤 \n",
    "    그 결과에 W 가중치를 더하는 흐름이다.\n",
    "\n",
    "    현재까지의 계산 그래프는 노드 사이에 스칼라값이 흘렀지만\n",
    "    이제는 행렬이 흐를 것이다.\n",
    "\n",
    "    그럼 역전파에 대하여 생각해보자.\n",
    "    행렬을 사용한 역전파도 행렬의 원소마다 전개해보면\n",
    "    스칼라 값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있다.\n",
    "    실제로 전개해보면 다음과 같은 식이 도출되게 된다.\n",
    "\n",
    "    dL/dx = dl/dy * W(t)\n",
    "    dL/dw = x(t) * dl/dy\n",
    "\n",
    "    여기서 t는 전치 행렬을 의미한다. 전치 행렬이란 i,j의 원소의 위치를 j,i로 변경하는 것이다\n",
    "    수식으로는 식 5.14를 확인하자.\n",
    "\n",
    "    만약 (2,3) 인 행렬의 전치 행렬을 구한다 하면 그 결과는 (3,2)가 된다.\n",
    "    \n",
    "    batch도 동일하지만 유일하게 달라지는 점은 x의 형태가 n,2가 된다는 것이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0,0,0],[10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(X_dot_W+B)\n",
    "\n",
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "dB = np.sum(dY,axis = 0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b =b \n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W)+ self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis= 0)\n",
    "        \n",
    "        return dx    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-6.3. Softmax-with-Loss 계층\n",
    "    마지막으로 출력층에서 사용하는 소프트 맥스 함수에 관하여 알아보자.\n",
    "    소프트 맥슨는 입력 값을 정규화 하여 출력한다.\n",
    "    손글씨에서 softmax 계층의 출력은 그림 5-28처럼 진행된다.\n",
    "\n",
    "    이제 소프트 맥스를 구현할것이다.\n",
    "    그전에 손실 함수인 교차 엔트로피 오차도 포함하여 softmax - with - loss 계층의 계산 그래프를 보자. 그림 5-29\n",
    "    간단히 과정은 다음과 같다.\n",
    "        1. softmax로 데이터가 들어온다.\n",
    "        2.이것을 통하여 y라는 정규화된 값을 출력한다.\n",
    "        3. 이 정규화 된 값은 다시 cross entropy error계층으로 들어간다.\n",
    "            그리고 이때 정답레이블도 같이 입력되어 이들로 부터 loss값을 계산한다.\n",
    "    \n",
    "    이제 주목할 것은 역전파 파트이다.\n",
    "    softmax 계층의 역전파는 결과값 - 예측값 이라는 말끔한 결과를 내어 놓았다. \n",
    "    y는 softmax계층의 출력이고 t는 정답레이블이다.\n",
    "    즉 softmax 계층의 출력과 정답 레이블의 차분인 것이다.\n",
    "\n",
    "    우리는 이 차분을 효율적이게 앞으로 전달하여야 한다. 앞으로 이 차분은\n",
    "    현재 출력과 정답 레이블의 오차르 ㄹ있는 그대로 드러내느 것이다.\n",
    "\n",
    "    이제 예를 한가지 들어보자.\n",
    "    정답 레이블이 (0,1,0 )일 때\n",
    "    softmax계층이 (0.3,0.2,0.5)를 출력했다고 해보자.\n",
    "\n",
    "    정답 레이블을 보면 정답의 인덱스를 1이다.\n",
    "    그런데 이때 확률이 겨우 20%라는 것을 확인해볼 수 있다.\n",
    "    즉, 일을 잘 못하고 있다는 것이다.\n",
    "    그렇다면 역전파에서는 (0.3,-0.8,0.5)라는 거대한 값을 전파하게 된다.\n",
    "\n",
    "    그렇다면 정답레이블은 동일한데\n",
    "    softmax 계층이 (0.01,0.99,0)을 출력하는 경우이다.\n",
    "    이경우 softmax 계층의 역전파가 보내느 오차는 비교적 작은 (0.01,-0.01,0)이다,\n",
    "    즉 학습의 정도도 작아진다는 것이다.\n",
    "\n",
    "    그럼 softmax-with - loss 계층을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "    \n",
    "    return y\n",
    "    \n",
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return  -np.sum(t*np.log(y+1e-7))/batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self,x,t):        \n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout =1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-7. 오차역전법 구현하기\n",
    "\n",
    "5-7.1. 신경망 학습의 전체 그림\n",
    "\n",
    "전제\n",
    "    신경망에는 적응 가능한 가중치와 편향이 존재하며, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라고한다.\n",
    "    신경망은 다음 4단계로 수행한다.\n",
    "\n",
    "1단계 - 미니 배치\n",
    "2단계 - 기울기 산출\n",
    "3단계 - 매개변수 갱신\n",
    "4단계 - 반복\n",
    "\n",
    "지금까지 공부한 오차역전파법이 등장하는 단계는 바로 기울기 산출이다. 앞장에서는 이기울기르 구하기 위해 수치 미분을 사용했엇다,\n",
    "그런데 수치미분은 구현은 간단하지만 계산이 오래 걸렸다. 오차역전파법을 비교적 빠르게 기울기를 구할 수 있다.\n",
    "\n",
    "5-7.2. 오차 역전파법을 적용한 신경망 구현하기\n",
    "    여기에서는 2층 신경망을 TwolayerNet 클래스로 구현할것이다.\n",
    "    우선은 이 클래스의 인스턴스 변수와 매서드를 정리한 표를 보자.\n",
    "\n",
    "    1.\n",
    "        params - dict변수 가충치 나 편향 값이 담겨있다.\n",
    "        layers - 순서가 있는 dict 신경망의 계층을 보관\n",
    "        lastLayter - 마지막 계층 , 본 예에서는 SoftmaxWithLoss 계층\n",
    "\n",
    "    2. \n",
    "        __init__(self,input_size,hidden_size,outpput_size,weight_init_std) \n",
    "        - 초기화를 수행한다. 인수는 앞에서 부터 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시  정규분포의 스케일\n",
    "\n",
    "        predict(self,x) - 예측을 수행한다. 인수 x는 이미지 데이터, 손실 함수의 값을 구한다.\n",
    "\n",
    "        accuracy(self,x,t) - 정확도를 구한다.\n",
    "        numerical_gradient(self) - 가중치 매개변수의 기울기를 수치 미분 방식을 구한다.\n",
    "        gradient(self,x,t) - 가중치 매개변수의 기울기를 오차역전파버으로 구한다.\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "def numerical_gradient1(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h) 계산\n",
    "        x[idx] = tmp_val+h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h) 계산\n",
    "        x[idx] = tmp_val-h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx]  = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val #값 복원\n",
    "        \n",
    "    return grad\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "def numerical_gradient_batch(f,X):\n",
    "    if X.ndim == 1:\n",
    "        return numerical_diff(f,X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        for idx,x in enumerate(X):\n",
    "            grad[idx] = numerical_gradient1(f,X[idx])\n",
    "        return grad\n",
    "    \n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0          # false를 충족하는 원소는 0으로 바꾸어버리라는 의미\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout \n",
    "        \n",
    "        return dx   \n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size,hidden_size, output_size,weight_init_std = 0.01):\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std *\\\n",
    "                            np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std *\\\n",
    "                            np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        #계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])\n",
    "        self.layers['Relu1'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "        \n",
    "        \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis = 1)\n",
    "        if t.ndim != 1 : t = np.argmax(t,axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력 데이터, t: 정답 레이블\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W:self.loss(x,t) #로스함수가 들어있는 거임\n",
    "    \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient_batch(loss_W,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient_batch(loss_W,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient_batch(loss_W,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient_batch(loss_W,self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self,x,t):\n",
    "        # 순전파\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        #역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        \n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "    \n",
    "        #결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.00013922026276162408\n",
      "b1 : 0.0038607111728059355\n",
      "W2 : 0.0026814515847468994\n",
      "b2 : 0.11334014726888117\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from data import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize =True,one_hot_label = True)\n",
    "# x_train = x_train.reshape(60000,784)\n",
    "# t_train = t_train.reshape(60000,1)\n",
    "network = TwoLayerNet(input_size = 784,hidden_size = 50, output_size = 10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch,t_batch)\n",
    "grad_backprop = network.gradient(x_batch,t_batch)\n",
    "\n",
    "for key in grad_backprop.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key]-grad_numerical[key]))\n",
    "    print(f'{key} : {diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-7.4) 오차역전파법을 사용한 학습 구현하기\n",
    "    마지막으로 오차역전파법을 사용하여 모델을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.08168333333333333 ,0.0802\n",
      "train acc, test acc : 0.10218333333333333 ,0.101\n",
      "train acc, test acc : 0.09871666666666666 ,0.098\n",
      "train acc, test acc : 0.09871666666666666 ,0.098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zm/91nct82j02j0xmt1jpbd7xrc0000gn/T/ipykernel_23718/1924815439.py:3: RuntimeWarning: invalid value encountered in subtract\n",
      "  exp_a = np.exp(a-c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.09871666666666666 ,0.098\n",
      "train acc, test acc : 0.09871666666666666 ,0.098\n",
      "train acc, test acc : 0.09871666666666666 ,0.098\n",
      "train acc, test acc : 0.09871666666666666 ,0.098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000027?line=48'>49</a>\u001b[0m train_loss_list\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000027?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m iter_per_epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000027?line=51'>52</a>\u001b[0m     train_acc \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49maccuracy(x_train,t_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000027?line=52'>53</a>\u001b[0m     test_acc \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39maccuracy(x_test,t_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000027?line=53'>54</a>\u001b[0m     train_acc_list\u001b[39m.\u001b[39mappend(test_acc)\n",
      "\u001b[1;32m/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb Cell 20'\u001b[0m in \u001b[0;36mTwoLayerNet.accuracy\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=86'>87</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccuracy\u001b[39m(\u001b[39mself\u001b[39m,x,t):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=87'>88</a>\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=88'>89</a>\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y,axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=89'>90</a>\u001b[0m     \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m : t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(t,axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb Cell 20'\u001b[0m in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=75'>76</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=76'>77</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=77'>78</a>\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000021?line=79'>80</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;32m/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb Cell 16'\u001b[0m in \u001b[0;36mAffine.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000015?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000015?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000015?line=10'>11</a>\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(x,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW)\u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/ch5_backpropagation.ipynb#ch0000015?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data import load_mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train,t_train),(x_test,y_test) = load_mnist(normalize =True,one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "iter_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "\n",
    "\n",
    "# 1에폭당 반복수\n",
    "iter_per_epoch = max(train_size / batch_size,1)\n",
    "\n",
    "\n",
    "#hyper params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(iter_num):\n",
    "    #미니배치 생성\n",
    "    \n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-= learning_rate*grad[key]\n",
    "    \n",
    "    #학습경과 기록\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i% iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(test_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f'train acc, test acc : {train_acc} ,{test_acc}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7eeedbc38737b45b3f3d9c30fb6c381a1d5f1910f05692e0cad4cfbf19e6b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
