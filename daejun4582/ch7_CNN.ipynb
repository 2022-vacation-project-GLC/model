{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 합성공 신경망(CNN)\n",
    ">\n",
    "    > 이번장 주제는 합성곱 신경망 이다.  \n",
    "    CNN은 이미지 인식과 음성 인식등 다양한 곳에서 사용되는데,  \n",
    "    특히 이미지 인식 분야에서 딥러닝 활요한 기법은 거의 다 CNN을 기초로 한다.  \n",
    "    이번장에서는 이 기술의 메커니즘을 자세히 설명하고 이를 파이썬으로 구현해보겠다.  \n",
    "\n",
    "## 7-1 전체 구조\n",
    ">\n",
    "    > 지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었다.  \n",
    "    이를 완전연결(fully-connected, 전결합)이라고 하며  \n",
    "    완전히 연결된 계층을 Affine 계층이라는 이름으로 구현했다.\n",
    "    Affine 계층ㅇ을 사용하면, 가령ㅇ 층이 5개인 전결합 신경망은 그림 7-1과 같이 구현 할 수 있다.  \n",
    "\n",
    ">\n",
    "    > 그림 7-1과 같이 전결합 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU계층이 이어진다.  \n",
    "    이 그림에서는 Affine-ReLU 조합이 4개가 쌓였고  \n",
    "    마지막 5번째 층은 Affine계층에 이어 소프트맥스 계층에서 최종 결과를 출력한다. 그럼 합성곱은 어떻게 다를까?\n",
    "\n",
    ">\n",
    "    > 그림 7-2가 바로 cnn의 예이다.  \n",
    "    그림과 같이 CNN에서는 새롱누 합성곱 계층과 풀링 계층이 추가가 된다. \n",
    "    CNN계층의 흐름은 Conv-ReLU-(Pooling) 과 같이 연결된다.  \n",
    "\n",
    ">\n",
    "    > 또다른 주목할 점은 출력에 가까운 층에서는 지금까지의 Affine-ReLU구성을 사용할수 있다는 것이다.  \n",
    "    또, 마지막 출력 계층에서는 Affine-Softmax조합을 그대로 사용한다.  \n",
    "    이상은 일반적인 CNN에서 흔히 볼 수 있는 구성이다. \n",
    "\n",
    "## 7-2 합성공 계층\n",
    ">\n",
    "    > CNN에서는 padding,stride등 CNN고유의 용어가 등장한다.  \n",
    "    또, 각 계층 사이에는 3차원 데이터 같이 입체적인 데이터가 흐른다는 점에서 완전연결 신경망과는 다르다.  \n",
    "    우선 이번 절 에서는 CNN에서 사용한 합성곱 계층의 구조를 차분히 살펴보기로 하겠다.\n",
    "\n",
    "### 7-2.1 합성공 계층\n",
    ">\n",
    "    > 지급까지는 전연결 계층을 사용하였다.  \n",
    "    완전연결 계층에서는 인접하는 계층의 뉴런이 모두 연결 되고 출력의 수는 임의로 정할 수 있었다.  \n",
    "\n",
    ">\n",
    "    > 완전 연결 계층의 문제는 무엇일까?  \n",
    "    바로 데이터의 형상이 무시된다는 사실이다.  \n",
    "    데이터가 이미지인 경우를 예를 들어보자.  \n",
    "    이미지는 통상 세로 가로 채널로 구성된 3차원 데이터 이다.  \n",
    "    그러나 완전 연결 계층에 입력할 땐느 3차원 데이터를 평평한 1차원 데이터로 평탄화 해줘야 한다.  \n",
    "    사실 지금까지 MNIST 데이터셋을 사용한 사례에서는 형상이 (1,28,28)인 이미지를  \n",
    "    1줄로 세운 784개의 데이터로 변환하여 입력하였다.\n",
    "\n",
    ">   \n",
    "    > 이미지는 3차원 형상이며, 이형상에는 소중한 공간적 정보가 담겨있다.  \n",
    "    예를 들어 공간적으로 가까운 픽셀은 값이 비슷하거나, RGB의 각 채널은 서로 밀접한 관련이 되어있거나  \n",
    "    거리가 먼 픽셀끼리는 별 연관이 없는 등, 3차원 속에서 의미를 가즌ㄴ 본질적인 패턴이 숨어있을 것이다.  \n",
    "    그러나 완전연결 계층은 형상을 무시하고 모든 입ㄹ력 데이터를 동등한 뉴런으로 취급하여 형상에 담긴 정보를 살릴수 없다.  \n",
    ">\n",
    "    >CNN에서는 합성공 계층의 입출력 데이터를 특징맵이라고도 부른다.(feature map)  \n",
    "    합성곱 계층의 입력 데이터를 입력 특징 맵(input feature map),  \n",
    "    출력 데이터를 출력특징 맵(output feature map)이라고 하는 식이다.  \n",
    "    이 책에서는 입출력 데이터와 특징 맵을 같은 의미로 사용한다.\n",
    "\n",
    "### 7-2.2 합성곱 연산  \n",
    ">\n",
    "    > 합성곱 계층에서의 합성곱 연산을 처리한다.  \n",
    "    합성곱 연산은 이미지 처리에서 말하는 필터 연산에 해당한다.  \n",
    "    구체적인 예를 보며 이해해보자(그림 7-3)  \n",
    "    이 예시에서 입력 데이터는 세로 가로 방향의 형상을 가졌고, 필터 역시 세로 가로 방향의 차원을 가진다.  \n",
    "    데이터와 필터의 형상을 (높이,너비)로 표기하며  \n",
    "    이예에서는 입력은 (4,4) 필터는 (3,3) 출력은 (2,2)가 된다.  \n",
    "    문헌에 따라 필터를 커널이라 칭하기도 한다.  \n",
    ">\n",
    "    > 그럼 그림 7-3의 합성곱 연산 예에서 어떤 계산이 이뤄지는지 설명하겠다.  \n",
    "    그림 7-4는 이 합성곱 연산의 계산 순서를 그려본 것이다.  \n",
    "\n",
    ">\n",
    "    >합성곱 연산은 필터의 윈도우를 일정 간격으로 이동해가며 입력 데이터에 적용한다.  \n",
    "    여기에서 말하는 윈도우는 그림 7-4의 회색 3x3부분을 가리킨다.  \n",
    "    이 그림에서 보듯 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구한다.(이 계산은 단일 곱셈-누산(fused multiply -add))  \n",
    "    이 과정에서 모든 장소에서 수행하면 합성공 연산의 출력이 완성된다.\n",
    "\n",
    ">\n",
    "    > 자, 완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수가 그동안의 '가중치'에 해당한다.  \n",
    "    그리고 CNN에서도 편향이 존재한다. 그림 7-3은 필터를 적용하는 단계까지만 보여준 것이고, 편향까지 포함하면 그림 7-5와 같은 흐름이 된다.  \n",
    "    그리고 편향은 항상 1x1 만 존재한다. 그 하나의 값을 필터를 적용한 모든 원소에 더하는 것이다.  \n",
    "\n",
    "### 7-2.3 Padding\n",
    ">\n",
    "    > 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채우기도 한다.  \n",
    "    이를 패딩이라 하며, 합성곱 연산에서 자주 이용하는 기법이다.  \n",
    "    예를 들어 그림 7-6은 (4,4) 크기의 입력 데이터에 폭이 1인 패딩을 적용한 모습이다.  \n",
    "    입력 데이터 사방 1픽셀을 특정 값으로 채우는 것이다.  \n",
    "\n",
    ">\n",
    "    > 그림과 같이 처음에 크기가 (4,4)인 입력 데이터에 패딩이 추가되어 (6,6)이 된다.  \n",
    "    이 입력에 (3,3) 크기의 필터를 걸면 (4,4) 크기의 출력 데이터가 생성된다.  \n",
    "    이 예에서는 패딩을 1로 설정했지만,2나 3등 원하는 정수로설정 할 수 있다. 만약 그림 7-5에 패딩을 2로 설정하면  \n",
    "    입력데이터의 크기는 (8,8)이 되고 3으로 설정하면 (10,10)이 된다.\n",
    "\n",
    "### 7-2.4 Stride\n",
    ">\n",
    "    >  필터를 적용하는 위치의 간격을 스트라이드 라고 한다.  \n",
    "    지금까지 본 예는 모두 스트라이드가 1이었지만  \n",
    "    예를 들어 스트라이드를 2로 하면 필터를 적용하는 윈도우가 두 칸씩 이동한다.(그림 7-7)\n",
    "\n",
    ">\n",
    "    > 그림 에서는 크기가 (7,7)인 입력 데이터에 스트라이드를 2로 설정한 필터를 적용한다.  \n",
    "    이처럼 스트라이드는 필터를 적용하는 간격을 지정한다.  \n",
    "\n",
    ">\n",
    "    > 그런데 스트라이드를 2로 하니 출력은 (3,3)이 된다.  \n",
    "    이처럼 스트라이드를 키우면 출력 크기는 작아진다.  \n",
    "    한편 패딩을 크게하면 출력 크기가 커졌다.  \n",
    "    이러한 관계를 수식화 하면 식 7.1이 된다.  \n",
    "    입력 크기를 (H,W) 필터 크기를 (FH,FW), 출력 크기를 (OH,OW), 패딩을 P, 스트라이드를 S라 하면 출력 크기는 다음식으로 계산한다.\n",
    "    단 주의할점은 출력크기가 정수로 나눠떨어지는 값이어야 한다는 점에 주의해야한다. 출력 크기가 정수가 아니면 오류를 내는 등의 대응을 해야한다.  \n",
    "\n",
    "### 7-2.5 3차원 데이터의 합성곱 연산\n",
    "\n",
    ">   \n",
    "    > 지금까지 2차원 형상을 다루는 합성곱 연산을 살펴보았다.  \n",
    "    그러나 이미지만 해도 세로, 가로에 더해서 채널까지 고려한 3차원 데이터 이다.   \n",
    "    이번절 에서는 조금 전과 같은 순서로 , 채널까지 고려한 3차원 데이터를 다루는 합성공 연산을 살펴보겠다.  \n",
    "\n",
    ">   \n",
    "    > 그림 7-8은 3차원 데이터의 합성곱 연산 예이다. 그리고 그림 7-9는 계산 순서이다.  \n",
    "    2차원일때와 비교하면 길이 방향으로 특징 맵이 늘어나 ㅆ다. 채널 쪽으로 특징 맵이 려러개 있다면  \n",
    "    입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고 그 겨로가를 더해서 하나로 출력을 얻는다.  \n",
    "    3차원 합성곱 연산에서 주의할 점은 입력 데이터의 채널 수와 필터의 채널수가 같아야 한다는 것이다.  \n",
    "\n",
    "### 7-2.6 블록으로 생각하기\n",
    "\n",
    ">\n",
    "    > 3차원 합성곱 연산은 데이터와 필터를 직육면체 블록이라고 생가하면 쉽다.  \n",
    "    블록은 3차원 직육면체이다. 또 3차원 데이터를 다차원 배열로 나타낼 때는 (채널,높이,너비) 순서로 쓰겠다.  \n",
    "    예를 들어 채널수 C, 높이 H, 너비 W인 데이터 형상은 (C,H,W)로 쓴다.  \n",
    "    필터도 같은 순서로 쓴다. (C,FH,FW)로 쓴다.\n",
    "\n",
    ">\n",
    "    > 자 이 예에서 출력 데이터는 한장의 특징 맵이다. 한장의 특징 맵을 다른 말로 하면 채널이 1개인 특징 맵이다. \n",
    "    그럼 합성곱 연산의 출력으로 다수의 채널을 내보내려면 어떻게 해야 할까?  \n",
    "    그 답은 필터(가중치)를 사용하는 것이다. 그림으로는(그림 7-11) 처럼 된다.\n",
    "\n",
    ">\n",
    "    > 이그림과 같이 필터를 FN개 적용하면 출력맵도 FN개가 생성된다.  \n",
    "    그리고 그 FN개의 맵을 모으면 형상이 (FN,OH,OW)인 블록이 완성됩니다.  \n",
    "    이 완성된 블록을 다음 계층으로 넘기겠다는 것이 CNN의 처리 흐름이다.\n",
    "\n",
    ">\n",
    "    > 이상에서 보든 합성곱 연산에서는 필터의 수도 고려해야 한다.  \n",
    "    그런 이유로 필터의 가중치를 4차원 데이터이며 (출력 채널수, 입력 채널수, 높이, 너비)순으로 쓴다.  \n",
    "    예를 들어 채널 수 3 (5,5)인 필터가 20개가 있다면 (20,3,5,5)로 쓴다.  \n",
    "    자 합성곱 연산에도 편향이 쓰인다. \n",
    "    그림 7-12는 그림 7-11에 편향을 더한 모습이다.\n",
    "\n",
    "### 7-2.7 배치처리\n",
    "\n",
    ">\n",
    "    >신경망 처리에서는 입력 데이터를 한 덩어리로 묶어 배치로 처리했다.  \n",
    "    전연결 신경망을 구현하면서는 이 방식을 지원하여 처리 효율을 높이고, 미니 배치 방식의 학습도 지원하도록 하겠다.  \n",
    "    그래서 데이터 형식은 다음과 같다, (데이터수, 채널수, 높이, 너비) \n",
    "    데이터가 N개일때 그림 7-12를 배치 처리한다면 데이터 형태가 그림 7-13 처럼 되는 것이다.  \n",
    "\n",
    "## 7-3 풀링 계층\n",
    "\n",
    ">\n",
    "    > 풀링은 세로 가로 방향의 공간을 줄이는 연산이다. 예를 들어 그림 7-14 와 같이 2x2 영역을 원소 하나로 집약하여 공간 크기를 줄인다.\n",
    "    그림은 max pooling을 스트라이드 2로 처리한 순서이다. max pooling은 그림과 같이  \n",
    "    2x2크기의 영역에서 가장 큰 원소 하나를 꺼낸다. 또 스트라이드는 이 예에서는 2로 설정했으므로 \n",
    "    2x2 윈도우가 원소 2칸 간격으로 이동한다.  \n",
    "    참고로 풀링의 윈도우 크기와 스트라디는 같은 값으로 설정하는 것이 보통이다.  \n",
    "\n",
    "### 7-3.1 풀링 계층의 특징\n",
    ">\n",
    "    > 풀링 계층의 특징은 다음과 같다.\n",
    "\n",
    "> - 학습해야할 매개변수가 없다.\n",
    ">\n",
    "    > 풀링 계층은 합성곱 계층과 달리 학습해야할 매개변수가 없다. 풀링은 대상 여역에서 최댓값이나 평균을 취하는 처리이므로 특별히 학습할 것이 없다.  \n",
    "> - 채널 수가 변하지 않는다.  \n",
    ">\n",
    "    > 풀링 연산은 입력 데이터의 채널수 그대로 출력 데이터로 내보낸다. 그림 처럼 채널마다 독립적으로 계산하기 때문이다.\n",
    "> - 입력의 변화에 영향을 적게 받는다.\n",
    ">\n",
    "    > 입력 데이터가 조금 변해도 풀리의 결과는 잘 변하지 않는다. \n",
    "    예를 들어 그림 7-16은 입력 데이터의 차이를 풀링이 흡수해 사라지게 하는 모습을 보여준다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 합성곱/풀링 계층 구현하기\n",
    "\n",
    ">\n",
    "    > 앞에서 설명한 대로 CNN에서 계층 사이를 흐르는 데이터는 4차원 데이터이다.  \n",
    "    예를 들어 데이터의 형상이 (10,1,28,28)이라면,  \n",
    "    이는 높이 28 너비 28 채널 1개인 데이터가 10개 라는 이야기다. 이를 파이썬으로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10,1,28,28)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 10개 중 첫 번째 데이터에 접근하려면 단순히 x[0]이라고 작성한다.  \n",
    "마찬가지로 두 번째 데이터는 x[1]에 위치하여 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x[0].shape)\n",
    "print(x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 첫번째 데이터의 첫 채널의 공간 데이터에 접근하려면 다음과 같이 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.13241126e-01 7.15087651e-01 3.69240197e-02 1.81291248e-01\n",
      "  7.54898984e-01 4.27093697e-02 5.48507439e-01 7.30427838e-01\n",
      "  3.38944782e-01 2.06511016e-01 2.10904205e-01 7.90578646e-01\n",
      "  4.36383532e-03 5.94165977e-01 4.81557834e-01 6.26884216e-01\n",
      "  5.56512245e-01 9.12964626e-01 9.99271527e-01 6.31449315e-01\n",
      "  7.50070337e-01 8.12081245e-01 1.74130271e-01 9.62217821e-01\n",
      "  7.73126501e-01 2.68482098e-01 1.03159111e-01 6.22258697e-01]\n",
      " [8.47759648e-01 6.48338462e-01 4.05374148e-01 2.38470538e-01\n",
      "  9.53337986e-01 5.25963514e-01 9.81930827e-01 3.57219631e-01\n",
      "  2.88102312e-01 2.72011966e-01 3.75935258e-01 5.12173172e-01\n",
      "  6.72114658e-01 9.14932574e-01 8.81350448e-01 7.38394144e-01\n",
      "  1.92353677e-01 3.81117134e-01 5.07159647e-01 3.12504809e-02\n",
      "  5.56239275e-01 5.81210274e-01 7.11194481e-01 1.34491517e-01\n",
      "  6.04745648e-02 5.32619881e-01 6.83214453e-01 4.59506312e-01]\n",
      " [3.99741329e-01 5.26194038e-01 6.03956301e-01 9.02664279e-01\n",
      "  9.04345452e-01 5.27914088e-01 9.19251646e-01 3.46485241e-01\n",
      "  5.61136629e-01 9.64143067e-01 9.15893915e-02 8.43121020e-01\n",
      "  5.56923654e-01 2.78942707e-02 8.89557061e-01 5.60689127e-01\n",
      "  1.01043426e-01 8.57890897e-01 2.95584171e-01 4.46408542e-01\n",
      "  3.03067390e-01 5.59020056e-01 2.33631756e-01 3.32816362e-01\n",
      "  6.70542261e-01 9.71488504e-01 9.60206290e-01 6.16446761e-01]\n",
      " [8.23255073e-01 2.00943646e-01 3.29636488e-01 3.40933902e-01\n",
      "  2.84505338e-01 8.81532777e-02 9.86472074e-01 3.53700571e-01\n",
      "  2.98907354e-01 3.08411340e-01 2.40984372e-01 5.80447955e-01\n",
      "  7.68569692e-01 8.64205404e-01 1.84101591e-01 5.58331503e-01\n",
      "  7.70717889e-01 6.36375578e-01 7.13685818e-01 3.27826405e-01\n",
      "  9.46275365e-01 4.00757708e-01 3.24317667e-01 8.54687773e-01\n",
      "  2.00996939e-01 3.12020989e-01 8.33122575e-01 2.84504287e-01]\n",
      " [3.43072360e-01 1.20199425e-02 1.83249283e-01 2.80873610e-01\n",
      "  1.42808734e-01 2.80025317e-01 1.44986197e-01 2.90242414e-01\n",
      "  1.89485907e-01 6.17662941e-01 8.22218975e-01 3.35972307e-01\n",
      "  4.53972658e-01 7.68125197e-01 3.52507700e-01 9.29979328e-01\n",
      "  1.14400266e-01 8.96871751e-02 5.08586001e-02 4.29303076e-01\n",
      "  5.34657248e-01 7.37643008e-01 3.01562031e-01 7.86758142e-01\n",
      "  1.79332914e-01 1.44723785e-01 2.27198085e-01 8.51089660e-01]\n",
      " [7.49669675e-01 4.86547134e-01 1.65842412e-03 7.12266892e-01\n",
      "  8.10425091e-02 6.44234098e-01 7.55784706e-02 3.99042239e-01\n",
      "  3.98533034e-01 7.61720199e-01 6.29164263e-01 8.02341742e-01\n",
      "  6.29192462e-01 5.95769384e-01 6.09846485e-01 5.54122874e-01\n",
      "  9.83024503e-02 4.36973566e-01 3.02018908e-01 1.35576726e-01\n",
      "  2.43065735e-01 2.19558882e-01 5.76464504e-01 2.47870795e-01\n",
      "  4.35476535e-01 6.97406608e-01 3.68826912e-01 6.54390042e-01]\n",
      " [2.78953504e-01 1.64546830e-01 8.09483835e-01 9.09650810e-01\n",
      "  7.09205302e-01 3.26145645e-01 1.41626400e-01 7.52092569e-01\n",
      "  1.36250914e-01 2.78780101e-02 6.39300415e-01 3.02141846e-01\n",
      "  6.66243430e-01 4.62961539e-01 2.02849382e-01 4.82140748e-01\n",
      "  7.90705112e-01 5.66074576e-01 3.84771065e-01 5.32180919e-01\n",
      "  5.91808416e-01 2.43612506e-01 7.26903575e-01 1.51245160e-01\n",
      "  7.61228161e-01 7.44066894e-01 4.31092592e-01 2.66192060e-01]\n",
      " [1.54592881e-01 1.99668636e-01 3.59882464e-01 1.48839012e-01\n",
      "  9.05845705e-01 6.98919550e-01 7.97240689e-01 9.87193552e-01\n",
      "  9.91422890e-01 5.04463342e-02 8.51448257e-01 4.62538963e-01\n",
      "  1.53299769e-01 7.66567386e-01 9.04433349e-02 5.72225554e-01\n",
      "  2.83925836e-01 7.52815685e-03 1.77020800e-01 1.31489412e-01\n",
      "  7.24500165e-01 3.25156933e-01 2.39945901e-01 4.68775847e-01\n",
      "  8.70990898e-01 7.14325035e-01 9.18699201e-01 4.77035119e-01]\n",
      " [8.96357775e-01 3.67932113e-01 1.44558509e-01 4.02103197e-01\n",
      "  9.03280583e-01 3.26616913e-01 7.92493181e-01 7.92234350e-01\n",
      "  7.84650316e-01 8.48541865e-01 8.34908399e-01 6.67563475e-01\n",
      "  4.96640364e-01 9.35690118e-01 1.25902584e-01 4.79209160e-01\n",
      "  1.51273356e-01 6.34615075e-01 2.36682012e-01 3.24355463e-01\n",
      "  9.65218524e-01 2.99064808e-01 2.32572537e-01 5.00910784e-01\n",
      "  2.39529095e-01 8.36133104e-02 4.53678744e-01 6.18786410e-01]\n",
      " [8.70637063e-01 2.00805455e-02 6.02695896e-01 7.69496789e-01\n",
      "  6.96803443e-01 8.13741340e-01 5.61429138e-01 8.93960082e-01\n",
      "  1.97487364e-01 7.32698464e-01 5.19836705e-01 9.95776150e-01\n",
      "  3.69292571e-01 6.23702617e-01 8.21332296e-01 4.37329542e-01\n",
      "  3.20589732e-01 4.99447877e-01 7.38468312e-01 6.19594636e-01\n",
      "  6.17081654e-01 8.10823926e-01 9.55651460e-01 8.20269698e-01\n",
      "  5.36029119e-01 7.39823510e-01 7.59236203e-01 3.00086715e-01]\n",
      " [2.17242494e-02 2.98089313e-01 7.24719884e-01 7.31745264e-01\n",
      "  9.72747620e-01 7.82508034e-01 4.90525164e-01 6.48851578e-01\n",
      "  4.68143362e-01 9.74694525e-01 9.35006943e-01 1.58697294e-01\n",
      "  8.13650477e-01 3.46085359e-01 2.45062414e-01 2.07545927e-01\n",
      "  5.96051784e-02 5.48965209e-01 5.91593588e-01 6.79147698e-01\n",
      "  4.12536187e-02 6.03540790e-01 6.34874433e-01 8.91677832e-01\n",
      "  4.87453744e-01 2.15523814e-01 7.09194767e-01 4.85435365e-03]\n",
      " [4.07035455e-02 2.85261699e-01 9.42843010e-01 8.20277896e-01\n",
      "  8.74326652e-02 6.66519619e-02 2.57147421e-01 7.83763081e-01\n",
      "  4.71601814e-01 8.20996401e-01 7.20209545e-01 2.86014592e-01\n",
      "  5.60707238e-01 3.96309057e-01 1.36804084e-01 4.00133847e-01\n",
      "  7.62481016e-01 8.41331261e-01 4.55786563e-01 8.72121369e-01\n",
      "  7.05237724e-02 8.38939471e-01 2.08047058e-01 2.61935787e-01\n",
      "  3.58243173e-01 9.91273948e-01 5.77432543e-01 4.58290525e-02]\n",
      " [7.97780822e-01 6.55191319e-01 8.39457359e-01 4.69358889e-01\n",
      "  5.78282664e-02 8.81924043e-01 8.42525682e-01 9.22141461e-01\n",
      "  9.24044001e-01 9.24181525e-01 2.72486550e-01 2.17302315e-01\n",
      "  4.96093764e-01 7.25350139e-02 6.59052140e-01 1.41934408e-01\n",
      "  3.64022283e-01 9.94549394e-01 4.50848753e-01 8.58138440e-01\n",
      "  8.60304521e-01 9.45197906e-01 3.10438438e-01 4.74308891e-01\n",
      "  5.33528683e-01 9.69711145e-01 2.41773239e-01 3.91419944e-01]\n",
      " [7.49568753e-01 4.46337906e-01 6.40345757e-01 8.31745157e-01\n",
      "  6.66649928e-01 1.31942418e-02 4.34277014e-01 2.75223189e-02\n",
      "  3.00584384e-01 5.26919067e-01 5.48077446e-01 7.49950245e-01\n",
      "  6.25605735e-01 5.77322926e-01 8.45230215e-01 7.76144485e-01\n",
      "  3.58000002e-01 6.47904662e-01 7.02581607e-02 7.42155251e-01\n",
      "  6.10730773e-01 3.99292106e-02 4.58577805e-01 2.29048317e-03\n",
      "  8.83688718e-01 2.56595343e-01 4.67287359e-01 9.12954764e-01]\n",
      " [1.90937494e-01 6.43700076e-01 7.41598445e-01 2.01446530e-01\n",
      "  7.47813465e-01 4.80671768e-01 1.73664386e-01 6.94529479e-01\n",
      "  8.44668083e-01 1.06606911e-01 7.20644886e-01 1.16286360e-01\n",
      "  2.90861864e-02 7.53331619e-01 4.00872129e-01 4.34468191e-01\n",
      "  4.20939167e-01 9.66964117e-01 7.19754918e-01 9.77664601e-02\n",
      "  1.98283016e-02 5.34177212e-01 2.79724541e-01 2.16013444e-01\n",
      "  3.90892696e-01 2.15595727e-01 5.40985771e-01 2.49677543e-01]\n",
      " [9.75892255e-01 6.90870229e-01 7.45820322e-01 4.77962283e-01\n",
      "  9.70022315e-02 9.98337568e-01 7.41929271e-01 2.35447380e-01\n",
      "  7.63157997e-01 5.34051615e-01 5.67355512e-01 2.93465554e-01\n",
      "  6.93704576e-01 5.08234865e-01 9.95044900e-01 1.56009041e-01\n",
      "  6.60790079e-02 3.68237860e-01 4.19778129e-02 7.19300661e-01\n",
      "  9.19708715e-01 2.72999284e-01 3.34294876e-01 1.45644147e-01\n",
      "  5.60585235e-01 2.89052611e-01 9.96857309e-03 9.63159199e-04]\n",
      " [5.65083261e-01 7.68256390e-01 1.76386904e-01 9.73097420e-02\n",
      "  5.56060082e-01 7.23901811e-01 2.40632614e-01 1.86320350e-01\n",
      "  1.83708997e-01 2.64945145e-01 6.29231422e-01 8.11116543e-02\n",
      "  4.90385322e-01 1.58934702e-01 9.21215715e-01 8.44108380e-01\n",
      "  3.86746162e-01 5.26498677e-02 6.18886288e-01 7.40257792e-01\n",
      "  5.59812551e-02 3.00110391e-01 5.49772365e-01 9.98041356e-02\n",
      "  9.64126673e-01 5.20263444e-01 8.52245253e-01 7.00800393e-01]\n",
      " [5.18607955e-01 6.49288974e-01 2.69147897e-01 5.76067282e-01\n",
      "  1.92819281e-02 5.87181635e-01 4.45016934e-01 5.23259045e-01\n",
      "  5.21612867e-01 2.90851195e-01 4.18155145e-01 5.72518544e-01\n",
      "  5.34372828e-01 8.29058932e-01 3.12194119e-01 1.62937590e-02\n",
      "  7.80101113e-01 4.65461316e-01 1.40096208e-02 4.74228527e-01\n",
      "  9.18678169e-01 1.03355774e-01 1.07751243e-01 2.92912963e-01\n",
      "  7.67557180e-01 4.76348827e-01 2.07830964e-01 5.84443304e-01]\n",
      " [5.61152050e-01 6.41544235e-01 1.81642766e-01 8.89699336e-01\n",
      "  8.00614067e-01 5.92113064e-01 6.14841627e-01 2.65350827e-01\n",
      "  2.37572272e-01 7.94139828e-01 2.35863483e-01 8.07552275e-02\n",
      "  2.65714280e-01 9.77703369e-01 3.67556222e-01 3.91769748e-01\n",
      "  9.34319196e-01 5.52295414e-01 1.01325434e-01 6.12821036e-01\n",
      "  9.96811577e-01 4.10014757e-01 3.87461228e-01 1.98030810e-01\n",
      "  8.44514157e-01 2.30218667e-01 1.24901477e-01 6.36039072e-02]\n",
      " [3.02950500e-01 5.97599688e-01 3.29225575e-01 2.91121244e-01\n",
      "  8.45412870e-01 3.31085975e-01 2.47884282e-01 3.03025169e-01\n",
      "  1.67197729e-01 4.83680346e-01 1.28499427e-01 4.78401998e-01\n",
      "  8.40885035e-01 7.19049549e-01 3.05344803e-01 5.01730716e-01\n",
      "  9.31438273e-02 6.97886301e-01 4.69130994e-01 5.68331102e-01\n",
      "  6.80471438e-01 8.15443352e-01 4.92171707e-01 4.15152028e-01\n",
      "  5.35349271e-01 3.99991144e-01 5.36680935e-01 2.21499904e-01]\n",
      " [3.63678281e-01 2.72387309e-02 4.42842337e-01 1.51453211e-01\n",
      "  9.19517216e-02 4.12223009e-01 4.62958871e-01 7.15231503e-01\n",
      "  1.23299198e-01 4.40710360e-01 3.53293889e-01 3.38844469e-01\n",
      "  3.33017907e-01 4.32760345e-02 8.62494073e-02 3.09748538e-01\n",
      "  2.80764156e-02 3.44532782e-01 7.12324494e-02 2.18016530e-01\n",
      "  4.71770869e-01 6.87389219e-02 3.83684851e-01 7.67999045e-01\n",
      "  9.90826324e-01 5.49791442e-01 6.66505289e-01 2.33292610e-01]\n",
      " [1.61472345e-01 9.56250297e-01 5.14479383e-01 5.15056719e-01\n",
      "  3.52921321e-01 9.51467554e-01 8.81735485e-01 1.79987085e-01\n",
      "  7.71528302e-01 1.10516353e-01 3.09224025e-01 7.94853801e-01\n",
      "  3.71173648e-01 8.93477166e-01 3.15903628e-01 9.56700439e-01\n",
      "  5.96385327e-01 4.64399432e-01 6.87634557e-01 6.57986768e-02\n",
      "  2.50925012e-01 8.34624459e-01 1.02137096e-01 8.77463712e-01\n",
      "  4.53399442e-01 4.76451002e-03 8.23564203e-01 5.16177926e-01]\n",
      " [7.21697111e-01 7.54271670e-01 6.54714669e-02 9.07782950e-01\n",
      "  5.30882426e-01 8.31944330e-01 6.05358756e-02 9.70071288e-01\n",
      "  7.37123380e-01 9.14437723e-01 4.39846901e-01 5.69827346e-01\n",
      "  8.26567763e-01 3.29130914e-01 8.03607020e-03 2.89621548e-01\n",
      "  5.66943480e-01 1.08301853e-01 6.40497332e-02 3.53363483e-02\n",
      "  1.71475094e-02 9.93915707e-01 2.64842647e-01 7.47729944e-01\n",
      "  9.09355397e-01 1.32031833e-01 5.15540929e-01 5.01785328e-01]\n",
      " [9.40423407e-01 1.34648483e-01 3.97979674e-01 6.91439631e-01\n",
      "  1.21784724e-02 1.07956149e-01 7.70543377e-01 4.12064408e-01\n",
      "  9.67654439e-01 4.51409774e-01 6.31857846e-01 6.90669108e-01\n",
      "  8.56406293e-01 1.26699532e-01 8.65933719e-01 8.03773218e-01\n",
      "  3.85245417e-03 9.38961782e-01 4.98147378e-01 4.68611953e-01\n",
      "  1.29433200e-01 4.56876355e-01 4.04322371e-01 8.59016368e-02\n",
      "  6.04592149e-01 4.01692519e-01 6.34800852e-01 2.87606043e-01]\n",
      " [8.75560077e-01 3.10790316e-01 6.34897453e-01 4.68272627e-01\n",
      "  8.16544309e-01 2.36650636e-01 7.13873203e-01 9.68248658e-01\n",
      "  7.98237876e-01 8.76674900e-01 9.31186857e-01 5.74922817e-01\n",
      "  4.51586646e-01 1.95345847e-01 4.06683541e-01 1.34225060e-01\n",
      "  3.37921072e-01 4.71331082e-02 7.08127492e-01 3.52984386e-01\n",
      "  4.56664370e-01 6.89549558e-01 8.95802332e-01 3.04106280e-01\n",
      "  6.00914356e-01 5.71802290e-01 6.48503416e-01 2.61608582e-01]\n",
      " [9.09512178e-01 4.93408803e-01 2.84607047e-01 9.68895885e-01\n",
      "  6.91453371e-01 9.07788655e-01 8.44302823e-01 8.15546397e-01\n",
      "  9.19715918e-01 8.33077674e-01 5.97393550e-01 2.03797683e-01\n",
      "  4.22830989e-01 8.73836548e-01 5.84832412e-01 3.18269386e-01\n",
      "  1.00680140e-02 8.14023328e-01 9.57220334e-01 9.30662429e-02\n",
      "  1.89080030e-01 3.33595087e-01 5.78449331e-01 5.38216910e-01\n",
      "  6.25485344e-01 5.35292060e-01 2.95273177e-01 3.43285928e-01]\n",
      " [6.10398179e-01 1.87922553e-01 5.47453142e-01 5.07073092e-01\n",
      "  6.03665442e-01 6.14913373e-01 3.40479625e-01 1.04809092e-01\n",
      "  9.69346495e-01 7.32315564e-01 1.09703266e-01 4.90789192e-01\n",
      "  1.74998872e-01 2.42487051e-01 7.32298281e-01 7.32444471e-01\n",
      "  8.09715939e-01 1.78182383e-01 7.96561203e-02 9.78996208e-01\n",
      "  7.98340321e-01 4.23172580e-01 3.61652065e-01 3.89967911e-01\n",
      "  9.40965978e-01 9.28376516e-01 2.93798204e-01 7.65626028e-01]\n",
      " [4.92661913e-01 4.17333150e-01 9.39167406e-01 9.20805956e-01\n",
      "  7.90133306e-01 9.68038559e-01 4.59231526e-01 7.57344344e-02\n",
      "  1.87242189e-01 1.43256879e-01 2.91225143e-02 3.31514858e-01\n",
      "  5.95362075e-01 6.12239074e-01 7.87670308e-01 3.70152508e-01\n",
      "  4.77864170e-01 1.50973128e-01 4.06155604e-01 6.85637110e-02\n",
      "  2.23034459e-01 8.50894510e-01 4.84427094e-01 7.22613023e-01\n",
      "  8.30394653e-01 4.40936537e-01 5.27420236e-01 6.64006584e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(x[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-4.2 im2col로 데이터 전개하기\n",
    ">\n",
    "    > 합성곱 연산을 곧이곧대로 구현하려면 for 문을 겹겹이 써야한다.  \n",
    "    생각만 해도 귀찮고 또 넘파이에 for문을 사용하면 성능이 떨어진다는 단점이 있다.  \n",
    "    이번 절 에서는 for문 대신 im2col이라는 편의함수를 사용하여 간단하게 구현해보겠다.  \n",
    "\n",
    ">\n",
    "    > im2col은 입력 데이터를 필터링(가중치 계산)하기 좋게 전개하는 함수이다.  \n",
    "    그림 7-17과 같이 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀐다(정확히는 배치 안의 데이터 수까지 포함한 4차원 데이터를 2차원으로)  \n",
    "    im2col은 필터링 하기 좋게 입력 데이터를 전개한다. 구체적으로는 그림 7-18과 같이 \n",
    "    입력 데이터에서 필터를 적용하는 영역을 한줄로 늘어놓는다.  \n",
    "    이 전개를 필터를 적용하는 모든 영역에 대해 수행하는데 im2col이다.  \n",
    "\n",
    ">\n",
    "    > 그림 7-18에는 보기 좋게끔 스트라이드를 크게 잡아 피러틩 적용 영역이 겹치지 않도록 했지만  \n",
    "    실제 상황에서는 영역이 겹치는 경우가 대부분이다. 필터 적용 영역이 겹치게 되면  \n",
    "    im2co로 전개한 후 원소 수가 원래 블록의 원소 수 보다 많아진다.  \n",
    "    그래서 im2col을 사용해 구현하며 메모미를 더 많이 소비한다는 단점이 있다.  \n",
    "    하지만 컴퓨터는 큰 행령을 묶어서 계산하는데 탁월하다.  \n",
    "    예를 들어 행렬 계산 라이브러러 등은 행렬 계산에 고도로 최적화되어 큰 행렬의 곱셈을 빠르게 계산 가능하다.  \n",
    "    그래서 문제를 행렬 계산으로 만들면 선형 대수 라이브러리를 활용해 효율을 높일 수 있다.  \n",
    "\n",
    ">\n",
    "    >im2col로 입력 데이터를 전개한 다음에는 합성곱 계층의 필터를 1열로 전개하고, 두 행렬의 곱을 계산하면 된다.  \n",
    "    이는 전결합 계층에서 한것과 거의 같다.  \n",
    "\n",
    "### 7-4.3 합성곱 계층 구현하기\n",
    ">\n",
    "    > 이 책에서는 im2col함수를 미리 만들어 제공한다. 사실 그 구현은 간단한 함수 10개 정도 묶은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from DLS_git_clone.common.util import im2col\n",
    "from DLS_git_clone.common.util import col2im\n",
    "\n",
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1,5,5,stride = 1, pad = 0)\n",
    "print(col1.shape) \n",
    "\n",
    "x2 = np.random.rand(10,3,7,7)\n",
    "col2 = im2col(x2,5,5,stride =1,pad = 0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "    >여기에서는 두 가지 예를 보여주고 있다.  \n",
    "      첫 번째 배치 크기가 1(데이터가 1개), 채널 3개 높이 너비가 7x7의 데이터이고,  \n",
    "      두 번째는 배치 크기만 10이고 나머지는 첫번째와 동일하다.  \n",
    "      im2co 함수를 적용한 두 경우 모두 2번째 차원의 원소는 75개이다.   \n",
    "      이 값은 필터의 원소 수와 같다. 또한 배치 크기가 1일때 im2col의 결과의 크기가 (9,75)이고  \n",
    "      10일 때는 그 10배인 (90,75) 크기의 데이터가 저장 된다.\n",
    "\n",
    ">   \n",
    "    > 이제 im2col을 사용하여 합성곱 계층을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self,W,b,stride = 1, pad = 0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride \n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self,x):\n",
    "        FN,C,FH,FW = self.W.shpae\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1+(H+2*self.pad - FH) /  self.stride)\n",
    "        out_w = int(1+(H+2*self.pad - FW) /  self.stride)\n",
    "        \n",
    "        col = im2col(x,FH,FW,self.stride,self,self.pad)\n",
    "        col_W = self.W.reshape (FN,-1).T\n",
    "        out = np.dot(col,col_W)+self.b\n",
    "        \n",
    "        out = out.reshape(N,out_h,out_w,-1).transpose(0,3,1,2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "    > 합성곱 계층은 필터, 편향 , 스트라이드, 패딩을 인수로 받아 초기화한다. \n",
    "    필터는 (FN,C,FH,FW)의 4차원 형상이다. 여기서 FN은 필터 개수, C는 채널, FH는 필터 높이,FW는 필터의 너비이다.  \n",
    "\n",
    ">   \n",
    "    > 앞의 합성곱 구현 코드에서 중요한 부분은 굴게 표시했다.  \n",
    "    이 부분에서 입력 데이터를 im2col로 전개하고 필터도 reshape을 사용해 2차원 배열로 전개한다.  \n",
    "    그리고 이렇게 전개한 두 행렬의 곱을 구한다.\n",
    ">\n",
    "    > 필터를 전개하는 부분은 그림 7-19에서 보듯 각 필터 블록을 1줄로 펼쳐 세운다.  \n",
    "    이때 reshape의 두번째 인수를 -1로 지정했는데, 이는 reshape 이 제공하는 편의 기능이다.  \n",
    "    reshape에 -1을 지정하면 다차원 배열의 원소 소구ㅏ 변환 후에도 똑같이 유지 되도록 적절히 묶어준다.  \n",
    "    앞의 코드에서 (10,3,5,5)형상을 한 다차원 배열 W의 원소수는 총 750개이다.  \n",
    "    이 배열에 reshape(10,-1)을 호출하면 750개의 원소를 10묶음으로 즉 형상이 (10,75)인 배열로 만들어준다.\n",
    "\n",
    ">\n",
    "    > 다음으로 forward 구현의 마지막에서 출력 데이터를 적절한 형상으로 바꿔준다. \n",
    "    이때 넘파이의 transpose함수를 사용하는데, 이는 다차원 배열의 축 순서를 바꿔주는 함수이다. 그림7-20을 참고\n",
    "\n",
    ">\n",
    "    > 이상이 합성곱 계층의 forward구현이다. im2col로 전개한 덕분에 전결합 계층의 Affine 계층과 거의 똑같이 구현할 수 있었다.  \n",
    "    다음은 합성곱 계층의 역전파를 구현할 차례지만, Affine 계층의 구현과 공통점 역시 많아 따로 설명하진 않겠다.  \n",
    "    주의 할 점으로는 합성곱 계층의 역전파에서는 in2col을 역으로 처리해야 한다. 이는 이 책이 제공하는 col2im을 사용하면 된다.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-4.4 풀링 계층 구현하기\n",
    "\n",
    ">\n",
    "    > 풀링 계층 구현도 합성곱 계층과 마찬가지로 im2col을 사용하여 입력 데이터를 전개한다.  \n",
    "    단, 풀링의 경우 채널 쪽이 독립적이라는 점이 합성곱 계층 때와 다르다.  \n",
    "    구체적으로는 그림 7-21과 같이 풀링 적용 영역을 채널마다 독립적으로 전개한다.  \n",
    "\n",
    "> \n",
    "    > 일단 그림과 같이 전개후 전개한 행렬에서 행별 최댓값을 구하고 적절하 형상으로 성형하기만 하면된다.\n",
    "    코드를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self,pool_h,pool_w,stride = 1, pad = 0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self. stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self,x):\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1+(H-self.pool_h)/self.stride) \n",
    "        out_w = int(1+(W-self.pool_w)/self.stride)\n",
    "        \n",
    "        #전개 -1\n",
    "        col = im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "        col = col.reshape(-1,self.pool_h*self.pool_w)\n",
    "        \n",
    "        #최댓값 -2\n",
    "        out = np.max(col, axis = 1)\n",
    "        \n",
    "        #성형 -3\n",
    "        out = out.reshape(N,out_h, out_w,C).transpose(0,3,1,2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "    \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "    > 풀링 계층은 다음과 같은 세 단계로 구현한다.  \n",
    "\n",
    "1. 입력 데이터를 전개한다.  \n",
    "2. 행별 최댓값을 구한다.  \n",
    "3. 적절한 모양으로 성형한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-5 CNN 구현해보기\n",
    "\n",
    ">\n",
    "    > 합성곱 계층과 풀링 계층을 구현했으니. 이 계층들을 조합하여 손글씨 숫자를 인식하는 CNN을 조립해보자.  \n",
    "    여기에서는 그림 7-23과 같이 구현한다.  \n",
    "    네트워크 흐름은 다음과 같다.\n",
    ">\n",
    "    > Conv - ReLU - Pooling - Affine - ReLU - Affine _Softmax\n",
    "\n",
    ">\n",
    "    > 우선 SimpleConvNet의 초기화 __init__을 살펴보자. 초기화 때는 다음 인수들을 받는다.  \n",
    "    1. input_dim - 입력 데이터(채널 수, 높이, 너비) 의 차원\n",
    "    2. conv_param - 합성곱 계층의 하이퍼파라미터(딕셔너리) 딕셔너리의 키는 다음과 같다  \n",
    "        - filter_num - 필터수  \n",
    "        - filter_size - 필터 크기    \n",
    "        - stride - 스트라이드  \n",
    "        - pad - 패딩\n",
    "    3. hidden _size\n",
    "    4. output_size  \n",
    "    5. weight_init_std   \n",
    "\n",
    ">\n",
    "    > 여기에서 합성곱 계층의 하이퍼파라미터는 딕셔너리 형태로 주어진다.  \n",
    "    이것은 필요한 하이퍼파라미터의 값이 예컨데 [filter_num:30,...]과 같이 저장된다는 것이다.  \n",
    "    \n",
    "    코드가 길어지므로 세부분으로 나눠 설명하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from DLS_git_clone.common.layers import*\n",
    "from DLS_git_clone.common.gradient import numerical_gradient\n",
    "class SimpleConvNet:\n",
    "    def __init__ (self,input_dim = (1,28,28),conv_param = {'filter_num':30,'filter_size':5, 'pad' :0, 'stride':1},hidden_size = 100, output_size = 10, weight_init_std = 0.01):\n",
    "        # 입력된 하이퍼파라미터 꺼내서 지정\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_num']\n",
    "        filter_pad = conv_param['filter_num']\n",
    "        filter_stride = conv_param['filter_num']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size-filter_size+2*filter_pad)/filter_stride + 1\n",
    "        pool_output_size = int(filter_num *(conv_output_size/2)*(conv_output_size/2))\n",
    "        \n",
    "        #매개변수 초기화\n",
    "        self.params ={}\n",
    "        self.params['W1'] = weight_init_std* np.random.randn(filter_num,input_dim[0],filter_size,filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std* np.random.randn(pool_output_size,hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std* np.random.randn(hidden_size,output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        #계층 생성\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],self.params['b1'],conv_param['stride'],conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h = 2, pool_w = 2, stride = 2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine1'] = Affine(self.params['W3'],self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer(y,t)\n",
    "    \n",
    "    def gradient(self,x,t):\n",
    "        #순전파\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        #역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        #결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행해보자 \n",
    "코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299439186535237\n",
      "=== epoch:1, train acc:0.354, test acc:0.364 ===\n",
      "train loss:2.295168108350172\n",
      "train loss:2.287531126020329\n",
      "train loss:2.283755385124487\n",
      "train loss:2.278317353884318\n",
      "train loss:2.2633684607426927\n",
      "train loss:2.254302239209231\n",
      "train loss:2.2146784113832596\n",
      "train loss:2.2008024580185053\n",
      "train loss:2.2000957259489313\n",
      "train loss:2.1481745961499716\n",
      "train loss:2.127522197595666\n",
      "train loss:2.0979233940200914\n",
      "train loss:2.0671824828164262\n",
      "train loss:2.00111781104453\n",
      "train loss:1.9917394216720212\n",
      "train loss:1.8683807804628472\n",
      "train loss:1.8250705033706363\n",
      "train loss:1.7886875784186678\n",
      "train loss:1.6644249630541763\n",
      "train loss:1.5216056887215077\n",
      "train loss:1.4853941945751858\n",
      "train loss:1.4121544615078536\n",
      "train loss:1.397860117076792\n",
      "train loss:1.1697840766956147\n",
      "train loss:1.115496883575166\n",
      "train loss:1.1026819196046573\n",
      "train loss:1.0550538071803959\n",
      "train loss:0.886154306315008\n",
      "train loss:0.9699471134281796\n",
      "train loss:0.9341959421692482\n",
      "train loss:0.9945685436506296\n",
      "train loss:0.8905713454088547\n",
      "train loss:0.7707127767456379\n",
      "train loss:0.7627099026249183\n",
      "train loss:0.7831387526466038\n",
      "train loss:0.7069975047692814\n",
      "train loss:0.6761989550300602\n",
      "train loss:0.6133600705083305\n",
      "train loss:0.5582821822897328\n",
      "train loss:0.6816501001355022\n",
      "train loss:0.596370316074281\n",
      "train loss:0.7536131638947814\n",
      "train loss:0.7651434054199037\n",
      "train loss:0.5223233915156312\n",
      "train loss:0.477850052940225\n",
      "train loss:0.4535990292245227\n",
      "train loss:0.7362358949814706\n",
      "train loss:0.6670057832404848\n",
      "train loss:0.5075593853529332\n",
      "train loss:0.5319735580198417\n",
      "train loss:0.481661500467363\n",
      "train loss:0.5116186157728554\n",
      "train loss:0.3852304200965688\n",
      "train loss:0.5888214083925991\n",
      "train loss:0.47401546506829945\n",
      "train loss:0.416643025522751\n",
      "train loss:0.5863854554237405\n",
      "train loss:0.41946674255811656\n",
      "train loss:0.5359510399848808\n",
      "train loss:0.5418435265922844\n",
      "train loss:0.33520861321568873\n",
      "train loss:0.3821448848857844\n",
      "train loss:0.515399761343258\n",
      "train loss:0.4462065395141699\n",
      "train loss:0.499858247361704\n",
      "train loss:0.341946226243773\n",
      "train loss:0.40407987478697716\n",
      "train loss:0.5136312657472205\n",
      "train loss:0.39367960836720217\n",
      "train loss:0.4399372132506201\n",
      "train loss:0.4458988320574867\n",
      "train loss:0.33354836418628553\n",
      "train loss:0.35164033066603906\n",
      "train loss:0.45509208234784526\n",
      "train loss:0.5379741247625696\n",
      "train loss:0.39735605110679956\n",
      "train loss:0.677629953257374\n",
      "train loss:0.39896168181141506\n",
      "train loss:0.6357064524744174\n",
      "train loss:0.42788095545327265\n",
      "train loss:0.4278790182444959\n",
      "train loss:0.4476983692194538\n",
      "train loss:0.3607463916736631\n",
      "train loss:0.6547309989355173\n",
      "train loss:0.39291502769340453\n",
      "train loss:0.3878219539939275\n",
      "train loss:0.3277386709175233\n",
      "train loss:0.19129309757376745\n",
      "train loss:0.4284052154365215\n",
      "train loss:0.4087715180376814\n",
      "train loss:0.3847549923254435\n",
      "train loss:0.4703737033360917\n",
      "train loss:0.5287825340272899\n",
      "train loss:0.5311365907448953\n",
      "train loss:0.48148983070374074\n",
      "train loss:0.38586487332489966\n",
      "train loss:0.35161503408526273\n",
      "train loss:0.4511539655806079\n",
      "train loss:0.581589802936763\n",
      "train loss:0.37267920953353995\n",
      "train loss:0.3979209580240833\n",
      "train loss:0.37026927123652503\n",
      "train loss:0.34575150671431054\n",
      "train loss:0.25234807542648674\n",
      "train loss:0.2598495930270133\n",
      "train loss:0.3823136903901714\n",
      "train loss:0.3286560184771632\n",
      "train loss:0.39695951091580783\n",
      "train loss:0.4093250832541815\n",
      "train loss:0.2749737348545308\n",
      "train loss:0.3235429183373282\n",
      "train loss:0.34277524559227907\n",
      "train loss:0.4108062787534113\n",
      "train loss:0.3649763277193575\n",
      "train loss:0.45172978458054464\n",
      "train loss:0.34828843314112995\n",
      "train loss:0.2719870243239506\n",
      "train loss:0.34624168466300553\n",
      "train loss:0.41156667364655\n",
      "train loss:0.48493752245566163\n",
      "train loss:0.3255266854857943\n",
      "train loss:0.47000291636351266\n",
      "train loss:0.32730538726098485\n",
      "train loss:0.3427891792741636\n",
      "train loss:0.3411039018071304\n",
      "train loss:0.3489297399903177\n",
      "train loss:0.31054155936924366\n",
      "train loss:0.3497197366904078\n",
      "train loss:0.3586428565841876\n",
      "train loss:0.26304415402599457\n",
      "train loss:0.3141337828053905\n",
      "train loss:0.22543262265858405\n",
      "train loss:0.31591028862410897\n",
      "train loss:0.2939814585463155\n",
      "train loss:0.2817553647583553\n",
      "train loss:0.2445835175579647\n",
      "train loss:0.3100958793569292\n",
      "train loss:0.2381402711925763\n",
      "train loss:0.38023325781796685\n",
      "train loss:0.2563705790776063\n",
      "train loss:0.4663418191588131\n",
      "train loss:0.28125518154066925\n",
      "train loss:0.2251166994057855\n",
      "train loss:0.22833210837789142\n",
      "train loss:0.49801343516819635\n",
      "train loss:0.23861967599710046\n",
      "train loss:0.2661705268825241\n",
      "train loss:0.3194250940270683\n",
      "train loss:0.5057640546173233\n",
      "train loss:0.46826042444346955\n",
      "train loss:0.33646027104380183\n",
      "train loss:0.30476513714689746\n",
      "train loss:0.25961385545882054\n",
      "train loss:0.38302138760734145\n",
      "train loss:0.4304203635250595\n",
      "train loss:0.22691806901273984\n",
      "train loss:0.2821005671156711\n",
      "train loss:0.39201472849252283\n",
      "train loss:0.34454796361211654\n",
      "train loss:0.2368709569161272\n",
      "train loss:0.40964585789808894\n",
      "train loss:0.3519735537641283\n",
      "train loss:0.2466535541642233\n",
      "train loss:0.5105825642903105\n",
      "train loss:0.20361801753928138\n",
      "train loss:0.35334013558005284\n",
      "train loss:0.2645316089091084\n",
      "train loss:0.37119681356975925\n",
      "train loss:0.2355716724261374\n",
      "train loss:0.25451556212100845\n",
      "train loss:0.328643940924729\n",
      "train loss:0.3425852651260627\n",
      "train loss:0.21435476348379265\n",
      "train loss:0.23562281553607314\n",
      "train loss:0.15892914730257526\n",
      "train loss:0.2833559912614055\n",
      "train loss:0.5013077678902973\n",
      "train loss:0.3614733565023378\n",
      "train loss:0.4899271143235699\n",
      "train loss:0.2762868023785517\n",
      "train loss:0.3361532486517635\n",
      "train loss:0.239236541979608\n",
      "train loss:0.338586030327891\n",
      "train loss:0.34677406874792865\n",
      "train loss:0.2856321468074268\n",
      "train loss:0.34128797308916725\n",
      "train loss:0.16950951979829895\n",
      "train loss:0.28095383168388766\n",
      "train loss:0.3160727391958085\n",
      "train loss:0.24351763242021068\n",
      "train loss:0.35817568381639775\n",
      "train loss:0.27643780123250883\n",
      "train loss:0.2530796541925531\n",
      "train loss:0.22563777279533184\n",
      "train loss:0.24256576712784558\n",
      "train loss:0.25406484517464845\n",
      "train loss:0.2475121275013975\n",
      "train loss:0.2976973639341244\n",
      "train loss:0.3694676563042297\n",
      "train loss:0.1890489100845268\n",
      "train loss:0.39863059974571585\n",
      "train loss:0.30288817395324286\n",
      "train loss:0.2810358979505217\n",
      "train loss:0.25873527354677944\n",
      "train loss:0.2847880921767973\n",
      "train loss:0.22773177612723244\n",
      "train loss:0.28998742215222767\n",
      "train loss:0.27248485692649316\n",
      "train loss:0.16619820742621458\n",
      "train loss:0.25047737069096065\n",
      "train loss:0.23135116789617446\n",
      "train loss:0.3282946208290926\n",
      "train loss:0.24015604759391593\n",
      "train loss:0.4264185729146077\n",
      "train loss:0.2717665390052758\n",
      "train loss:0.37193831063845273\n",
      "train loss:0.24996265031187678\n",
      "train loss:0.21166251430178384\n",
      "train loss:0.14323397998378437\n",
      "train loss:0.3381002752943857\n",
      "train loss:0.29672331441326244\n",
      "train loss:0.26242714820340224\n",
      "train loss:0.5252370164267517\n",
      "train loss:0.20316336498884635\n",
      "train loss:0.31405654660174526\n",
      "train loss:0.4444775388543778\n",
      "train loss:0.26820690233111405\n",
      "train loss:0.25054486889428945\n",
      "train loss:0.22869270970309827\n",
      "train loss:0.24240856873943367\n",
      "train loss:0.17927170576243676\n",
      "train loss:0.24805459060780827\n",
      "train loss:0.23629254253989412\n",
      "train loss:0.18697769390411392\n",
      "train loss:0.33463741140304876\n",
      "train loss:0.30731760749291337\n",
      "train loss:0.16478632749492855\n",
      "train loss:0.25887776542775764\n",
      "train loss:0.24799129978258386\n",
      "train loss:0.21944576675115848\n",
      "train loss:0.217980739583291\n",
      "train loss:0.38474106954861836\n",
      "train loss:0.37452151499262554\n",
      "train loss:0.28981254122549827\n",
      "train loss:0.22499778787565045\n",
      "train loss:0.26071448851207074\n",
      "train loss:0.2660405372387232\n",
      "train loss:0.23988489368673802\n",
      "train loss:0.21718080711790005\n",
      "train loss:0.21691311149900802\n",
      "train loss:0.3972201418937347\n",
      "train loss:0.44333115380027394\n",
      "train loss:0.18469000522323945\n",
      "train loss:0.2792043277698279\n",
      "train loss:0.29498862859874186\n",
      "train loss:0.30695283699303855\n",
      "train loss:0.17743796413622664\n",
      "train loss:0.13346693915315128\n",
      "train loss:0.2618214652076368\n",
      "train loss:0.3344532714728769\n",
      "train loss:0.3034781083273258\n",
      "train loss:0.24488509524699356\n",
      "train loss:0.21270209875445278\n",
      "train loss:0.3193291484856184\n",
      "train loss:0.19571111929401655\n",
      "train loss:0.2575852917071415\n",
      "train loss:0.22396287892064895\n",
      "train loss:0.21520316597702546\n",
      "train loss:0.17184445009882007\n",
      "train loss:0.3269595208429212\n",
      "train loss:0.26558180055319147\n",
      "train loss:0.12554452570400015\n",
      "train loss:0.14607249374472028\n",
      "train loss:0.27289366715074953\n",
      "train loss:0.27534130566426473\n",
      "train loss:0.222067216872475\n",
      "train loss:0.27118057001927787\n",
      "train loss:0.24926277210060424\n",
      "train loss:0.21450239467235604\n",
      "train loss:0.23013371036898028\n",
      "train loss:0.19515961145377436\n",
      "train loss:0.11559799401579422\n",
      "train loss:0.19982272312858465\n",
      "train loss:0.31860420697083464\n",
      "train loss:0.20599969223042508\n",
      "train loss:0.269660171347357\n",
      "train loss:0.18664126746405302\n",
      "train loss:0.2393074286420173\n",
      "train loss:0.17748709822833322\n",
      "train loss:0.2701117033465019\n",
      "train loss:0.4278922444335124\n",
      "train loss:0.17110652647309813\n",
      "train loss:0.2835494987465486\n",
      "train loss:0.4088536255686098\n",
      "train loss:0.3085562700593887\n",
      "train loss:0.2428423659864396\n",
      "train loss:0.36374748174922966\n",
      "train loss:0.2625303196659162\n",
      "train loss:0.26089267207448313\n",
      "train loss:0.2348777992264888\n",
      "train loss:0.26482002826799006\n",
      "train loss:0.12746917271109587\n",
      "train loss:0.1926241918749071\n",
      "train loss:0.35009372361586927\n",
      "train loss:0.3110550867838197\n",
      "train loss:0.2384969490026621\n",
      "train loss:0.17417711046005147\n",
      "train loss:0.1450835545021987\n",
      "train loss:0.14184251969352932\n",
      "train loss:0.24418610149443887\n",
      "train loss:0.2196564573851628\n",
      "train loss:0.3467073489439229\n",
      "train loss:0.18017890996370864\n",
      "train loss:0.24209390824492136\n",
      "train loss:0.1422427868307787\n",
      "train loss:0.14381129512041133\n",
      "train loss:0.18558140919372434\n",
      "train loss:0.2551601656335519\n",
      "train loss:0.24516805529186683\n",
      "train loss:0.17107028165850238\n",
      "train loss:0.13364024692462742\n",
      "train loss:0.20214253517524702\n",
      "train loss:0.22080537473951378\n",
      "train loss:0.2316001804248692\n",
      "train loss:0.1638166551925951\n",
      "train loss:0.20250747403381233\n",
      "train loss:0.1741614223784425\n",
      "train loss:0.1774763771373561\n",
      "train loss:0.13762775859871015\n",
      "train loss:0.31146204425754886\n",
      "train loss:0.17795844006677666\n",
      "train loss:0.35025086440472014\n",
      "train loss:0.3041665504359781\n",
      "train loss:0.18171422264051415\n",
      "train loss:0.11159618592923869\n",
      "train loss:0.30719692422076555\n",
      "train loss:0.1827004247452915\n",
      "train loss:0.15191278694578325\n",
      "train loss:0.23193754134621736\n",
      "train loss:0.23218798551049094\n",
      "train loss:0.29211501112128063\n",
      "train loss:0.11524885872654622\n",
      "train loss:0.2030463237164059\n",
      "train loss:0.17063939127322394\n",
      "train loss:0.2358806317322768\n",
      "train loss:0.14882103400047916\n",
      "train loss:0.17430804503346128\n",
      "train loss:0.26704791250160104\n",
      "train loss:0.27358405696303995\n",
      "train loss:0.4105273329657817\n",
      "train loss:0.20451314715242616\n",
      "train loss:0.22271831946055726\n",
      "train loss:0.1360419089945628\n",
      "train loss:0.187890875353021\n",
      "train loss:0.13640370013193415\n",
      "train loss:0.2556839328593895\n",
      "train loss:0.14369347569971583\n",
      "train loss:0.2141933744258424\n",
      "train loss:0.11979217747722318\n",
      "train loss:0.19152725564400705\n",
      "train loss:0.24925676961254623\n",
      "train loss:0.21405689280495996\n",
      "train loss:0.30243357008942107\n",
      "train loss:0.20748112173365402\n",
      "train loss:0.2575748943395638\n",
      "train loss:0.19798889723103819\n",
      "train loss:0.1643831156446174\n",
      "train loss:0.1960426551163815\n",
      "train loss:0.24010157857093659\n",
      "train loss:0.2413244042596037\n",
      "train loss:0.24276102089749269\n",
      "train loss:0.09441835248692816\n",
      "train loss:0.08514205950563544\n",
      "train loss:0.156431222790481\n",
      "train loss:0.2101006651076204\n",
      "train loss:0.20338296719324758\n",
      "train loss:0.13509168389034656\n",
      "train loss:0.10025255182461094\n",
      "train loss:0.14714888444069066\n",
      "train loss:0.28221374247452513\n",
      "train loss:0.1783194122139833\n",
      "train loss:0.208163427522935\n",
      "train loss:0.21944660188047613\n",
      "train loss:0.12459911850287707\n",
      "train loss:0.2391817859906312\n",
      "train loss:0.14065128742105645\n",
      "train loss:0.07730202372269569\n",
      "train loss:0.20358028386608903\n",
      "train loss:0.24100866121595804\n",
      "train loss:0.16431487611557113\n",
      "train loss:0.26997211615860983\n",
      "train loss:0.18686954192938518\n",
      "train loss:0.14042137480514663\n",
      "train loss:0.24073502665189672\n",
      "train loss:0.16636065972544345\n",
      "train loss:0.1944844303014735\n",
      "train loss:0.19068135690416024\n",
      "train loss:0.17935573513957362\n",
      "train loss:0.2833848524349182\n",
      "train loss:0.10361865253959396\n",
      "train loss:0.2173978407861167\n",
      "train loss:0.18154565440976803\n",
      "train loss:0.1776967584886556\n",
      "train loss:0.10174288401552004\n",
      "train loss:0.23204941696354872\n",
      "train loss:0.18032923373963666\n",
      "train loss:0.1803881408818095\n",
      "train loss:0.13462777162176282\n",
      "train loss:0.16525536325670098\n",
      "train loss:0.11428715930316231\n",
      "train loss:0.14363625484622136\n",
      "train loss:0.2021957168506463\n",
      "train loss:0.14121632010535226\n",
      "train loss:0.07353389927999625\n",
      "train loss:0.1621738135197418\n",
      "train loss:0.1964743588139533\n",
      "train loss:0.15211369923129245\n",
      "train loss:0.08060028767947795\n",
      "train loss:0.29803780942195096\n",
      "train loss:0.11313707430559601\n",
      "train loss:0.13077012185703724\n",
      "train loss:0.05039383461667489\n",
      "train loss:0.1640609835868274\n",
      "train loss:0.09175194052236221\n",
      "train loss:0.11280328632260125\n",
      "train loss:0.16684148767962806\n",
      "train loss:0.1138505653991273\n",
      "train loss:0.25030423868701324\n",
      "train loss:0.1686661317817011\n",
      "train loss:0.2122241040621057\n",
      "train loss:0.2869133211298426\n",
      "train loss:0.16325780998475625\n",
      "train loss:0.06301363405302018\n",
      "train loss:0.213027791219796\n",
      "train loss:0.133790054569515\n",
      "train loss:0.253307597929782\n",
      "train loss:0.08628680519934973\n",
      "train loss:0.1972458208960013\n",
      "train loss:0.17378549587597958\n",
      "train loss:0.1661917921744664\n",
      "train loss:0.19947954631235765\n",
      "train loss:0.15186657097050685\n",
      "train loss:0.22109196965208505\n",
      "train loss:0.09060668139841127\n",
      "train loss:0.1429938227958899\n",
      "train loss:0.10650237815425749\n",
      "train loss:0.13225775228866662\n",
      "train loss:0.09375736767474165\n",
      "train loss:0.23329960158706986\n",
      "train loss:0.3744258458859051\n",
      "train loss:0.21242097513925212\n",
      "train loss:0.10561265249610488\n",
      "train loss:0.2828330021648802\n",
      "train loss:0.25157091541169446\n",
      "train loss:0.20430518590931537\n",
      "train loss:0.18206567658494346\n",
      "train loss:0.15942278059365525\n",
      "train loss:0.12377767627105313\n",
      "train loss:0.20529401136637773\n",
      "train loss:0.1546463310273506\n",
      "train loss:0.24098149628359689\n",
      "train loss:0.08569458670012166\n",
      "train loss:0.21944288893109198\n",
      "train loss:0.14271492098257046\n",
      "train loss:0.12334207886794481\n",
      "train loss:0.08272841223502167\n",
      "train loss:0.13637896773947006\n",
      "train loss:0.1996368904299083\n",
      "train loss:0.13976521142894588\n",
      "train loss:0.09801576557142544\n",
      "train loss:0.11361795150487422\n",
      "train loss:0.15798679735166624\n",
      "train loss:0.177646971448599\n",
      "train loss:0.22266269442880138\n",
      "train loss:0.08139182036514299\n",
      "train loss:0.10600025639199967\n",
      "train loss:0.1533465721777888\n",
      "train loss:0.04617077583546253\n",
      "train loss:0.15328252226153596\n",
      "train loss:0.1899753078030736\n",
      "train loss:0.12810681101793298\n",
      "train loss:0.08599639136480831\n",
      "train loss:0.14344390075343344\n",
      "train loss:0.09983198293617286\n",
      "train loss:0.18074791154463998\n",
      "train loss:0.06876309114727105\n",
      "train loss:0.1417915534298352\n",
      "train loss:0.22117031373990628\n",
      "train loss:0.19836370204850184\n",
      "train loss:0.061021512844273304\n",
      "train loss:0.13246243492989607\n",
      "train loss:0.0776559242001326\n",
      "train loss:0.1474086132728835\n",
      "train loss:0.10188023131454264\n",
      "train loss:0.04381940918240112\n",
      "train loss:0.2092839865650337\n",
      "train loss:0.0886560053667077\n",
      "train loss:0.04977274972206984\n",
      "train loss:0.19617329554064028\n",
      "train loss:0.20364807224905554\n",
      "train loss:0.1498719253315428\n",
      "train loss:0.23065714942928753\n",
      "train loss:0.2879790951810802\n",
      "train loss:0.19138899529912265\n",
      "train loss:0.15330969416772156\n",
      "train loss:0.18461297205307686\n",
      "train loss:0.1904199807479493\n",
      "train loss:0.17533818871973136\n",
      "train loss:0.27267178524068464\n",
      "train loss:0.19474684427137093\n",
      "train loss:0.13290306235946586\n",
      "train loss:0.1451523909114352\n",
      "train loss:0.22599507867724372\n",
      "train loss:0.1165075154820909\n",
      "train loss:0.09913629834753397\n",
      "train loss:0.2936982523861281\n",
      "train loss:0.08657213261214367\n",
      "train loss:0.1542491591589888\n",
      "train loss:0.17624872159678998\n",
      "train loss:0.09308860618351583\n",
      "train loss:0.10542427666221649\n",
      "train loss:0.07825226010806677\n",
      "train loss:0.14282728559083174\n",
      "train loss:0.06409088423372633\n",
      "train loss:0.13423595159325152\n",
      "train loss:0.10926410980351063\n",
      "train loss:0.08428798301960415\n",
      "train loss:0.11114728433653205\n",
      "train loss:0.14732764122851846\n",
      "train loss:0.13105654567171893\n",
      "train loss:0.11840681458611257\n",
      "train loss:0.10941730622473658\n",
      "train loss:0.13860986224787747\n",
      "train loss:0.06706471706756562\n",
      "train loss:0.08836697123219472\n",
      "train loss:0.1547345242432525\n",
      "train loss:0.17268480205068848\n",
      "train loss:0.11898492253184831\n",
      "train loss:0.07325437526166395\n",
      "train loss:0.1417157361723884\n",
      "train loss:0.14209276159066594\n",
      "train loss:0.1644385286883907\n",
      "train loss:0.11938667815913055\n",
      "train loss:0.15928315424484443\n",
      "train loss:0.11186027172754448\n",
      "train loss:0.16831605099585129\n",
      "train loss:0.08867061478643157\n",
      "train loss:0.07099111396476981\n",
      "train loss:0.08712386654897963\n",
      "train loss:0.11984115636239455\n",
      "train loss:0.15136671392784562\n",
      "train loss:0.10588470508379612\n",
      "train loss:0.1012464046800079\n",
      "train loss:0.21834790474805993\n",
      "train loss:0.16253792522377583\n",
      "train loss:0.08887989322144206\n",
      "train loss:0.1152077255364156\n",
      "train loss:0.07621150417344802\n",
      "train loss:0.11344866338321433\n",
      "train loss:0.09928965225552686\n",
      "train loss:0.10954730698020959\n",
      "train loss:0.12232801691567129\n",
      "train loss:0.0868168378089182\n",
      "train loss:0.09866582645505785\n",
      "train loss:0.13727608522983636\n",
      "train loss:0.05053791873465676\n",
      "train loss:0.2493281517830251\n",
      "train loss:0.07177719043214516\n",
      "train loss:0.10200264516018677\n",
      "train loss:0.07835418274151573\n",
      "train loss:0.12693820835520686\n",
      "train loss:0.07959605385444973\n",
      "train loss:0.088407518016951\n",
      "train loss:0.1011705424826192\n",
      "train loss:0.219676457054147\n",
      "train loss:0.18703936565021856\n",
      "train loss:0.07068465650398567\n",
      "train loss:0.10330856572962877\n",
      "train loss:0.07643075000004369\n",
      "train loss:0.057862985826422936\n",
      "train loss:0.09594764607699002\n",
      "train loss:0.19152035425133743\n",
      "train loss:0.13236008132535346\n",
      "train loss:0.11356837991388176\n",
      "train loss:0.14447435272309236\n",
      "train loss:0.1878772147277595\n",
      "train loss:0.12435560983521451\n",
      "train loss:0.110777467661643\n",
      "train loss:0.058562879441078824\n",
      "train loss:0.0794417642168452\n",
      "train loss:0.14707343986804974\n",
      "train loss:0.15974870099508137\n",
      "train loss:0.19970522922856063\n",
      "train loss:0.06109825054293843\n",
      "train loss:0.10253020997049032\n",
      "train loss:0.14537603249767111\n",
      "train loss:0.13597347500625825\n",
      "train loss:0.08232206390595008\n",
      "train loss:0.13672441966337992\n",
      "train loss:0.1358169369962353\n",
      "=== epoch:2, train acc:0.969, test acc:0.964 ===\n",
      "train loss:0.09983751993029905\n",
      "train loss:0.08693007169995288\n",
      "train loss:0.06226541494214865\n",
      "train loss:0.15605077760753164\n",
      "train loss:0.19191280494686283\n",
      "train loss:0.14308936071805128\n",
      "train loss:0.11299847407252744\n",
      "train loss:0.055277921549650076\n",
      "train loss:0.08502444795247077\n",
      "train loss:0.15520283040004462\n",
      "train loss:0.1105582424762677\n",
      "train loss:0.07500340865738594\n",
      "train loss:0.05469610097680264\n",
      "train loss:0.10430398739891902\n",
      "train loss:0.239601310989124\n",
      "train loss:0.16123696828729922\n",
      "train loss:0.06942133463220555\n",
      "train loss:0.06954069539769507\n",
      "train loss:0.07615680615585672\n",
      "train loss:0.18026264757600444\n",
      "train loss:0.16951693353709646\n",
      "train loss:0.12332497362373374\n",
      "train loss:0.13513331928088945\n",
      "train loss:0.10698816089721175\n",
      "train loss:0.06799877685083922\n",
      "train loss:0.10727268324446505\n",
      "train loss:0.11746517300222081\n",
      "train loss:0.16763563285035613\n",
      "train loss:0.10907499206518477\n",
      "train loss:0.08353727072807152\n",
      "train loss:0.25574492167861584\n",
      "train loss:0.05541829549677026\n",
      "train loss:0.16963348270332204\n",
      "train loss:0.10675375102405252\n",
      "train loss:0.04540318217025461\n",
      "train loss:0.12415807280255663\n",
      "train loss:0.052608029329903036\n",
      "train loss:0.12962050530272076\n",
      "train loss:0.12482532630054692\n",
      "train loss:0.09013184935766205\n",
      "train loss:0.07769525965330637\n",
      "train loss:0.13571329460505438\n",
      "train loss:0.09939557377660861\n",
      "train loss:0.1012468699785935\n",
      "train loss:0.12215697213720622\n",
      "train loss:0.17188098593581208\n",
      "train loss:0.13656669205235594\n",
      "train loss:0.10566674779739024\n",
      "train loss:0.047796655924843376\n",
      "train loss:0.05625829221351052\n",
      "train loss:0.09016161556366534\n",
      "train loss:0.07703339745216475\n",
      "train loss:0.0663494688219291\n",
      "train loss:0.16753888156755828\n",
      "train loss:0.14205713843123\n",
      "train loss:0.04186556279907323\n",
      "train loss:0.10342046197753668\n",
      "train loss:0.03847062253020173\n",
      "train loss:0.15746838881057676\n",
      "train loss:0.06901537160572659\n",
      "train loss:0.12774777383092878\n",
      "train loss:0.13277510258863304\n",
      "train loss:0.14197813231168502\n",
      "train loss:0.13580723213754917\n",
      "train loss:0.04894318690732482\n",
      "train loss:0.15457805125929963\n",
      "train loss:0.1119651906127514\n",
      "train loss:0.053897584800385555\n",
      "train loss:0.1870463551663675\n",
      "train loss:0.06222760798085306\n",
      "train loss:0.04415399346689944\n",
      "train loss:0.04634155200988022\n",
      "train loss:0.04194252147169328\n",
      "train loss:0.0637209791356305\n",
      "train loss:0.20059554733130822\n",
      "train loss:0.02823578295815303\n",
      "train loss:0.19427396386405038\n",
      "train loss:0.12320404698162259\n",
      "train loss:0.08305313486312613\n",
      "train loss:0.05845685359820941\n",
      "train loss:0.09163864205502684\n",
      "train loss:0.08027831671204996\n",
      "train loss:0.045853412233109854\n",
      "train loss:0.1163210715132197\n",
      "train loss:0.13229290510432778\n",
      "train loss:0.05063013175255193\n",
      "train loss:0.1170777081908297\n",
      "train loss:0.11044987274708092\n",
      "train loss:0.16443681108921207\n",
      "train loss:0.15069645840161094\n",
      "train loss:0.07802621967491624\n",
      "train loss:0.0617006697115781\n",
      "train loss:0.08449187612306085\n",
      "train loss:0.08609405365844504\n",
      "train loss:0.14257687051267623\n",
      "train loss:0.05972527933352114\n",
      "train loss:0.09812413754600537\n",
      "train loss:0.11354079677612396\n",
      "train loss:0.1230857089795109\n",
      "train loss:0.11054579492622489\n",
      "train loss:0.158892596928965\n",
      "train loss:0.06424224578875964\n",
      "train loss:0.18850366640068958\n",
      "train loss:0.09972948279226941\n",
      "train loss:0.08717947154308407\n",
      "train loss:0.0937433565655715\n",
      "train loss:0.07839168668132097\n",
      "train loss:0.05522328322844215\n",
      "train loss:0.08537082263027576\n",
      "train loss:0.080599457672495\n",
      "train loss:0.21340150946517406\n",
      "train loss:0.048342999900741424\n",
      "train loss:0.053362653547112476\n",
      "train loss:0.2658817226793261\n",
      "train loss:0.09870367233553382\n",
      "train loss:0.05106966946013824\n",
      "train loss:0.10588227311875938\n",
      "train loss:0.07825906307095284\n",
      "train loss:0.12596388915121587\n",
      "train loss:0.14280760959838792\n",
      "train loss:0.12702986988960452\n",
      "train loss:0.07532734711350807\n",
      "train loss:0.15992053990495825\n",
      "train loss:0.17998521112021418\n",
      "train loss:0.16565773250339474\n",
      "train loss:0.06561467888060402\n",
      "train loss:0.11828861312592258\n",
      "train loss:0.06166633924819043\n",
      "train loss:0.09203443635421371\n",
      "train loss:0.129918225099511\n",
      "train loss:0.06876000649822295\n",
      "train loss:0.1637572615500284\n",
      "train loss:0.029655502414359075\n",
      "train loss:0.05344604879037625\n",
      "train loss:0.13316724874122238\n",
      "train loss:0.07622141372704017\n",
      "train loss:0.15890564018615666\n",
      "train loss:0.06989969812267756\n",
      "train loss:0.03995350906289122\n",
      "train loss:0.043276385368404016\n",
      "train loss:0.07828249633236385\n",
      "train loss:0.16114380051481408\n",
      "train loss:0.0584494463290008\n",
      "train loss:0.14183873403973768\n",
      "train loss:0.08288095571894359\n",
      "train loss:0.1129586280282915\n",
      "train loss:0.1676051603968968\n",
      "train loss:0.15133455253942102\n",
      "train loss:0.08989634826825472\n",
      "train loss:0.031006078900122\n",
      "train loss:0.048473217736153276\n",
      "train loss:0.16678285794819533\n",
      "train loss:0.07539607856717676\n",
      "train loss:0.04210425351556376\n",
      "train loss:0.054807758225926255\n",
      "train loss:0.11877008732370102\n",
      "train loss:0.07642650545085994\n",
      "train loss:0.0567127127851654\n",
      "train loss:0.08544403750231291\n",
      "train loss:0.05032353229898984\n",
      "train loss:0.051598295543378914\n",
      "train loss:0.06701187283081946\n",
      "train loss:0.1608238524818292\n",
      "train loss:0.09521849455984233\n",
      "train loss:0.07833921212150105\n",
      "train loss:0.22394914814212508\n",
      "train loss:0.07915427397000208\n",
      "train loss:0.13394021168396264\n",
      "train loss:0.0642845610021856\n",
      "train loss:0.06842184714211968\n",
      "train loss:0.03743909168272405\n",
      "train loss:0.11345813269050721\n",
      "train loss:0.06994852296214975\n",
      "train loss:0.11662771248958363\n",
      "train loss:0.06102774229225538\n",
      "train loss:0.12805481944061278\n",
      "train loss:0.09523090330286005\n",
      "train loss:0.08445492067037058\n",
      "train loss:0.10325682665579167\n",
      "train loss:0.11110583153665987\n",
      "train loss:0.0668354204477035\n",
      "train loss:0.08319163487599722\n",
      "train loss:0.14472217170926494\n",
      "train loss:0.11823947458226984\n",
      "train loss:0.04010143712314883\n",
      "train loss:0.10849952012029798\n",
      "train loss:0.05630443972932425\n",
      "train loss:0.04499733390331264\n",
      "train loss:0.10669190597643341\n",
      "train loss:0.09190057104317452\n",
      "train loss:0.08796137676299214\n",
      "train loss:0.09452453926107653\n",
      "train loss:0.0874578067645849\n",
      "train loss:0.21431187021610928\n",
      "train loss:0.059603291652463024\n",
      "train loss:0.11289980666630532\n",
      "train loss:0.07945503136541379\n",
      "train loss:0.07598923305603736\n",
      "train loss:0.07716374713078811\n",
      "train loss:0.08714814139185643\n",
      "train loss:0.10819806017784683\n",
      "train loss:0.16617455955272145\n",
      "train loss:0.07885995040328264\n",
      "train loss:0.030349975412285467\n",
      "train loss:0.07070264885160077\n",
      "train loss:0.07774977714115647\n",
      "train loss:0.09362621786969687\n",
      "train loss:0.08579392090194475\n",
      "train loss:0.15387212047311027\n",
      "train loss:0.06259904555359327\n",
      "train loss:0.09160100676368482\n",
      "train loss:0.058678341971130366\n",
      "train loss:0.06174213656279062\n",
      "train loss:0.11601232951444553\n",
      "train loss:0.10043883465081396\n",
      "train loss:0.04534341804945687\n",
      "train loss:0.11401499388190008\n",
      "train loss:0.09802099129774328\n",
      "train loss:0.11210003238509665\n",
      "train loss:0.10288799637640801\n",
      "train loss:0.09500468359770066\n",
      "train loss:0.06845133677572181\n",
      "train loss:0.03454519558452208\n",
      "train loss:0.07063561836754077\n",
      "train loss:0.24414020578458334\n",
      "train loss:0.028827176480959768\n",
      "train loss:0.02843935713395968\n",
      "train loss:0.07832665847797965\n",
      "train loss:0.12057543943633801\n",
      "train loss:0.07274201000445517\n",
      "train loss:0.12966242002511869\n",
      "train loss:0.1441582235111193\n",
      "train loss:0.2355786963710217\n",
      "train loss:0.07978346008177331\n",
      "train loss:0.07430248741815383\n",
      "train loss:0.12488142070810512\n",
      "train loss:0.06984547457473031\n",
      "train loss:0.13276137485003062\n",
      "train loss:0.060659377808869375\n",
      "train loss:0.15382812309234992\n",
      "train loss:0.09146836615631672\n",
      "train loss:0.0989468695980506\n",
      "train loss:0.038306162630075\n",
      "train loss:0.07613361918244152\n",
      "train loss:0.06782330997900037\n",
      "train loss:0.03292870694134848\n",
      "train loss:0.0385252496568246\n",
      "train loss:0.11831974172395872\n",
      "train loss:0.03943937211711721\n",
      "train loss:0.09202690300774366\n",
      "train loss:0.13612138361597076\n",
      "train loss:0.1268407359290165\n",
      "train loss:0.04252304416042809\n",
      "train loss:0.05635490656050287\n",
      "train loss:0.1219464659768591\n",
      "train loss:0.05759813161053841\n",
      "train loss:0.09172478068313865\n",
      "train loss:0.04763471680608285\n",
      "train loss:0.06028778231775309\n",
      "train loss:0.11095471175643432\n",
      "train loss:0.058640498970114925\n",
      "train loss:0.04151435015002674\n",
      "train loss:0.040103177437745566\n",
      "train loss:0.04062595585386895\n",
      "train loss:0.041179902294244924\n",
      "train loss:0.0846307714375507\n",
      "train loss:0.1550533880242658\n",
      "train loss:0.22431328283913476\n",
      "train loss:0.0404834514120924\n",
      "train loss:0.12671500192824758\n",
      "train loss:0.10232651230383395\n",
      "train loss:0.08201381950309601\n",
      "train loss:0.09507227507086634\n",
      "train loss:0.09108586564024625\n",
      "train loss:0.0799884823917945\n",
      "train loss:0.13182203165135625\n",
      "train loss:0.0931350890309525\n",
      "train loss:0.1061115798347728\n",
      "train loss:0.05038287485987518\n",
      "train loss:0.09805594179371582\n",
      "train loss:0.03588525501053383\n",
      "train loss:0.07016005235109726\n",
      "train loss:0.1132298894532901\n",
      "train loss:0.07030229249017357\n",
      "train loss:0.10189331127288993\n",
      "train loss:0.06597639171702423\n",
      "train loss:0.07124634640609191\n",
      "train loss:0.19049664379119075\n",
      "train loss:0.0756348813412914\n",
      "train loss:0.01955977734764063\n",
      "train loss:0.11344323823049182\n",
      "train loss:0.04502293402645776\n",
      "train loss:0.03928957930726689\n",
      "train loss:0.11100889265102654\n",
      "train loss:0.12605534802800356\n",
      "train loss:0.10364218129857017\n",
      "train loss:0.04817347640675444\n",
      "train loss:0.04508788719262115\n",
      "train loss:0.05599386346708157\n",
      "train loss:0.04826350179239574\n",
      "train loss:0.0355820907799048\n",
      "train loss:0.10948116289336704\n",
      "train loss:0.12046745288265732\n",
      "train loss:0.03812588380298812\n",
      "train loss:0.03425066977949104\n",
      "train loss:0.056309906084063534\n",
      "train loss:0.11983032192299206\n",
      "train loss:0.06639113365253536\n",
      "train loss:0.028073132823551103\n",
      "train loss:0.05833649881595612\n",
      "train loss:0.01509133840062139\n",
      "train loss:0.056194309074956234\n",
      "train loss:0.09249948589644966\n",
      "train loss:0.07431461704711186\n",
      "train loss:0.08754324701286195\n",
      "train loss:0.11039022345596403\n",
      "train loss:0.0399281687246393\n",
      "train loss:0.06540839241028974\n",
      "train loss:0.12276454185420863\n",
      "train loss:0.05643797299327146\n",
      "train loss:0.13487518210351607\n",
      "train loss:0.07948220437810097\n",
      "train loss:0.14150568038571554\n",
      "train loss:0.0564443956193284\n",
      "train loss:0.0774797578041325\n",
      "train loss:0.0531382899942556\n",
      "train loss:0.10781852445478737\n",
      "train loss:0.08474448010144182\n",
      "train loss:0.048777199131588206\n",
      "train loss:0.11130180278884862\n",
      "train loss:0.0379629915356161\n",
      "train loss:0.026221975767965932\n",
      "train loss:0.10588546285956708\n",
      "train loss:0.11227327161045512\n",
      "train loss:0.07135704566029688\n",
      "train loss:0.05389660980654824\n",
      "train loss:0.07463924930726891\n",
      "train loss:0.1524530805611133\n",
      "train loss:0.05065467453772099\n",
      "train loss:0.1901104936268101\n",
      "train loss:0.0304256999802681\n",
      "train loss:0.11389007956914055\n",
      "train loss:0.06378733057590787\n",
      "train loss:0.04712685207084525\n",
      "train loss:0.06489364167570512\n",
      "train loss:0.07749846821049795\n",
      "train loss:0.067342782613694\n",
      "train loss:0.0747159091867463\n",
      "train loss:0.04871572571631778\n",
      "train loss:0.1168634981522942\n",
      "train loss:0.09757779628176523\n",
      "train loss:0.1011941203053584\n",
      "train loss:0.056266373856296376\n",
      "train loss:0.0889519447931682\n",
      "train loss:0.07066596807546568\n",
      "train loss:0.05536877696169238\n",
      "train loss:0.11630879462724694\n",
      "train loss:0.1147735475379791\n",
      "train loss:0.09272989071368073\n",
      "train loss:0.1038930413378559\n",
      "train loss:0.06247367519499557\n",
      "train loss:0.04413169276795603\n",
      "train loss:0.05907904237007831\n",
      "train loss:0.08756305499772284\n",
      "train loss:0.03986455619919542\n",
      "train loss:0.09985855191695345\n",
      "train loss:0.0455917907812165\n",
      "train loss:0.06602794596979958\n",
      "train loss:0.16085289170067665\n",
      "train loss:0.04814356588941687\n",
      "train loss:0.038664078687987956\n",
      "train loss:0.15944475120144613\n",
      "train loss:0.10633236244572326\n",
      "train loss:0.05111119939131561\n",
      "train loss:0.12771358523897094\n",
      "train loss:0.06659426271170604\n",
      "train loss:0.05035278419505637\n",
      "train loss:0.028798510448801863\n",
      "train loss:0.06717253419887427\n",
      "train loss:0.023056785839249\n",
      "train loss:0.05476486949191425\n",
      "train loss:0.061358115689968636\n",
      "train loss:0.03839582101158953\n",
      "train loss:0.17446162903422718\n",
      "train loss:0.01688122309246408\n",
      "train loss:0.0571681279452197\n",
      "train loss:0.07667595502807623\n",
      "train loss:0.11670327721846303\n",
      "train loss:0.07676192591598159\n",
      "train loss:0.12266912027454227\n",
      "train loss:0.07270559691508638\n",
      "train loss:0.07208240810120482\n",
      "train loss:0.07521411269712484\n",
      "train loss:0.23149914694195683\n",
      "train loss:0.06043635468673953\n",
      "train loss:0.06380758213535369\n",
      "train loss:0.13000197212291012\n",
      "train loss:0.03458377577230437\n",
      "train loss:0.17920848104048726\n",
      "train loss:0.05624991504333018\n",
      "train loss:0.06894895286104122\n",
      "train loss:0.06119356448373875\n",
      "train loss:0.048693609463204536\n",
      "train loss:0.09279730060527229\n",
      "train loss:0.052129711053628965\n",
      "train loss:0.08962501528679048\n",
      "train loss:0.08783700903401527\n",
      "train loss:0.09804516141054702\n",
      "train loss:0.07275882155990368\n",
      "train loss:0.11659926966096144\n",
      "train loss:0.1069903205016241\n",
      "train loss:0.08359746029691338\n",
      "train loss:0.09063598819492873\n",
      "train loss:0.05345794509050746\n",
      "train loss:0.03683577401611602\n",
      "train loss:0.050603186935015323\n",
      "train loss:0.02776143984200195\n",
      "train loss:0.05968100702387769\n",
      "train loss:0.05199373369155966\n",
      "train loss:0.11290223904614426\n",
      "train loss:0.16939999083939322\n",
      "train loss:0.03728195074041989\n",
      "train loss:0.08271351674579078\n",
      "train loss:0.1051224736284146\n",
      "train loss:0.057988133861524745\n",
      "train loss:0.025163590572795904\n",
      "train loss:0.049265299790059745\n",
      "train loss:0.17124015022874559\n",
      "train loss:0.05046210145832328\n",
      "train loss:0.04143595093005624\n",
      "train loss:0.08945444250445661\n",
      "train loss:0.05357304048936399\n",
      "train loss:0.031259971492329085\n",
      "train loss:0.05903027263911538\n",
      "train loss:0.12446586250295306\n",
      "train loss:0.09340608239100973\n",
      "train loss:0.055064068700346525\n",
      "train loss:0.045409625079068275\n",
      "train loss:0.030228222815593918\n",
      "train loss:0.08410162366304162\n",
      "train loss:0.06035096805328246\n",
      "train loss:0.11459354119779315\n",
      "train loss:0.06649747471759222\n",
      "train loss:0.19365179324834716\n",
      "train loss:0.11572393937780948\n",
      "train loss:0.035669060938189244\n",
      "train loss:0.0533979465636495\n",
      "train loss:0.13460217670647207\n",
      "train loss:0.0877708687732877\n",
      "train loss:0.0805372780095405\n",
      "train loss:0.11556438390863537\n",
      "train loss:0.07640924179111465\n",
      "train loss:0.03097763106424741\n",
      "train loss:0.12003735771716412\n",
      "train loss:0.04084984472049127\n",
      "train loss:0.07109254292031174\n",
      "train loss:0.0835763522320209\n",
      "train loss:0.0923804659428101\n",
      "train loss:0.08593019333661639\n",
      "train loss:0.05320893987477601\n",
      "train loss:0.12527762181505825\n",
      "train loss:0.029432301537450002\n",
      "train loss:0.045805357203628175\n",
      "train loss:0.0410573654526383\n",
      "train loss:0.09272688406104546\n",
      "train loss:0.08171820218311943\n",
      "train loss:0.12160259458683488\n",
      "train loss:0.04197299828441797\n",
      "train loss:0.04761956500273157\n",
      "train loss:0.02712863607059217\n",
      "train loss:0.08381849603565698\n",
      "train loss:0.12780842189642277\n",
      "train loss:0.04030300802828407\n",
      "train loss:0.03982859091794374\n",
      "train loss:0.1033297419238255\n",
      "train loss:0.042430000696711136\n",
      "train loss:0.022362069152403383\n",
      "train loss:0.03793250060257692\n",
      "train loss:0.08357343531608699\n",
      "train loss:0.07371797714725804\n",
      "train loss:0.1004184131592782\n",
      "train loss:0.1338262057219767\n",
      "train loss:0.06768456872191679\n",
      "train loss:0.08743888601484501\n",
      "train loss:0.09291842916462191\n",
      "train loss:0.045671961928806945\n",
      "train loss:0.04685583582201798\n",
      "train loss:0.05178175774689887\n",
      "train loss:0.06777089130492263\n",
      "train loss:0.09120295039595325\n",
      "train loss:0.033663506394135914\n",
      "train loss:0.043047815620103035\n",
      "train loss:0.021636463069195647\n",
      "train loss:0.03888960521825203\n",
      "train loss:0.07613872060385697\n",
      "train loss:0.05006627943270946\n",
      "train loss:0.05990419085956028\n",
      "train loss:0.06624372675015017\n",
      "train loss:0.038417889273855324\n",
      "train loss:0.02378110130588222\n",
      "train loss:0.08081427261070549\n",
      "train loss:0.1869990641458346\n",
      "train loss:0.06752360210640011\n",
      "train loss:0.05721161370164387\n",
      "train loss:0.04330017567948269\n",
      "train loss:0.04948500921427981\n",
      "train loss:0.049378352093779075\n",
      "train loss:0.1192449051578557\n",
      "train loss:0.05201753670961812\n",
      "train loss:0.08205525163155195\n",
      "train loss:0.042365765314168365\n",
      "train loss:0.05320943840484225\n",
      "train loss:0.050111280950053946\n",
      "train loss:0.06157078960561205\n",
      "train loss:0.03627880902354548\n",
      "train loss:0.026107653444222053\n",
      "train loss:0.11838839056583189\n",
      "train loss:0.041918388227110547\n",
      "train loss:0.10295359151068663\n",
      "train loss:0.05727958852104069\n",
      "train loss:0.08719141875681247\n",
      "train loss:0.039603441738428204\n",
      "train loss:0.04324930495131827\n",
      "train loss:0.051129813110323726\n",
      "train loss:0.10415711175479955\n",
      "train loss:0.013790267815914541\n",
      "train loss:0.05583781405742246\n",
      "train loss:0.073161330698119\n",
      "train loss:0.049591163579753406\n",
      "train loss:0.10088879834869711\n",
      "train loss:0.14055144302222128\n",
      "train loss:0.04180796097344557\n",
      "train loss:0.0594735730294434\n",
      "train loss:0.07872932718053904\n",
      "train loss:0.1042369971378646\n",
      "train loss:0.01848585144462369\n",
      "train loss:0.047054130195972564\n",
      "train loss:0.020676589379061\n",
      "train loss:0.03726933249705921\n",
      "train loss:0.011687809454095503\n",
      "train loss:0.03226099590643875\n",
      "train loss:0.032654652044185974\n",
      "train loss:0.04887637067210283\n",
      "train loss:0.15023658404579052\n",
      "train loss:0.058762422155148986\n",
      "train loss:0.0862386414466695\n",
      "train loss:0.04282219734400341\n",
      "train loss:0.06394353323985266\n",
      "train loss:0.086025373793009\n",
      "train loss:0.05971183396779899\n",
      "train loss:0.13080299099764736\n",
      "train loss:0.027857702870463952\n",
      "train loss:0.03479904624284722\n",
      "train loss:0.05643798538586192\n",
      "train loss:0.019587077687411924\n",
      "train loss:0.030575874621116148\n",
      "train loss:0.02089337970466206\n",
      "train loss:0.03169720399010699\n",
      "train loss:0.06643424568174006\n",
      "train loss:0.01853330960540564\n",
      "train loss:0.08507371687286013\n",
      "train loss:0.02885069692288527\n",
      "train loss:0.0558011165104037\n",
      "train loss:0.06395054189247108\n",
      "train loss:0.04272382247668732\n",
      "train loss:0.016589801739597575\n",
      "train loss:0.07122955200529814\n",
      "train loss:0.04409975217745414\n",
      "train loss:0.09560949450757962\n",
      "train loss:0.05223061133235626\n",
      "train loss:0.08636059280176062\n",
      "train loss:0.059103481543854954\n",
      "train loss:0.03559425850867362\n",
      "train loss:0.06471279105477284\n",
      "train loss:0.05398491443896208\n",
      "train loss:0.054830779230591015\n",
      "train loss:0.05139594817602921\n",
      "train loss:0.011813164893651836\n",
      "train loss:0.02787070925474146\n",
      "train loss:0.05879540401915455\n",
      "train loss:0.12515387763610356\n",
      "train loss:0.01666193577935529\n",
      "train loss:0.02759255237998695\n",
      "train loss:0.030152559866459074\n",
      "train loss:0.06940105699409402\n",
      "train loss:0.04101881379933481\n",
      "train loss:0.034175035924565826\n",
      "train loss:0.04007059854535565\n",
      "train loss:0.07544910031694108\n",
      "train loss:0.04095936327488463\n",
      "train loss:0.04520032509625672\n",
      "train loss:0.03507371859031298\n",
      "train loss:0.05332849477177226\n",
      "train loss:0.035750128125305154\n",
      "train loss:0.06829166569897366\n",
      "train loss:0.12235241717572434\n",
      "train loss:0.025538384353352553\n",
      "train loss:0.056695069909626715\n",
      "train loss:0.05847766767904026\n",
      "train loss:0.037721870707393995\n",
      "=== epoch:3, train acc:0.98, test acc:0.978 ===\n",
      "train loss:0.02874638887549079\n",
      "train loss:0.037501101740989744\n",
      "train loss:0.10064724742301229\n",
      "train loss:0.0495959324834835\n",
      "train loss:0.011328941430441395\n",
      "train loss:0.06834726475183439\n",
      "train loss:0.039772426078678896\n",
      "train loss:0.13503921499103372\n",
      "train loss:0.059833947694593694\n",
      "train loss:0.12326656348765706\n",
      "train loss:0.05459052889175302\n",
      "train loss:0.04322282900765581\n",
      "train loss:0.05215376059150497\n",
      "train loss:0.03963861122024574\n",
      "train loss:0.10796740481602622\n",
      "train loss:0.020113279638740593\n",
      "train loss:0.04270748013848286\n",
      "train loss:0.014286083808807226\n",
      "train loss:0.047143067245000794\n",
      "train loss:0.04812597582009655\n",
      "train loss:0.05936283022709461\n",
      "train loss:0.03194126496971051\n",
      "train loss:0.0539309226352933\n",
      "train loss:0.030450947712097016\n",
      "train loss:0.0739719144261661\n",
      "train loss:0.09737024143865818\n",
      "train loss:0.039517084118649015\n",
      "train loss:0.05558420983988535\n",
      "train loss:0.05573043702676255\n",
      "train loss:0.020927924494779267\n",
      "train loss:0.027289299013203004\n",
      "train loss:0.12860770131599059\n",
      "train loss:0.041236654731995205\n",
      "train loss:0.0424104787633117\n",
      "train loss:0.057034053813275455\n",
      "train loss:0.029813099485160078\n",
      "train loss:0.06881253899553323\n",
      "train loss:0.10486646612977121\n",
      "train loss:0.1080032143457777\n",
      "train loss:0.03584420338884158\n",
      "train loss:0.011993631991902025\n",
      "train loss:0.029990826808957975\n",
      "train loss:0.05918469727447509\n",
      "train loss:0.05977320391049914\n",
      "train loss:0.05468676124635449\n",
      "train loss:0.08275641978777656\n",
      "train loss:0.05008757426773371\n",
      "train loss:0.018782043319099596\n",
      "train loss:0.029696296947451635\n",
      "train loss:0.03246818639973332\n",
      "train loss:0.04920810444813797\n",
      "train loss:0.04063918614213427\n",
      "train loss:0.044456044535663056\n",
      "train loss:0.05246400649594303\n",
      "train loss:0.12593579550187317\n",
      "train loss:0.04378154976366631\n",
      "train loss:0.12421353506767764\n",
      "train loss:0.07105161508449895\n",
      "train loss:0.09674860690975055\n",
      "train loss:0.04985652276493068\n",
      "train loss:0.09687796068614668\n",
      "train loss:0.015882047068787254\n",
      "train loss:0.1343635370150716\n",
      "train loss:0.051223453233049804\n",
      "train loss:0.04382923190825076\n",
      "train loss:0.07578664620169694\n",
      "train loss:0.11617697163370756\n",
      "train loss:0.13815938153076496\n",
      "train loss:0.12781330571511862\n",
      "train loss:0.06003380530143426\n",
      "train loss:0.04391286415064582\n",
      "train loss:0.03433086130649631\n",
      "train loss:0.04499572390746786\n",
      "train loss:0.054403128658381365\n",
      "train loss:0.06517452512951043\n",
      "train loss:0.055126481180937575\n",
      "train loss:0.032298887765540484\n",
      "train loss:0.022354728572832203\n",
      "train loss:0.02252590905670474\n",
      "train loss:0.04778968415637319\n",
      "train loss:0.026013401590926962\n",
      "train loss:0.01805975783368336\n",
      "train loss:0.032862472813833594\n",
      "train loss:0.043702286339461806\n",
      "train loss:0.09750840181184352\n",
      "train loss:0.040317773010678674\n",
      "train loss:0.028363114229758027\n",
      "train loss:0.056412261029028014\n",
      "train loss:0.05980970555086848\n",
      "train loss:0.058180129800863\n",
      "train loss:0.06375806545788185\n",
      "train loss:0.036202530167296916\n",
      "train loss:0.048918316524986745\n",
      "train loss:0.09419202197226753\n",
      "train loss:0.03923044914715317\n",
      "train loss:0.1408474025431763\n",
      "train loss:0.03285448320328619\n",
      "train loss:0.021096997032435883\n",
      "train loss:0.07408292681474059\n",
      "train loss:0.049359717064811265\n",
      "train loss:0.037478126612824327\n",
      "train loss:0.06318260057313392\n",
      "train loss:0.07027825711144171\n",
      "train loss:0.12394819708236549\n",
      "train loss:0.07776230134221111\n",
      "train loss:0.07417261800985225\n",
      "train loss:0.04328259537466985\n",
      "train loss:0.09340330666829168\n",
      "train loss:0.06226598010907582\n",
      "train loss:0.024313387730297693\n",
      "train loss:0.05600332262963132\n",
      "train loss:0.012304867861982576\n",
      "train loss:0.07998512791363588\n",
      "train loss:0.05468019357537564\n",
      "train loss:0.05301540619916184\n",
      "train loss:0.03422254786213582\n",
      "train loss:0.04680964922120169\n",
      "train loss:0.0173248042829608\n",
      "train loss:0.08816103829023211\n",
      "train loss:0.109917113259978\n",
      "train loss:0.03405753710283844\n",
      "train loss:0.09322781205203721\n",
      "train loss:0.03349950549201073\n",
      "train loss:0.09196496554070162\n",
      "train loss:0.05385428385140449\n",
      "train loss:0.044235355912233876\n",
      "train loss:0.051248678637456475\n",
      "train loss:0.059339502173140476\n",
      "train loss:0.024547029529624252\n",
      "train loss:0.1352891173588941\n",
      "train loss:0.022243149701404\n",
      "train loss:0.059073023859815284\n",
      "train loss:0.08403827572470884\n",
      "train loss:0.09003917563822134\n",
      "train loss:0.09543156959366658\n",
      "train loss:0.10567243788903792\n",
      "train loss:0.033787761998538855\n",
      "train loss:0.032431536783373474\n",
      "train loss:0.03322495394709782\n",
      "train loss:0.04288541545761371\n",
      "train loss:0.04460405058158098\n",
      "train loss:0.06893322346633368\n",
      "train loss:0.05152431652855559\n",
      "train loss:0.0805710497779258\n",
      "train loss:0.02425817214215731\n",
      "train loss:0.07470790470413335\n",
      "train loss:0.03293498968918081\n",
      "train loss:0.02448458424016272\n",
      "train loss:0.07461443282369781\n",
      "train loss:0.025626933800460154\n",
      "train loss:0.03228616501190406\n",
      "train loss:0.07326293562002745\n",
      "train loss:0.048444599779549354\n",
      "train loss:0.0806476352220307\n",
      "train loss:0.045287876699366826\n",
      "train loss:0.04291930590399035\n",
      "train loss:0.012102080804411114\n",
      "train loss:0.04154895608201758\n",
      "train loss:0.07608100610072756\n",
      "train loss:0.05148379046982263\n",
      "train loss:0.03591798733749603\n",
      "train loss:0.051963784401054654\n",
      "train loss:0.07365427181943517\n",
      "train loss:0.04707846667946404\n",
      "train loss:0.009204447303569556\n",
      "train loss:0.04954907047641735\n",
      "train loss:0.022553918269066532\n",
      "train loss:0.0577458153548105\n",
      "train loss:0.02858915865152602\n",
      "train loss:0.04249052578336599\n",
      "train loss:0.05073525724531022\n",
      "train loss:0.04564282023145202\n",
      "train loss:0.022246687788179426\n",
      "train loss:0.04222053103041984\n",
      "train loss:0.032355618178915094\n",
      "train loss:0.1069582517908493\n",
      "train loss:0.03750784889402742\n",
      "train loss:0.01808112911195283\n",
      "train loss:0.02880170449550482\n",
      "train loss:0.07015537825713511\n",
      "train loss:0.030823224396384182\n",
      "train loss:0.06121595567074763\n",
      "train loss:0.08702491934200816\n",
      "train loss:0.06722693461798795\n",
      "train loss:0.12175180213473484\n",
      "train loss:0.04706770362179746\n",
      "train loss:0.02093105999062137\n",
      "train loss:0.06898091821500404\n",
      "train loss:0.03474354579784604\n",
      "train loss:0.02022739732921404\n",
      "train loss:0.039154139931871085\n",
      "train loss:0.03596755194061147\n",
      "train loss:0.04174385656485263\n",
      "train loss:0.044354235650169195\n",
      "train loss:0.031113094024499143\n",
      "train loss:0.06616598239880613\n",
      "train loss:0.023589399344648754\n",
      "train loss:0.06366270062083323\n",
      "train loss:0.026276193237274436\n",
      "train loss:0.07100737921051156\n",
      "train loss:0.12738510730425964\n",
      "train loss:0.06458669035505206\n",
      "train loss:0.13373262946838538\n",
      "train loss:0.0607448502580961\n",
      "train loss:0.2772144153468441\n",
      "train loss:0.037228218376885634\n",
      "train loss:0.024592822950117478\n",
      "train loss:0.11036284301922256\n",
      "train loss:0.12255225666772318\n",
      "train loss:0.0958341525324766\n",
      "train loss:0.020295048629440586\n",
      "train loss:0.035026029632192056\n",
      "train loss:0.02135371151535062\n",
      "train loss:0.01565744738938829\n",
      "train loss:0.036777197047924134\n",
      "train loss:0.07118099990731165\n",
      "train loss:0.14679269037557444\n",
      "train loss:0.10319870886769814\n",
      "train loss:0.07321450513653913\n",
      "train loss:0.06540652227323221\n",
      "train loss:0.041417396593519805\n",
      "train loss:0.040405109192845325\n",
      "train loss:0.06921677955039529\n",
      "train loss:0.10382494794883299\n",
      "train loss:0.06983491710989206\n",
      "train loss:0.06832857746535835\n",
      "train loss:0.05144483410002208\n",
      "train loss:0.15202965315497932\n",
      "train loss:0.08146722502316414\n",
      "train loss:0.036580826908395835\n",
      "train loss:0.04799800304850791\n",
      "train loss:0.059274658706479896\n",
      "train loss:0.05105857774141486\n",
      "train loss:0.028754071650233592\n",
      "train loss:0.02368755096265619\n",
      "train loss:0.025839019064221543\n",
      "train loss:0.05044099287004076\n",
      "train loss:0.023998970198574384\n",
      "train loss:0.04482543510712945\n",
      "train loss:0.0715125158016909\n",
      "train loss:0.031687164842619416\n",
      "train loss:0.06909752323758404\n",
      "train loss:0.037064391018525836\n",
      "train loss:0.031867138831133145\n",
      "train loss:0.018185071044026047\n",
      "train loss:0.0644110844760055\n",
      "train loss:0.0990542814133114\n",
      "train loss:0.0656642060063108\n",
      "train loss:0.08233436579080236\n",
      "train loss:0.03888816734379958\n",
      "train loss:0.020500812192906174\n",
      "train loss:0.1054306377182696\n",
      "train loss:0.03176419411774075\n",
      "train loss:0.06336873213109456\n",
      "train loss:0.0285448861540065\n",
      "train loss:0.0808780808196611\n",
      "train loss:0.03493057670204416\n",
      "train loss:0.05355112547132794\n",
      "train loss:0.06845292036287065\n",
      "train loss:0.024878173640513413\n",
      "train loss:0.010956663126261312\n",
      "train loss:0.060743292735708554\n",
      "train loss:0.05963781585145553\n",
      "train loss:0.021164803546957954\n",
      "train loss:0.05448144037457853\n",
      "train loss:0.048628625150397925\n",
      "train loss:0.03042864526740095\n",
      "train loss:0.09277668539732453\n",
      "train loss:0.0764461888571408\n",
      "train loss:0.01898840367100142\n",
      "train loss:0.12846561950045673\n",
      "train loss:0.09462522111130334\n",
      "train loss:0.024794760162139023\n",
      "train loss:0.0491864342454419\n",
      "train loss:0.06588338621743009\n",
      "train loss:0.1911203118033175\n",
      "train loss:0.02002157364231997\n",
      "train loss:0.0933822311317702\n",
      "train loss:0.029483359798432217\n",
      "train loss:0.12003598755941203\n",
      "train loss:0.032681788690455\n",
      "train loss:0.007791274717330576\n",
      "train loss:0.039738000843944625\n",
      "train loss:0.021052033319039532\n",
      "train loss:0.03557693073446692\n",
      "train loss:0.09042891313133099\n",
      "train loss:0.0942674253677009\n",
      "train loss:0.07403332669289016\n",
      "train loss:0.05072573562906611\n",
      "train loss:0.02991936878276674\n",
      "train loss:0.05742794632047696\n",
      "train loss:0.018833336075008814\n",
      "train loss:0.04537350260312509\n",
      "train loss:0.04019792452686903\n",
      "train loss:0.04000525449065681\n",
      "train loss:0.06756310036160024\n",
      "train loss:0.0259994016719774\n",
      "train loss:0.030354843008777003\n",
      "train loss:0.00850228553312414\n",
      "train loss:0.08068862807002121\n",
      "train loss:0.035253046972359356\n",
      "train loss:0.03756901404280086\n",
      "train loss:0.021774275012694213\n",
      "train loss:0.03738275479681959\n",
      "train loss:0.037917158028855286\n",
      "train loss:0.07899315110696703\n",
      "train loss:0.04378319235889059\n",
      "train loss:0.018380417021757547\n",
      "train loss:0.042044914039490136\n",
      "train loss:0.008593301336175368\n",
      "train loss:0.058545017362026545\n",
      "train loss:0.06759828266420229\n",
      "train loss:0.02589400903250892\n",
      "train loss:0.03595816225714487\n",
      "train loss:0.08453582784182283\n",
      "train loss:0.03802812148187836\n",
      "train loss:0.13644346805580052\n",
      "train loss:0.027898050576512384\n",
      "train loss:0.05177387877199679\n",
      "train loss:0.045022332024815315\n",
      "train loss:0.05920459322899843\n",
      "train loss:0.011783649236001628\n",
      "train loss:0.07397895027669828\n",
      "train loss:0.019266454486349133\n",
      "train loss:0.032547565478019645\n",
      "train loss:0.027036368882363698\n",
      "train loss:0.02767770238109104\n",
      "train loss:0.030707557242518536\n",
      "train loss:0.059507900558216004\n",
      "train loss:0.06564346050453215\n",
      "train loss:0.01937900123099443\n",
      "train loss:0.06654421014755652\n",
      "train loss:0.05323608844949471\n",
      "train loss:0.04360154367736552\n",
      "train loss:0.0756007899983559\n",
      "train loss:0.0315121738622412\n",
      "train loss:0.02800423450175563\n",
      "train loss:0.0625751725472347\n",
      "train loss:0.07908757715998602\n",
      "train loss:0.04695383117148134\n",
      "train loss:0.025837329954318228\n",
      "train loss:0.04109076742887767\n",
      "train loss:0.02461452322110594\n",
      "train loss:0.03186898461475304\n",
      "train loss:0.03311449037076877\n",
      "train loss:0.061652951627220134\n",
      "train loss:0.026955845755952598\n",
      "train loss:0.020034519256522443\n",
      "train loss:0.025284085382652317\n",
      "train loss:0.02497640183625551\n",
      "train loss:0.06975728992469839\n",
      "train loss:0.05678699823387088\n",
      "train loss:0.06132518731461681\n",
      "train loss:0.05876212773061986\n",
      "train loss:0.0435225494760201\n",
      "train loss:0.052192877256496996\n",
      "train loss:0.04110862846630082\n",
      "train loss:0.01516343779104607\n",
      "train loss:0.058839746529467954\n",
      "train loss:0.05827526588152043\n",
      "train loss:0.08484671930113973\n",
      "train loss:0.03910981151282912\n",
      "train loss:0.022910515383955842\n",
      "train loss:0.023809374548357784\n",
      "train loss:0.026564426327541208\n",
      "train loss:0.015609480300899859\n",
      "train loss:0.07422223309681625\n",
      "train loss:0.02613308306129252\n",
      "train loss:0.05363203832749958\n",
      "train loss:0.07628113351335872\n",
      "train loss:0.05179008122378279\n",
      "train loss:0.05365553547509151\n",
      "train loss:0.06816509162618573\n",
      "train loss:0.02165943911858724\n",
      "train loss:0.038711897667791255\n",
      "train loss:0.01920109863389547\n",
      "train loss:0.06900452107801361\n",
      "train loss:0.010295212606238037\n",
      "train loss:0.09196702495757585\n",
      "train loss:0.04127436924132078\n",
      "train loss:0.03615955285769821\n",
      "train loss:0.010672721405961734\n",
      "train loss:0.11127790158946077\n",
      "train loss:0.11657887916153133\n",
      "train loss:0.10228266639024668\n",
      "train loss:0.06078655776861934\n",
      "train loss:0.0629084634533421\n",
      "train loss:0.01687940580716782\n",
      "train loss:0.045447724108447114\n",
      "train loss:0.02313038213246159\n",
      "train loss:0.030440223274747335\n",
      "train loss:0.09259028972419514\n",
      "train loss:0.09538146197605021\n",
      "train loss:0.024813287605847972\n",
      "train loss:0.041329318190345424\n",
      "train loss:0.022061388226345152\n",
      "train loss:0.031289138381062995\n",
      "train loss:0.06538073233605783\n",
      "train loss:0.0359045210519563\n",
      "train loss:0.012433744266817719\n",
      "train loss:0.025121215048746114\n",
      "train loss:0.012321683286151172\n",
      "train loss:0.011854493281840916\n",
      "train loss:0.04014891656563536\n",
      "train loss:0.023947874600492938\n",
      "train loss:0.02971287143943505\n",
      "train loss:0.011565904563476575\n",
      "train loss:0.026639770768681497\n",
      "train loss:0.034778373002782\n",
      "train loss:0.04845513904611954\n",
      "train loss:0.06734557626863215\n",
      "train loss:0.049778764034838135\n",
      "train loss:0.02650103415474552\n",
      "train loss:0.058785915448534114\n",
      "train loss:0.043783896845233726\n",
      "train loss:0.07411901670325108\n",
      "train loss:0.10270527948212176\n",
      "train loss:0.02659576378907651\n",
      "train loss:0.05437434576114925\n",
      "train loss:0.030465913515924852\n",
      "train loss:0.028697614685933574\n",
      "train loss:0.050566433154009556\n",
      "train loss:0.02226292952284809\n",
      "train loss:0.015640386845753463\n",
      "train loss:0.03455150760301498\n",
      "train loss:0.03070259299871185\n",
      "train loss:0.015123508100551453\n",
      "train loss:0.08595831488026981\n",
      "train loss:0.012436906465353788\n",
      "train loss:0.010874570008521822\n",
      "train loss:0.007830979099658646\n",
      "train loss:0.03518214693237251\n",
      "train loss:0.059579147953329616\n",
      "train loss:0.02248245506661569\n",
      "train loss:0.03725355827716207\n",
      "train loss:0.02004912425506026\n",
      "train loss:0.013757823187170424\n",
      "train loss:0.0497768047455243\n",
      "train loss:0.0495410563854065\n",
      "train loss:0.03229796705843611\n",
      "train loss:0.031312276336546575\n",
      "train loss:0.045288143974456165\n",
      "train loss:0.006972575177297453\n",
      "train loss:0.008239204140679168\n",
      "train loss:0.01981590087676326\n",
      "train loss:0.0621598287012247\n",
      "train loss:0.07068751389917932\n",
      "train loss:0.07602514125334574\n",
      "train loss:0.0841993213731943\n",
      "train loss:0.04981478477369046\n",
      "train loss:0.08288885981590052\n",
      "train loss:0.12597777344665761\n",
      "train loss:0.033483580733166984\n",
      "train loss:0.05445657436357981\n",
      "train loss:0.10728497059776006\n",
      "train loss:0.04044943276846367\n",
      "train loss:0.031332990890268954\n",
      "train loss:0.05300194800474273\n",
      "train loss:0.02092539831144829\n",
      "train loss:0.028626491372107998\n",
      "train loss:0.04331580131553001\n",
      "train loss:0.042909229730346714\n",
      "train loss:0.09437876950401156\n",
      "train loss:0.028857006930901305\n",
      "train loss:0.032688899444151334\n",
      "train loss:0.07984319835931773\n",
      "train loss:0.012118798886670012\n",
      "train loss:0.04514538412041778\n",
      "train loss:0.021444391849996806\n",
      "train loss:0.030736124788000804\n",
      "train loss:0.03474606373169149\n",
      "train loss:0.05202784000446369\n",
      "train loss:0.07227515272944046\n",
      "train loss:0.035197369753635493\n",
      "train loss:0.05560032366005554\n",
      "train loss:0.02064681147525663\n",
      "train loss:0.018121810592623692\n",
      "train loss:0.03816351456505156\n",
      "train loss:0.021425543870606974\n",
      "train loss:0.022984193991428828\n",
      "train loss:0.04614858214558074\n",
      "train loss:0.024970680330330182\n",
      "train loss:0.02770361916516186\n",
      "train loss:0.06336635486210046\n",
      "train loss:0.02157067329503859\n",
      "train loss:0.019691167165345545\n",
      "train loss:0.08831064907956092\n",
      "train loss:0.06166182484843207\n",
      "train loss:0.010829416929612057\n",
      "train loss:0.03597047598825161\n",
      "train loss:0.06502184540804107\n",
      "train loss:0.049288578112725245\n",
      "train loss:0.02039899654561182\n",
      "train loss:0.015033061323505084\n",
      "train loss:0.05924261184079665\n",
      "train loss:0.04579272307820709\n",
      "train loss:0.013630672139788405\n",
      "train loss:0.016672207619053806\n",
      "train loss:0.07543196341362507\n",
      "train loss:0.025721604856357317\n",
      "train loss:0.04348206728578139\n",
      "train loss:0.020574928764846706\n",
      "train loss:0.02951620672430706\n",
      "train loss:0.010052903521870042\n",
      "train loss:0.03383623744499835\n",
      "train loss:0.04417551258671573\n",
      "train loss:0.012901522175649565\n",
      "train loss:0.05118011014805592\n",
      "train loss:0.024130080233625003\n",
      "train loss:0.011338629169209257\n",
      "train loss:0.02336607283080641\n",
      "train loss:0.040560142068721036\n",
      "train loss:0.032482788733180404\n",
      "train loss:0.03369348755247823\n",
      "train loss:0.016599944274078456\n",
      "train loss:0.12918101322663927\n",
      "train loss:0.08549178199850192\n",
      "train loss:0.09567365708717615\n",
      "train loss:0.09239832443052631\n",
      "train loss:0.030331335706390038\n",
      "train loss:0.04143382991267032\n",
      "train loss:0.03561975412655705\n",
      "train loss:0.051335846931292464\n",
      "train loss:0.03496712911200652\n",
      "train loss:0.09316263076171484\n",
      "train loss:0.04174274798693511\n",
      "train loss:0.06738487414967158\n",
      "train loss:0.026230372620615255\n",
      "train loss:0.03498177512996294\n",
      "train loss:0.023120436686460653\n",
      "train loss:0.0300295378051909\n",
      "train loss:0.11375709342951275\n",
      "train loss:0.15308853855708893\n",
      "train loss:0.0575063777215398\n",
      "train loss:0.018736023972540528\n",
      "train loss:0.013473919138346386\n",
      "train loss:0.06160811491175895\n",
      "train loss:0.011468118087532177\n",
      "train loss:0.03761870586451462\n",
      "train loss:0.05755620461983817\n",
      "train loss:0.02790415351275688\n",
      "train loss:0.1413242114243014\n",
      "train loss:0.04144350889732382\n",
      "train loss:0.011303041196233814\n",
      "train loss:0.03908878772765958\n",
      "train loss:0.08046212830291424\n",
      "train loss:0.032481156640576846\n",
      "train loss:0.024068050170003602\n",
      "train loss:0.040873562759434205\n",
      "train loss:0.033510809767763765\n",
      "train loss:0.061998413937087625\n",
      "train loss:0.007095297260758787\n",
      "train loss:0.06622559032951154\n",
      "train loss:0.0444911954894924\n",
      "train loss:0.013919572205662865\n",
      "train loss:0.030051265228391248\n",
      "train loss:0.017071530900578332\n",
      "train loss:0.027908792496631946\n",
      "train loss:0.0692743208799717\n",
      "train loss:0.020884224157579286\n",
      "train loss:0.0641478163892991\n",
      "train loss:0.016049236056470767\n",
      "train loss:0.0927374575981673\n",
      "train loss:0.037894604522448895\n",
      "train loss:0.0900137759020444\n",
      "train loss:0.0749349173504997\n",
      "train loss:0.02470788441575357\n",
      "train loss:0.04079885660021858\n",
      "train loss:0.03358562367673076\n",
      "train loss:0.03397460474914036\n",
      "train loss:0.03727675291785258\n",
      "train loss:0.017306465419401586\n",
      "train loss:0.009482461041266484\n",
      "train loss:0.013906657332969342\n",
      "train loss:0.06522934006392585\n",
      "train loss:0.04525728415378099\n",
      "train loss:0.06366397636421549\n",
      "train loss:0.0223354645053094\n",
      "train loss:0.026952882837474868\n",
      "train loss:0.07621159593663286\n",
      "train loss:0.047961832982601295\n",
      "train loss:0.07413356138576241\n",
      "train loss:0.030269392252178614\n",
      "train loss:0.036230946577097835\n",
      "train loss:0.016828939935697545\n",
      "train loss:0.07684062817981749\n",
      "train loss:0.04628500392302633\n",
      "train loss:0.02254256933556125\n",
      "train loss:0.03884870507162465\n",
      "train loss:0.02518761396345377\n",
      "train loss:0.02269127249626512\n",
      "train loss:0.039097958087496665\n",
      "train loss:0.0838379733253711\n",
      "train loss:0.02473621960360201\n",
      "train loss:0.03390067235476602\n",
      "train loss:0.07210696420008109\n",
      "train loss:0.019838508046256576\n",
      "train loss:0.018101012868606114\n",
      "train loss:0.034274796046391096\n",
      "train loss:0.04331513771416105\n",
      "=== epoch:4, train acc:0.984, test acc:0.977 ===\n",
      "train loss:0.07534980307282972\n",
      "train loss:0.03019565878845171\n",
      "train loss:0.031635601691628906\n",
      "train loss:0.1656124672799988\n",
      "train loss:0.06637067239496028\n",
      "train loss:0.07513851351670264\n",
      "train loss:0.04590434268933074\n",
      "train loss:0.06243029186364561\n",
      "train loss:0.033701060191729124\n",
      "train loss:0.035489042662827604\n",
      "train loss:0.01931705561159768\n",
      "train loss:0.04463120076687968\n",
      "train loss:0.026977385840311164\n",
      "train loss:0.04560311615058\n",
      "train loss:0.02315045625404821\n",
      "train loss:0.08573362842330104\n",
      "train loss:0.05874234107391573\n",
      "train loss:0.12197751469234466\n",
      "train loss:0.06772023942944122\n",
      "train loss:0.02655888310960871\n",
      "train loss:0.07570851250297846\n",
      "train loss:0.012294260699392883\n",
      "train loss:0.046961959476559414\n",
      "train loss:0.02154618260884926\n",
      "train loss:0.03132911100523933\n",
      "train loss:0.08968774043540644\n",
      "train loss:0.11206827124460104\n",
      "train loss:0.03558785289264457\n",
      "train loss:0.01714419295283466\n",
      "train loss:0.1181876225236878\n",
      "train loss:0.07264953193559232\n",
      "train loss:0.02469270561701572\n",
      "train loss:0.03509420767637376\n",
      "train loss:0.22057810761582095\n",
      "train loss:0.010975225838364243\n",
      "train loss:0.022962621232661698\n",
      "train loss:0.03152642151489178\n",
      "train loss:0.01939152902364944\n",
      "train loss:0.0206484984196873\n",
      "train loss:0.05197473876156202\n",
      "train loss:0.055657158289903357\n",
      "train loss:0.05716657350830663\n",
      "train loss:0.013553560713344029\n",
      "train loss:0.1323181544811375\n",
      "train loss:0.05658341075140212\n",
      "train loss:0.022540203492904008\n",
      "train loss:0.08977062071328337\n",
      "train loss:0.01145325911293028\n",
      "train loss:0.045640544395186035\n",
      "train loss:0.03390974790602375\n",
      "train loss:0.043942171732636266\n",
      "train loss:0.02435649940773125\n",
      "train loss:0.05597565907202138\n",
      "train loss:0.05082011031539969\n",
      "train loss:0.0949582450259378\n",
      "train loss:0.08588563316956625\n",
      "train loss:0.0488754571119304\n",
      "train loss:0.0340993577019168\n",
      "train loss:0.02524308232415509\n",
      "train loss:0.05233297612829893\n",
      "train loss:0.06728733353658985\n",
      "train loss:0.049423494657309774\n",
      "train loss:0.006356065452372028\n",
      "train loss:0.07530249164248778\n",
      "train loss:0.06944482177837845\n",
      "train loss:0.03465573608023029\n",
      "train loss:0.03580892146450704\n",
      "train loss:0.025237999355114966\n",
      "train loss:0.044132325144578106\n",
      "train loss:0.019153752823430566\n",
      "train loss:0.03559272774136523\n",
      "train loss:0.07145577483427801\n",
      "train loss:0.07658256332646043\n",
      "train loss:0.11331270852963714\n",
      "train loss:0.0345839753033316\n",
      "train loss:0.01197545299241816\n",
      "train loss:0.060192195898584246\n",
      "train loss:0.03166148669499492\n",
      "train loss:0.030068332214686147\n",
      "train loss:0.012671691482435852\n",
      "train loss:0.038489613117711\n",
      "train loss:0.04273196238482458\n",
      "train loss:0.023091017182777646\n",
      "train loss:0.03291729547744162\n",
      "train loss:0.049352035158465195\n",
      "train loss:0.038914935893583975\n",
      "train loss:0.014365068287614986\n",
      "train loss:0.021027017412966954\n",
      "train loss:0.09928152710269326\n",
      "train loss:0.023780515845639373\n",
      "train loss:0.05027995072830139\n",
      "train loss:0.02515224914843225\n",
      "train loss:0.02391216858323542\n",
      "train loss:0.0513387386151168\n",
      "train loss:0.008068468554075524\n",
      "train loss:0.022744107038597104\n",
      "train loss:0.04557481327490504\n",
      "train loss:0.024237219512187724\n",
      "train loss:0.031208174836592096\n",
      "train loss:0.04351080971991553\n",
      "train loss:0.007530462950190758\n",
      "train loss:0.033261548343259\n",
      "train loss:0.02526275972671169\n",
      "train loss:0.02641534548520357\n",
      "train loss:0.03330510039008765\n",
      "train loss:0.08936409105388574\n",
      "train loss:0.016512248238756272\n",
      "train loss:0.047088362620798796\n",
      "train loss:0.04178649839862663\n",
      "train loss:0.026126116921008243\n",
      "train loss:0.008686846545051934\n",
      "train loss:0.008318690759539558\n",
      "train loss:0.062204644236669236\n",
      "train loss:0.023698681715220413\n",
      "train loss:0.008142636225907204\n",
      "train loss:0.0158215820665412\n",
      "train loss:0.07262893216249744\n",
      "train loss:0.08573672390782622\n",
      "train loss:0.07159725028623193\n",
      "train loss:0.13721248546874903\n",
      "train loss:0.08174236677603929\n",
      "train loss:0.008826750610624436\n",
      "train loss:0.02097276810647256\n",
      "train loss:0.041777526130185684\n",
      "train loss:0.11097094229893763\n",
      "train loss:0.062245140717784676\n",
      "train loss:0.04431807938532531\n",
      "train loss:0.06340320665659537\n",
      "train loss:0.07914142831588132\n",
      "train loss:0.015131381529563915\n",
      "train loss:0.059819399476394926\n",
      "train loss:0.0327442750047166\n",
      "train loss:0.024757427509412788\n",
      "train loss:0.02897461094351993\n",
      "train loss:0.033695089993741954\n",
      "train loss:0.04349852234284994\n",
      "train loss:0.012884984607611658\n",
      "train loss:0.021410969353458813\n",
      "train loss:0.03170092902656954\n",
      "train loss:0.01245651319461159\n",
      "train loss:0.02813909721551544\n",
      "train loss:0.016087353155828373\n",
      "train loss:0.06286270129383817\n",
      "train loss:0.029846710247799203\n",
      "train loss:0.06613804936122156\n",
      "train loss:0.01287219101684286\n",
      "train loss:0.017547815038432252\n",
      "train loss:0.012364932892959419\n",
      "train loss:0.021524856923302037\n",
      "train loss:0.07549061473968147\n",
      "train loss:0.0187835471812921\n",
      "train loss:0.018514401533138443\n",
      "train loss:0.09466935530835523\n",
      "train loss:0.021597340567324964\n",
      "train loss:0.06748746029992848\n",
      "train loss:0.05204152143034361\n",
      "train loss:0.036770509506743064\n",
      "train loss:0.01496681753774911\n",
      "train loss:0.07815760610235849\n",
      "train loss:0.015361985155185508\n",
      "train loss:0.07083223374895697\n",
      "train loss:0.07987709363920166\n",
      "train loss:0.011258008761224672\n",
      "train loss:0.0161983795023478\n",
      "train loss:0.028425050840914533\n",
      "train loss:0.019624545553060948\n",
      "train loss:0.02236456928595139\n",
      "train loss:0.02561991223134054\n",
      "train loss:0.027036353304228208\n",
      "train loss:0.026700813444671918\n",
      "train loss:0.07884971031295934\n",
      "train loss:0.05761824810467171\n",
      "train loss:0.0325482265025141\n",
      "train loss:0.01551364014996183\n",
      "train loss:0.07011113225397939\n",
      "train loss:0.030225260584375385\n",
      "train loss:0.026876553608556772\n",
      "train loss:0.013849841134947902\n",
      "train loss:0.011458542988903992\n",
      "train loss:0.040449863215878186\n",
      "train loss:0.010725872021174795\n",
      "train loss:0.03748404761488678\n",
      "train loss:0.02848231846668621\n",
      "train loss:0.05562648719040615\n",
      "train loss:0.006558755625694293\n",
      "train loss:0.01684774989857564\n",
      "train loss:0.02432583686270905\n",
      "train loss:0.05107580591382673\n",
      "train loss:0.03702032940960797\n",
      "train loss:0.05011591805886995\n",
      "train loss:0.007581931444548578\n",
      "train loss:0.07765151641892382\n",
      "train loss:0.04121601400599279\n",
      "train loss:0.07131806450374238\n",
      "train loss:0.05274638402784181\n",
      "train loss:0.013734728635628904\n",
      "train loss:0.021859645579935735\n",
      "train loss:0.029947838933071468\n",
      "train loss:0.010053561379346647\n",
      "train loss:0.06185809771651415\n",
      "train loss:0.049880497212478135\n",
      "train loss:0.00910987650421671\n",
      "train loss:0.03250945941808321\n",
      "train loss:0.040514923008068804\n",
      "train loss:0.03897039686722385\n",
      "train loss:0.0368331217794374\n",
      "train loss:0.14192780590905132\n",
      "train loss:0.01110748609101184\n",
      "train loss:0.050647018750377165\n",
      "train loss:0.009457165652125564\n",
      "train loss:0.009474437698077022\n",
      "train loss:0.038392505720057195\n",
      "train loss:0.02711496312545709\n",
      "train loss:0.05661654991745319\n",
      "train loss:0.034304115663293035\n",
      "train loss:0.010035103440835538\n",
      "train loss:0.00994668148337149\n",
      "train loss:0.027402079713979743\n",
      "train loss:0.046151286294622604\n",
      "train loss:0.06393011163168631\n",
      "train loss:0.053314463183338406\n",
      "train loss:0.023592569487081855\n",
      "train loss:0.02181491068102508\n",
      "train loss:0.01865837682253038\n",
      "train loss:0.016424839883166818\n",
      "train loss:0.04356239775245076\n",
      "train loss:0.06097702631162779\n",
      "train loss:0.021598817646072643\n",
      "train loss:0.0464705514467488\n",
      "train loss:0.017042707652807267\n",
      "train loss:0.06299020665831867\n",
      "train loss:0.04519686666967084\n",
      "train loss:0.007324624833814082\n",
      "train loss:0.02422141754342369\n",
      "train loss:0.06045495017387695\n",
      "train loss:0.009657418833952706\n",
      "train loss:0.0770717927325097\n",
      "train loss:0.021420538584671\n",
      "train loss:0.023123071071441337\n",
      "train loss:0.030050384304812643\n",
      "train loss:0.009409414416571955\n",
      "train loss:0.03760611111916619\n",
      "train loss:0.04306224031068331\n",
      "train loss:0.003771329231466417\n",
      "train loss:0.02035187462298239\n",
      "train loss:0.02356366551896981\n",
      "train loss:0.02223647469188303\n",
      "train loss:0.05844660577983452\n",
      "train loss:0.011864860741925465\n",
      "train loss:0.0678837043719698\n",
      "train loss:0.04421845464061973\n",
      "train loss:0.020676145950216388\n",
      "train loss:0.026181373440656085\n",
      "train loss:0.017907830250811912\n",
      "train loss:0.049291534369320836\n",
      "train loss:0.032427688921957784\n",
      "train loss:0.04235117094522594\n",
      "train loss:0.02234616440983572\n",
      "train loss:0.036125760160758795\n",
      "train loss:0.005755893583553379\n",
      "train loss:0.017027040654868784\n",
      "train loss:0.0162404447892048\n",
      "train loss:0.05053283955101958\n",
      "train loss:0.022590307529537015\n",
      "train loss:0.042302171262579374\n",
      "train loss:0.05876550274635237\n",
      "train loss:0.011153206599378225\n",
      "train loss:0.03195376295600397\n",
      "train loss:0.017935949061537754\n",
      "train loss:0.022717900480162614\n",
      "train loss:0.09683291697570555\n",
      "train loss:0.015467690865261246\n",
      "train loss:0.02514107685587627\n",
      "train loss:0.0384769221206595\n",
      "train loss:0.04749994607031969\n",
      "train loss:0.00669784455575037\n",
      "train loss:0.03662372539069399\n",
      "train loss:0.05731124985934253\n",
      "train loss:0.05404474265432634\n",
      "train loss:0.03757812787127555\n",
      "train loss:0.03352149147767614\n",
      "train loss:0.018546761318624504\n",
      "train loss:0.04859547555444021\n",
      "train loss:0.044297212972174156\n",
      "train loss:0.052314524733338266\n",
      "train loss:0.0882853630664736\n",
      "train loss:0.03950356960742486\n",
      "train loss:0.017471275224055532\n",
      "train loss:0.03280136147734773\n",
      "train loss:0.024138808744427784\n",
      "train loss:0.05035026053329281\n",
      "train loss:0.02373897472954125\n",
      "train loss:0.042441773831103244\n",
      "train loss:0.017789715273801222\n",
      "train loss:0.013596469613976446\n",
      "train loss:0.05389112110043894\n",
      "train loss:0.016892299461033117\n",
      "train loss:0.017652588401826014\n",
      "train loss:0.03270686323667018\n",
      "train loss:0.03863270544033618\n",
      "train loss:0.030256207013603972\n",
      "train loss:0.04891593242061488\n",
      "train loss:0.02276762436318053\n",
      "train loss:0.040514674318269286\n",
      "train loss:0.015939676314806046\n",
      "train loss:0.034461671512760715\n",
      "train loss:0.037211363947594764\n",
      "train loss:0.026517525277188935\n",
      "train loss:0.027087013837791198\n",
      "train loss:0.015746832270455435\n",
      "train loss:0.025453533603384\n",
      "train loss:0.04843815217929435\n",
      "train loss:0.026415009647984548\n",
      "train loss:0.01538253105390077\n",
      "train loss:0.025721165296029334\n",
      "train loss:0.02249345225373622\n",
      "train loss:0.046184198220971334\n",
      "train loss:0.017605618087549086\n",
      "train loss:0.01014089995767923\n",
      "train loss:0.050697507180598685\n",
      "train loss:0.06749355011374748\n",
      "train loss:0.031034973394670685\n",
      "train loss:0.027812044727402972\n",
      "train loss:0.0694486325203218\n",
      "train loss:0.04314985009549189\n",
      "train loss:0.03176881548765708\n",
      "train loss:0.04802001025928013\n",
      "train loss:0.0040953013657673235\n",
      "train loss:0.02239607484390888\n",
      "train loss:0.024195541010446204\n",
      "train loss:0.009237444479581719\n",
      "train loss:0.0030561523078343512\n",
      "train loss:0.020679932641311075\n",
      "train loss:0.06293680064033219\n",
      "train loss:0.010828277258250555\n",
      "train loss:0.0716366129848793\n",
      "train loss:0.013115646592386572\n",
      "train loss:0.022728534733058418\n",
      "train loss:0.02868135858762548\n",
      "train loss:0.02520313741941481\n",
      "train loss:0.009190525342115835\n",
      "train loss:0.017173752179348845\n",
      "train loss:0.03473302428766311\n",
      "train loss:0.006142738557916127\n",
      "train loss:0.013021963411346014\n",
      "train loss:0.023207601979464577\n",
      "train loss:0.013449355836648788\n",
      "train loss:0.0066849666045305225\n",
      "train loss:0.015625877318736766\n",
      "train loss:0.036994362447392494\n",
      "train loss:0.014034426834167007\n",
      "train loss:0.023022955409421595\n",
      "train loss:0.05525519838256752\n",
      "train loss:0.11728405143609417\n",
      "train loss:0.02329192594880445\n",
      "train loss:0.078831586638257\n",
      "train loss:0.009762706973730478\n",
      "train loss:0.06157287211683147\n",
      "train loss:0.03798949463064419\n",
      "train loss:0.061785426754279185\n",
      "train loss:0.007177153956097736\n",
      "train loss:0.03478512524029385\n",
      "train loss:0.028873552976197887\n",
      "train loss:0.02288444042850603\n",
      "train loss:0.05457669282095989\n",
      "train loss:0.0321562635137414\n",
      "train loss:0.05709613572339614\n",
      "train loss:0.019740322933360196\n",
      "train loss:0.05930850298816652\n",
      "train loss:0.01731915848040132\n",
      "train loss:0.020357641161865777\n",
      "train loss:0.01233650368551597\n",
      "train loss:0.01843894078120308\n",
      "train loss:0.009226928051624507\n",
      "train loss:0.015500422001394752\n",
      "train loss:0.017733038414020966\n",
      "train loss:0.013772190636959128\n",
      "train loss:0.022066743468405526\n",
      "train loss:0.09904841857425872\n",
      "train loss:0.0228927463625523\n",
      "train loss:0.019007341901421032\n",
      "train loss:0.038997693069930166\n",
      "train loss:0.02859410634638924\n",
      "train loss:0.06519415022901874\n",
      "train loss:0.018933968178932586\n",
      "train loss:0.015974147246469884\n",
      "train loss:0.04396323264952492\n",
      "train loss:0.04035136401086561\n",
      "train loss:0.014895815071613874\n",
      "train loss:0.012493468495186915\n",
      "train loss:0.055027028604860015\n",
      "train loss:0.029367412989007087\n",
      "train loss:0.0283417660217732\n",
      "train loss:0.036781602264964006\n",
      "train loss:0.0203133128168494\n",
      "train loss:0.04363937160289672\n",
      "train loss:0.006517328816834051\n",
      "train loss:0.02851458151146756\n",
      "train loss:0.06489767113131772\n",
      "train loss:0.03533154862410064\n",
      "train loss:0.016494091535024836\n",
      "train loss:0.02841884726221281\n",
      "train loss:0.10816697997111485\n",
      "train loss:0.02070770912235408\n",
      "train loss:0.05769486366133271\n",
      "train loss:0.06646863347276946\n",
      "train loss:0.029693433581761845\n",
      "train loss:0.0077046390534418795\n",
      "train loss:0.052806679983581385\n",
      "train loss:0.008412385203387517\n",
      "train loss:0.040711847907321325\n",
      "train loss:0.030310500377888752\n",
      "train loss:0.03909797748792359\n",
      "train loss:0.007021819693666219\n",
      "train loss:0.019171353367965504\n",
      "train loss:0.044304122535635315\n",
      "train loss:0.04105043991312225\n",
      "train loss:0.05452597780662474\n",
      "train loss:0.06442575297303782\n",
      "train loss:0.011627245980953571\n",
      "train loss:0.04608984936886389\n",
      "train loss:0.022221839932699803\n",
      "train loss:0.022551524388388543\n",
      "train loss:0.07032039814501337\n",
      "train loss:0.05030742041128223\n",
      "train loss:0.023850293330021918\n",
      "train loss:0.005699617779154521\n",
      "train loss:0.006910844270814485\n",
      "train loss:0.009102944320985556\n",
      "train loss:0.031117989054788478\n",
      "train loss:0.012757867154332943\n",
      "train loss:0.04271668189127196\n",
      "train loss:0.015338326270914693\n",
      "train loss:0.0523852060318897\n",
      "train loss:0.02545801493621596\n",
      "train loss:0.02978018836169849\n",
      "train loss:0.02866703765024474\n",
      "train loss:0.04606327238055521\n",
      "train loss:0.06325704211104076\n",
      "train loss:0.03329179171306743\n",
      "train loss:0.03148578182242754\n",
      "train loss:0.017697252466974626\n",
      "train loss:0.0418780970695044\n",
      "train loss:0.01384465382000773\n",
      "train loss:0.00492526681825328\n",
      "train loss:0.052257234420755\n",
      "train loss:0.02081480871965362\n",
      "train loss:0.04626529399036099\n",
      "train loss:0.018373228980431956\n",
      "train loss:0.10684047857369924\n",
      "train loss:0.019830588027202528\n",
      "train loss:0.0684921733716098\n",
      "train loss:0.01924078680980161\n",
      "train loss:0.012008042523390305\n",
      "train loss:0.035498924086002935\n",
      "train loss:0.03446648664734615\n",
      "train loss:0.06248992756756935\n",
      "train loss:0.022738673284211905\n",
      "train loss:0.11909087959049845\n",
      "train loss:0.10901582219091667\n",
      "train loss:0.003045563155790167\n",
      "train loss:0.014490277975724282\n",
      "train loss:0.041990095406416635\n",
      "train loss:0.07319216448348385\n",
      "train loss:0.013343199296113604\n",
      "train loss:0.029483287214120836\n",
      "train loss:0.015999719964884265\n",
      "train loss:0.03105648580515203\n",
      "train loss:0.020031801721589262\n",
      "train loss:0.02770761023577457\n",
      "train loss:0.006683891620967237\n",
      "train loss:0.11647933723000507\n",
      "train loss:0.01418032389159286\n",
      "train loss:0.041629250007343764\n",
      "train loss:0.03263065388639383\n",
      "train loss:0.0093188104544246\n",
      "train loss:0.013452555495035112\n",
      "train loss:0.011587273398428211\n",
      "train loss:0.03165825434348151\n",
      "train loss:0.007083985369530607\n",
      "train loss:0.07015476849365182\n",
      "train loss:0.00950023629261775\n",
      "train loss:0.007673999977455507\n",
      "train loss:0.024811555267067183\n",
      "train loss:0.04298194526879458\n",
      "train loss:0.08588228705387413\n",
      "train loss:0.009518274656574654\n",
      "train loss:0.010774460976994709\n",
      "train loss:0.06175606743891973\n",
      "train loss:0.014646549648317733\n",
      "train loss:0.01711354599134003\n",
      "train loss:0.010832648357689378\n",
      "train loss:0.013036741852698928\n",
      "train loss:0.018245024909121936\n",
      "train loss:0.04957135984665336\n",
      "train loss:0.03822410548970186\n",
      "train loss:0.020961804877518486\n",
      "train loss:0.011844178072114697\n",
      "train loss:0.026172649437806426\n",
      "train loss:0.027332685703203054\n",
      "train loss:0.0312953078540308\n",
      "train loss:0.044404105241792405\n",
      "train loss:0.025164266449714777\n",
      "train loss:0.015314609996650899\n",
      "train loss:0.01361334964823293\n",
      "train loss:0.01962373773097058\n",
      "train loss:0.03416449238640487\n",
      "train loss:0.019701216883428446\n",
      "train loss:0.00518602068706961\n",
      "train loss:0.052480036187410734\n",
      "train loss:0.026863106137353127\n",
      "train loss:0.002916774185440506\n",
      "train loss:0.03418600575976822\n",
      "train loss:0.023823842867282882\n",
      "train loss:0.01352182902734398\n",
      "train loss:0.20662635336862073\n",
      "train loss:0.013000451823342599\n",
      "train loss:0.03184860663116842\n",
      "train loss:0.016374510777590456\n",
      "train loss:0.07076742567480936\n",
      "train loss:0.09007359480710699\n",
      "train loss:0.059957786119440006\n",
      "train loss:0.02591035180190192\n",
      "train loss:0.04948993081844459\n",
      "train loss:0.0060554290351277976\n",
      "train loss:0.018318637804155692\n",
      "train loss:0.03772130605688957\n",
      "train loss:0.011543291341192339\n",
      "train loss:0.014603075880385438\n",
      "train loss:0.017721880714794816\n",
      "train loss:0.0641926429638817\n",
      "train loss:0.0219459979006549\n",
      "train loss:0.09872500142883055\n",
      "train loss:0.035331440375701934\n",
      "train loss:0.013185058747626453\n",
      "train loss:0.04565711079701556\n",
      "train loss:0.02326665290480513\n",
      "train loss:0.03291652184753146\n",
      "train loss:0.016420272377550515\n",
      "train loss:0.0038117837983008152\n",
      "train loss:0.009195043975462785\n",
      "train loss:0.016668984068743745\n",
      "train loss:0.01077592492870008\n",
      "train loss:0.05210162196366628\n",
      "train loss:0.004378576817595483\n",
      "train loss:0.02280499819171324\n",
      "train loss:0.026591461369427073\n",
      "train loss:0.019948910174161515\n",
      "train loss:0.05250282408025012\n",
      "train loss:0.005049378506845428\n",
      "train loss:0.042002088564514574\n",
      "train loss:0.011701242794415347\n",
      "train loss:0.03347150985426231\n",
      "train loss:0.028967539463756572\n",
      "train loss:0.06279735655865343\n",
      "train loss:0.038120958042077914\n",
      "train loss:0.03041001487254924\n",
      "train loss:0.01837142076831389\n",
      "train loss:0.05163601597036228\n",
      "train loss:0.02067641880442575\n",
      "train loss:0.03895428301824766\n",
      "train loss:0.019546136416672036\n",
      "train loss:0.0054065800893604125\n",
      "train loss:0.024923201286083118\n",
      "train loss:0.10984676795100523\n",
      "train loss:0.05372217087998801\n",
      "train loss:0.04810547575447627\n",
      "train loss:0.06145346728331056\n",
      "train loss:0.012349411556011993\n",
      "train loss:0.03222432273418603\n",
      "train loss:0.06934067650712461\n",
      "train loss:0.023075404841198414\n",
      "train loss:0.026000902927974256\n",
      "train loss:0.0342818510315468\n",
      "train loss:0.02114449879927761\n",
      "train loss:0.00458637297697198\n",
      "train loss:0.01857200120187221\n",
      "train loss:0.06652208100994655\n",
      "train loss:0.006813592439631312\n",
      "train loss:0.06929890045459905\n",
      "train loss:0.029715383656061915\n",
      "train loss:0.05052113965167731\n",
      "train loss:0.012490553003963649\n",
      "train loss:0.07165877696908991\n",
      "train loss:0.01965807564352097\n",
      "train loss:0.04699452037846787\n",
      "train loss:0.0164538555784594\n",
      "train loss:0.059066994654659505\n",
      "train loss:0.06271411225709346\n",
      "train loss:0.017747555713786972\n",
      "train loss:0.04771173823763972\n",
      "train loss:0.0053983496242775795\n",
      "train loss:0.026285444931405867\n",
      "train loss:0.023772137930357484\n",
      "train loss:0.004530454123403149\n",
      "train loss:0.006594496645918375\n",
      "train loss:0.012950411368998266\n",
      "train loss:0.0222660494828839\n",
      "train loss:0.012218782313019396\n",
      "train loss:0.02329181921725918\n",
      "=== epoch:5, train acc:0.988, test acc:0.984 ===\n",
      "train loss:0.012391437579437878\n",
      "train loss:0.007403597105437899\n",
      "train loss:0.22095819833004907\n",
      "train loss:0.015930655209574804\n",
      "train loss:0.005741919247024639\n",
      "train loss:0.03626582616102336\n",
      "train loss:0.04795293216339644\n",
      "train loss:0.03437819712310948\n",
      "train loss:0.019011956296092425\n",
      "train loss:0.03177482450742378\n",
      "train loss:0.005176706109531399\n",
      "train loss:0.08374108579276238\n",
      "train loss:0.0712813370513163\n",
      "train loss:0.020861448489993165\n",
      "train loss:0.07493227903950479\n",
      "train loss:0.023588255634022177\n",
      "train loss:0.01656824647372411\n",
      "train loss:0.003978026375675675\n",
      "train loss:0.04527869878354322\n",
      "train loss:0.015756634204797296\n",
      "train loss:0.011584547370092449\n",
      "train loss:0.14050582523444385\n",
      "train loss:0.004737186954205055\n",
      "train loss:0.03415367163330571\n",
      "train loss:0.07853090966890454\n",
      "train loss:0.014884275158325171\n",
      "train loss:0.03498086898386506\n",
      "train loss:0.07673302001326453\n",
      "train loss:0.028652662164108375\n",
      "train loss:0.04930980707066685\n",
      "train loss:0.006464494325606756\n",
      "train loss:0.0219472715006649\n",
      "train loss:0.028382662309329742\n",
      "train loss:0.08065750138656712\n",
      "train loss:0.04258016285676974\n",
      "train loss:0.011845752349244749\n",
      "train loss:0.09537919551403831\n",
      "train loss:0.014534266279619791\n",
      "train loss:0.022557075809227625\n",
      "train loss:0.056538044639894744\n",
      "train loss:0.004958029916118905\n",
      "train loss:0.03918795427208175\n",
      "train loss:0.02392644210864164\n",
      "train loss:0.017436605120587076\n",
      "train loss:0.14776214531307189\n",
      "train loss:0.03323139392869434\n",
      "train loss:0.012296673067295195\n",
      "train loss:0.07472177491608785\n",
      "train loss:0.017176856596869856\n",
      "train loss:0.011632711788829592\n",
      "train loss:0.04739339887489205\n",
      "train loss:0.04099659693458422\n",
      "train loss:0.02399942286899408\n",
      "train loss:0.026715807545266146\n",
      "train loss:0.06012316840150223\n",
      "train loss:0.026784321630685476\n",
      "train loss:0.005613157638667298\n",
      "train loss:0.03506383945143823\n",
      "train loss:0.025610395523539175\n",
      "train loss:0.023783113122795824\n",
      "train loss:0.03075000971999107\n",
      "train loss:0.019298327763218236\n",
      "train loss:0.01688150401695337\n",
      "train loss:0.03864051541676457\n",
      "train loss:0.025201027160419623\n",
      "train loss:0.07795630667297934\n",
      "train loss:0.03815048561437263\n",
      "train loss:0.01649576380450485\n",
      "train loss:0.026263202311927695\n",
      "train loss:0.06557267224990075\n",
      "train loss:0.07210556008517777\n",
      "train loss:0.0084281373762462\n",
      "train loss:0.01844569904586954\n",
      "train loss:0.06332465529692856\n",
      "train loss:0.04742675524019264\n",
      "train loss:0.035957805210070054\n",
      "train loss:0.035456565277741624\n",
      "train loss:0.0615467027202142\n",
      "train loss:0.008898487679160242\n",
      "train loss:0.023212779527594703\n",
      "train loss:0.09138363486803033\n",
      "train loss:0.005161267499440287\n",
      "train loss:0.027972420181142845\n",
      "train loss:0.05479610495025597\n",
      "train loss:0.03573547664048484\n",
      "train loss:0.011091892612745912\n",
      "train loss:0.04081358953048118\n",
      "train loss:0.021079470510508815\n",
      "train loss:0.022255386049594365\n",
      "train loss:0.010270792397662846\n",
      "train loss:0.007905246632123218\n",
      "train loss:0.007698053360395984\n",
      "train loss:0.03727932197871056\n",
      "train loss:0.013123191566024525\n",
      "train loss:0.04411017477653505\n",
      "train loss:0.006767529497756564\n",
      "train loss:0.07699369543282363\n",
      "train loss:0.013291036853447562\n",
      "train loss:0.019291785234183397\n",
      "train loss:0.08190315522716161\n",
      "train loss:0.019825223088738915\n",
      "train loss:0.013553017406769206\n",
      "train loss:0.01751996306263197\n",
      "train loss:0.07644778560550977\n",
      "train loss:0.09182397680269663\n",
      "train loss:0.030814359022302847\n",
      "train loss:0.02338701935452449\n",
      "train loss:0.021894400493799284\n",
      "train loss:0.012994578688424272\n",
      "train loss:0.055580547513476825\n",
      "train loss:0.02840008157933861\n",
      "train loss:0.04693542304414017\n",
      "train loss:0.02974307710296542\n",
      "train loss:0.016616235864548683\n",
      "train loss:0.01751451558646822\n",
      "train loss:0.010866768128737334\n",
      "train loss:0.016113947040441105\n",
      "train loss:0.017364456974251583\n",
      "train loss:0.004675902639698453\n",
      "train loss:0.022978095776147144\n",
      "train loss:0.018668029361797556\n",
      "train loss:0.06314938536171608\n",
      "train loss:0.014273659824132654\n",
      "train loss:0.013503563834030336\n",
      "train loss:0.031091827030316342\n",
      "train loss:0.0498015708688291\n",
      "train loss:0.028502779879506824\n",
      "train loss:0.010181323589388414\n",
      "train loss:0.010773220691812961\n",
      "train loss:0.01014163902524324\n",
      "train loss:0.013067934604270404\n",
      "train loss:0.00998160708740948\n",
      "train loss:0.04700444772223225\n",
      "train loss:0.03064520126305017\n",
      "train loss:0.027569280120634452\n",
      "train loss:0.024560788097030016\n",
      "train loss:0.010483098102547373\n",
      "train loss:0.0676152235564712\n",
      "train loss:0.005351019922018788\n",
      "train loss:0.03604406652995306\n",
      "train loss:0.06521763129518232\n",
      "train loss:0.008094531906071355\n",
      "train loss:0.02201046995560474\n",
      "train loss:0.030479105157475313\n",
      "train loss:0.0321634334169607\n",
      "train loss:0.06869151236235008\n",
      "train loss:0.0026117377594551154\n",
      "train loss:0.07450411951117933\n",
      "train loss:0.04765672928789686\n",
      "train loss:0.02053311586660977\n",
      "train loss:0.01857026041869579\n",
      "train loss:0.014442020021629394\n",
      "train loss:0.05280755659771072\n",
      "train loss:0.00851266336870344\n",
      "train loss:0.0803831116496652\n",
      "train loss:0.023695993318222728\n",
      "train loss:0.04121054201519502\n",
      "train loss:0.04406863727404051\n",
      "train loss:0.05264993925330298\n",
      "train loss:0.017590084389618236\n",
      "train loss:0.02262118841242516\n",
      "train loss:0.017427282749632237\n",
      "train loss:0.029564168040705788\n",
      "train loss:0.023474815983755355\n",
      "train loss:0.030544537049311583\n",
      "train loss:0.024658741011224686\n",
      "train loss:0.05292803201157677\n",
      "train loss:0.02500676825154646\n",
      "train loss:0.03664236149992742\n",
      "train loss:0.058902611043159483\n",
      "train loss:0.025426435724795506\n",
      "train loss:0.02314787497043482\n",
      "train loss:0.017635866043816114\n",
      "train loss:0.009543193922331487\n",
      "train loss:0.015222079903905526\n",
      "train loss:0.026706145079243754\n",
      "train loss:0.06492179608273153\n",
      "train loss:0.018166975205692163\n",
      "train loss:0.020283292089977076\n",
      "train loss:0.013251198494454568\n",
      "train loss:0.04262675470367383\n",
      "train loss:0.02828217531808428\n",
      "train loss:0.04828772499480138\n",
      "train loss:0.07493659106221316\n",
      "train loss:0.03320972018041445\n",
      "train loss:0.015629678213933247\n",
      "train loss:0.01803574776593822\n",
      "train loss:0.009360564711392606\n",
      "train loss:0.008347727889361604\n",
      "train loss:0.04134921680864172\n",
      "train loss:0.016932167855423483\n",
      "train loss:0.04095080359755742\n",
      "train loss:0.06802871136602465\n",
      "train loss:0.007900234962165725\n",
      "train loss:0.039573451894177054\n",
      "train loss:0.07121005363910982\n",
      "train loss:0.004873772170857909\n",
      "train loss:0.03731481723290246\n",
      "train loss:0.005113751199470222\n",
      "train loss:0.05950519502994117\n",
      "train loss:0.025871710682704593\n",
      "train loss:0.021076809921593317\n",
      "train loss:0.017934346704237866\n",
      "train loss:0.011847058310031942\n",
      "train loss:0.03332590290192775\n",
      "train loss:0.07949330040916266\n",
      "train loss:0.009231982010712464\n",
      "train loss:0.05057137163159808\n",
      "train loss:0.034335579549681\n",
      "train loss:0.1413451517945996\n",
      "train loss:0.03561927217641986\n",
      "train loss:0.005469345076468051\n",
      "train loss:0.01922634766105079\n",
      "train loss:0.029333504594341253\n",
      "train loss:0.02182037749626731\n",
      "train loss:0.006489793569210836\n",
      "train loss:0.013039604140516286\n",
      "train loss:0.03201800009910553\n",
      "train loss:0.08885646525186684\n",
      "train loss:0.04988916105375465\n",
      "train loss:0.007799561151877794\n",
      "train loss:0.020123649967878788\n",
      "train loss:0.03700838247386355\n",
      "train loss:0.00787980630167596\n",
      "train loss:0.016272193006937396\n",
      "train loss:0.010236605208403022\n",
      "train loss:0.0042125303836574755\n",
      "train loss:0.10641513771432923\n",
      "train loss:0.00569703162448989\n",
      "train loss:0.08346786447615127\n",
      "train loss:0.015904911823993013\n",
      "train loss:0.014796894983991402\n",
      "train loss:0.01237723149283209\n",
      "train loss:0.008036083626073838\n",
      "train loss:0.004184985790783735\n",
      "train loss:0.04120896867794059\n",
      "train loss:0.014659912166535106\n",
      "train loss:0.0067661706353848\n",
      "train loss:0.06532836629776458\n",
      "train loss:0.06795919615303661\n",
      "train loss:0.02497303361662776\n",
      "train loss:0.015789888337199288\n",
      "train loss:0.018561725742375166\n",
      "train loss:0.004372197103501363\n",
      "train loss:0.038377162057153774\n",
      "train loss:0.038707125792990627\n",
      "train loss:0.08579258400812882\n",
      "train loss:0.03702902617696848\n",
      "train loss:0.02255312147865278\n",
      "train loss:0.012558538301886923\n",
      "train loss:0.021861359032662758\n",
      "train loss:0.007342614547322949\n",
      "train loss:0.017372320326323056\n",
      "train loss:0.019878895596083777\n",
      "train loss:0.016657646722517485\n",
      "train loss:0.006855333393781632\n",
      "train loss:0.039235134706569995\n",
      "train loss:0.007980448959124135\n",
      "train loss:0.023618309616015007\n",
      "train loss:0.03258116059382208\n",
      "train loss:0.028538928399296382\n",
      "train loss:0.03004464704539672\n",
      "train loss:0.021809271001398855\n",
      "train loss:0.0060244998773986876\n",
      "train loss:0.025531487912011062\n",
      "train loss:0.019019004481169003\n",
      "train loss:0.07659772720371982\n",
      "train loss:0.021479896541060264\n",
      "train loss:0.024878508887673532\n",
      "train loss:0.02599272767517312\n",
      "train loss:0.051621025519937025\n",
      "train loss:0.04332854314478181\n",
      "train loss:0.04086020693756701\n",
      "train loss:0.06587860749108104\n",
      "train loss:0.008933903240775608\n",
      "train loss:0.0471768886051347\n",
      "train loss:0.06473151386751795\n",
      "train loss:0.03253566873591687\n",
      "train loss:0.01027030691327559\n",
      "train loss:0.010710321163664835\n",
      "train loss:0.029358083115883775\n",
      "train loss:0.02302460505482315\n",
      "train loss:0.02967779636631632\n",
      "train loss:0.01717816021034407\n",
      "train loss:0.012210452745475418\n",
      "train loss:0.008756432897289904\n",
      "train loss:0.01398464554729123\n",
      "train loss:0.011362071279402619\n",
      "train loss:0.011616192722704586\n",
      "train loss:0.05665585591871441\n",
      "train loss:0.012378294411697726\n",
      "train loss:0.023417171528869823\n",
      "train loss:0.010927747243471467\n",
      "train loss:0.014663140313502479\n",
      "train loss:0.004692532789589309\n",
      "train loss:0.067241702801628\n",
      "train loss:0.024008124770752298\n",
      "train loss:0.07325135199825566\n",
      "train loss:0.03987553645408371\n",
      "train loss:0.0388782799197429\n",
      "train loss:0.13644519527258733\n",
      "train loss:0.026620045346835607\n",
      "train loss:0.019986542605986344\n",
      "train loss:0.028986590143755705\n",
      "train loss:0.10667092183291177\n",
      "train loss:0.0066441087831040505\n",
      "train loss:0.005890295515633673\n",
      "train loss:0.009193542272666213\n",
      "train loss:0.006803720646873559\n",
      "train loss:0.008402830986922714\n",
      "train loss:0.01567517637227796\n",
      "train loss:0.006761855786702045\n",
      "train loss:0.018679010235182727\n",
      "train loss:0.11941381892698207\n",
      "train loss:0.016561327939959573\n",
      "train loss:0.07288287890577684\n",
      "train loss:0.04841809317672476\n",
      "train loss:0.028175169410202813\n",
      "train loss:0.018319988528998604\n",
      "train loss:0.021344660365655393\n",
      "train loss:0.029712696693984113\n",
      "train loss:0.010620899052048738\n",
      "train loss:0.009554899562465158\n",
      "train loss:0.024139688604499727\n",
      "train loss:0.005323162548727468\n",
      "train loss:0.0068385398645752936\n",
      "train loss:0.011575607131479225\n",
      "train loss:0.061321385959930697\n",
      "train loss:0.04865344695079218\n",
      "train loss:0.0725314622743975\n",
      "train loss:0.055873352932160995\n",
      "train loss:0.015630035775100554\n",
      "train loss:0.03539969890085774\n",
      "train loss:0.013612898546201927\n",
      "train loss:0.14728700974079964\n",
      "train loss:0.046582147195270124\n",
      "train loss:0.0044209059962431515\n",
      "train loss:0.03715165106054795\n",
      "train loss:0.03867716701468805\n",
      "train loss:0.025016865974422813\n",
      "train loss:0.039122482909793914\n",
      "train loss:0.021975592311275025\n",
      "train loss:0.02145186654142862\n",
      "train loss:0.07297840183731114\n",
      "train loss:0.06445772342115028\n",
      "train loss:0.03313506837503467\n",
      "train loss:0.14320717829170726\n",
      "train loss:0.016946347549318976\n",
      "train loss:0.012154633213427803\n",
      "train loss:0.007564583146544945\n",
      "train loss:0.028463552896503014\n",
      "train loss:0.04093736549315863\n",
      "train loss:0.031008003888613987\n",
      "train loss:0.019842887396439114\n",
      "train loss:0.021031320705899925\n",
      "train loss:0.009337774595996422\n",
      "train loss:0.01223607140197084\n",
      "train loss:0.04564923921052974\n",
      "train loss:0.021285379553716625\n",
      "train loss:0.03166589658552733\n",
      "train loss:0.009636313905213311\n",
      "train loss:0.008543547694441983\n",
      "train loss:0.00809961453200343\n",
      "train loss:0.013202028037426419\n",
      "train loss:0.04364421412386669\n",
      "train loss:0.005546539219703815\n",
      "train loss:0.01902751387088821\n",
      "train loss:0.012628622167652958\n",
      "train loss:0.035103815876565754\n",
      "train loss:0.012108989920475712\n",
      "train loss:0.01681817618265297\n",
      "train loss:0.035342029116332704\n",
      "train loss:0.004940053519160741\n",
      "train loss:0.0770557272383519\n",
      "train loss:0.023466445809109705\n",
      "train loss:0.0722330456405148\n",
      "train loss:0.00406859450104494\n",
      "train loss:0.03459034489668908\n",
      "train loss:0.04033833573356552\n",
      "train loss:0.013117734834656171\n",
      "train loss:0.004853656426406536\n",
      "train loss:0.012798663805652722\n",
      "train loss:0.05247481639184704\n",
      "train loss:0.1001753458126592\n",
      "train loss:0.012008330102044228\n",
      "train loss:0.023138484911859623\n",
      "train loss:0.019547240872315404\n",
      "train loss:0.019824991609053207\n",
      "train loss:0.010353797209286121\n",
      "train loss:0.03883094253930623\n",
      "train loss:0.012436642247171219\n",
      "train loss:0.04985499682348064\n",
      "train loss:0.05786136328959632\n",
      "train loss:0.022870607574218136\n",
      "train loss:0.005386091033352363\n",
      "train loss:0.060090844325619244\n",
      "train loss:0.022708735516808066\n",
      "train loss:0.03370105081087605\n",
      "train loss:0.02307124976691098\n",
      "train loss:0.029753300103035472\n",
      "train loss:0.0227665569168973\n",
      "train loss:0.017312307526222342\n",
      "train loss:0.021969839450707854\n",
      "train loss:0.008619743834759955\n",
      "train loss:0.012693055866868303\n",
      "train loss:0.019403123694803754\n",
      "train loss:0.07169910773312244\n",
      "train loss:0.011781443536778995\n",
      "train loss:0.0082556590098449\n",
      "train loss:0.00919623468900948\n",
      "train loss:0.015667094518951617\n",
      "train loss:0.02964651032559375\n",
      "train loss:0.024390797595836847\n",
      "train loss:0.014128595494014047\n",
      "train loss:0.11873810741104487\n",
      "train loss:0.01413566790801978\n",
      "train loss:0.009370696633998121\n",
      "train loss:0.06515195103757075\n",
      "train loss:0.005168422984895848\n",
      "train loss:0.04249245042716441\n",
      "train loss:0.01702518956401872\n",
      "train loss:0.06508610273672807\n",
      "train loss:0.018596260300144093\n",
      "train loss:0.05227016901360379\n",
      "train loss:0.024744980635797907\n",
      "train loss:0.04014138391198792\n",
      "train loss:0.01019784633204338\n",
      "train loss:0.004503758156428053\n",
      "train loss:0.011184042281460931\n",
      "train loss:0.03302670622099918\n",
      "train loss:0.008768823760492803\n",
      "train loss:0.003275729807127226\n",
      "train loss:0.013288567108759551\n",
      "train loss:0.022033007013571103\n",
      "train loss:0.04457638768976507\n",
      "train loss:0.05382788826971502\n",
      "train loss:0.005162036800755974\n",
      "train loss:0.01534359025294545\n",
      "train loss:0.02472921452253933\n",
      "train loss:0.01838786276606196\n",
      "train loss:0.04521056161781388\n",
      "train loss:0.011457012661914774\n",
      "train loss:0.008063573767803011\n",
      "train loss:0.03152049633719834\n",
      "train loss:0.023819353570076675\n",
      "train loss:0.010323984490113286\n",
      "train loss:0.014320996030914966\n",
      "train loss:0.026391367285459467\n",
      "train loss:0.024778476996178335\n",
      "train loss:0.01776490504999946\n",
      "train loss:0.013836016534173337\n",
      "train loss:0.039751705148794045\n",
      "train loss:0.006490813834730063\n",
      "train loss:0.005311262575628557\n",
      "train loss:0.014767778037782543\n",
      "train loss:0.019459008922297818\n",
      "train loss:0.008105552997637312\n",
      "train loss:0.022055529596267144\n",
      "train loss:0.008411149215972443\n",
      "train loss:0.0051774596345609694\n",
      "train loss:0.021374485527271413\n",
      "train loss:0.020414724931593263\n",
      "train loss:0.003577830679522851\n",
      "train loss:0.006076226688402509\n",
      "train loss:0.022997144202871328\n",
      "train loss:0.024547235128796738\n",
      "train loss:0.04344331670470697\n",
      "train loss:0.020213094353745853\n",
      "train loss:0.00878998537946023\n",
      "train loss:0.012403870753145661\n",
      "train loss:0.028508213153894468\n",
      "train loss:0.02303189733474644\n",
      "train loss:0.013115282293714696\n",
      "train loss:0.019793320979811017\n",
      "train loss:0.014586859264237338\n",
      "train loss:0.03811177799773071\n",
      "train loss:0.011613666884678426\n",
      "train loss:0.006806106364379179\n",
      "train loss:0.006254472408063495\n",
      "train loss:0.04509800554419719\n",
      "train loss:0.02507116829954386\n",
      "train loss:0.01884713292863285\n",
      "train loss:0.017031943559484054\n",
      "train loss:0.03608438554592752\n",
      "train loss:0.0071665623143500525\n",
      "train loss:0.013515888106838056\n",
      "train loss:0.015641759525470523\n",
      "train loss:0.006526355041715945\n",
      "train loss:0.003504093684998946\n",
      "train loss:0.018453870729024987\n",
      "train loss:0.01753412158940152\n",
      "train loss:0.01655417907306826\n",
      "train loss:0.020228923553559222\n",
      "train loss:0.029215080643864477\n",
      "train loss:0.023248399574494105\n",
      "train loss:0.0462285611454837\n",
      "train loss:0.004701190921740135\n",
      "train loss:0.008440601314761247\n",
      "train loss:0.009397105019003437\n",
      "train loss:0.0406060195554508\n",
      "train loss:0.05340753802924476\n",
      "train loss:0.011823028420884039\n",
      "train loss:0.037035716187405955\n",
      "train loss:0.09210165946956973\n",
      "train loss:0.004510256935315779\n",
      "train loss:0.004054864920939516\n",
      "train loss:0.004496984040285044\n",
      "train loss:0.06412887217494503\n",
      "train loss:0.011627131527720373\n",
      "train loss:0.008179579108781827\n",
      "train loss:0.029195990496774807\n",
      "train loss:0.002321650190978609\n",
      "train loss:0.016921154895197042\n",
      "train loss:0.013463343603598665\n",
      "train loss:0.03226834217651088\n",
      "train loss:0.08708587273478771\n",
      "train loss:0.024787653617981478\n",
      "train loss:0.01601272616327504\n",
      "train loss:0.007604721815565084\n",
      "train loss:0.045456067679668866\n",
      "train loss:0.0649572600283201\n",
      "train loss:0.00575726555209764\n",
      "train loss:0.02568945038508558\n",
      "train loss:0.026144164078783692\n",
      "train loss:0.031639585420855085\n",
      "train loss:0.02252769262007893\n",
      "train loss:0.01808645941470423\n",
      "train loss:0.018072276782537665\n",
      "train loss:0.051224920723525934\n",
      "train loss:0.018308058780685556\n",
      "train loss:0.004510424635664323\n",
      "train loss:0.020092893564277854\n",
      "train loss:0.0038990363766451923\n",
      "train loss:0.013945734348027563\n",
      "train loss:0.007534758714763492\n",
      "train loss:0.00830639412090727\n",
      "train loss:0.02364151214150969\n",
      "train loss:0.032298097401748514\n",
      "train loss:0.008327096349994227\n",
      "train loss:0.10914897918319738\n",
      "train loss:0.022549894891476095\n",
      "train loss:0.010421194535949025\n",
      "train loss:0.008896923519272203\n",
      "train loss:0.04527618455146865\n",
      "train loss:0.01593991462131375\n",
      "train loss:0.024895025446955807\n",
      "train loss:0.007709856022477649\n",
      "train loss:0.05117169000172145\n",
      "train loss:0.007537777317643712\n",
      "train loss:0.06270726197000111\n",
      "train loss:0.011774157856809997\n",
      "train loss:0.015317718984701398\n",
      "train loss:0.02470296484172298\n",
      "train loss:0.03890346086929406\n",
      "train loss:0.008821609417411655\n",
      "train loss:0.019488516175727562\n",
      "train loss:0.006712406892446229\n",
      "train loss:0.01777675154962304\n",
      "train loss:0.054149390248696624\n",
      "train loss:0.01490254633312507\n",
      "train loss:0.016546698777017865\n",
      "train loss:0.016247052431108258\n",
      "train loss:0.022230188188599072\n",
      "train loss:0.0403512058319256\n",
      "train loss:0.018889998380518584\n",
      "train loss:0.01918374992768427\n",
      "train loss:0.01124258173494129\n",
      "train loss:0.009326271148126104\n",
      "train loss:0.019097100992482064\n",
      "train loss:0.009635045937667744\n",
      "train loss:0.024325859401434138\n",
      "train loss:0.015275514982261628\n",
      "train loss:0.013585640026853245\n",
      "train loss:0.005343450390592906\n",
      "train loss:0.009535918836318635\n",
      "train loss:0.035753845508331\n",
      "train loss:0.006319982916522895\n",
      "train loss:0.01695249391183484\n",
      "train loss:0.010334007712058049\n",
      "train loss:0.03664171572964641\n",
      "train loss:0.06870178143908956\n",
      "train loss:0.024151150785079484\n",
      "train loss:0.009470605138506089\n",
      "train loss:0.018288978540204216\n",
      "train loss:0.04955041248483015\n",
      "train loss:0.03076778554700028\n",
      "train loss:0.00491456882937074\n",
      "train loss:0.05905984046827672\n",
      "train loss:0.04522406784537535\n",
      "train loss:0.01387865967474895\n",
      "train loss:0.015920395248107108\n",
      "train loss:0.02416701267496127\n",
      "train loss:0.02103070758884755\n",
      "train loss:0.034354077483776335\n",
      "train loss:0.004428864662167034\n",
      "train loss:0.0076476034103218245\n",
      "train loss:0.008297164713527335\n",
      "train loss:0.00806799743845861\n",
      "train loss:0.01825818326607807\n",
      "train loss:0.011269563144251309\n",
      "=== epoch:6, train acc:0.989, test acc:0.981 ===\n",
      "train loss:0.00998813065744477\n",
      "train loss:0.004363781022762663\n",
      "train loss:0.09704706209793061\n",
      "train loss:0.014489521431496283\n",
      "train loss:0.0045755171871920556\n",
      "train loss:0.017249463011752106\n",
      "train loss:0.02083162667089943\n",
      "train loss:0.009181106921167326\n",
      "train loss:0.03846813035042985\n",
      "train loss:0.008133191277615139\n",
      "train loss:0.02501208408954192\n",
      "train loss:0.015579988983206262\n",
      "train loss:0.08078372474468219\n",
      "train loss:0.004101934333220755\n",
      "train loss:0.02175470730288526\n",
      "train loss:0.014550187806432115\n",
      "train loss:0.03268148497551075\n",
      "train loss:0.013962207281565551\n",
      "train loss:0.010819655135064048\n",
      "train loss:0.015397474415728456\n",
      "train loss:0.0019916849019919834\n",
      "train loss:0.024875355647923158\n",
      "train loss:0.012711296670403063\n",
      "train loss:0.0068226486910684435\n",
      "train loss:0.00578744725142194\n",
      "train loss:0.009699410868725849\n",
      "train loss:0.016308957518546333\n",
      "train loss:0.01673490663182401\n",
      "train loss:0.018664091590062687\n",
      "train loss:0.027559260692803466\n",
      "train loss:0.005112994561394187\n",
      "train loss:0.009800015689835385\n",
      "train loss:0.010896923497794902\n",
      "train loss:0.006915825997745393\n",
      "train loss:0.016077331508836536\n",
      "train loss:0.022804855252964988\n",
      "train loss:0.003150633269826992\n",
      "train loss:0.038089807503864016\n",
      "train loss:0.017224879379279582\n",
      "train loss:0.007342564781183613\n",
      "train loss:0.018956526339360458\n",
      "train loss:0.004550000877273212\n",
      "train loss:0.01160642436529822\n",
      "train loss:0.004846754756257229\n",
      "train loss:0.025168265741774803\n",
      "train loss:0.06606766752263873\n",
      "train loss:0.11072293886564365\n",
      "train loss:0.018680319376467927\n",
      "train loss:0.02021660496299419\n",
      "train loss:0.01703023268805201\n",
      "train loss:0.006096607142679014\n",
      "train loss:0.08131343251654409\n",
      "train loss:0.07604684916827313\n",
      "train loss:0.0065550764276317034\n",
      "train loss:0.050222730136847395\n",
      "train loss:0.03438450801985302\n",
      "train loss:0.011726573013222875\n",
      "train loss:0.030400500562529697\n",
      "train loss:0.012028493061279568\n",
      "train loss:0.01393639906771404\n",
      "train loss:0.024694696739453566\n",
      "train loss:0.01419727962399952\n",
      "train loss:0.010798218528215749\n",
      "train loss:0.007575252938914023\n",
      "train loss:0.010176803121123477\n",
      "train loss:0.004584724918031499\n",
      "train loss:0.010998329392802111\n",
      "train loss:0.04692509428747984\n",
      "train loss:0.006284417148048801\n",
      "train loss:0.035747623020368394\n",
      "train loss:0.006974201739296665\n",
      "train loss:0.02397187520212786\n",
      "train loss:0.006633573438070567\n",
      "train loss:0.006952737072658679\n",
      "train loss:0.05845590635315798\n",
      "train loss:0.023298297051520388\n",
      "train loss:0.0024250043386239693\n",
      "train loss:0.00866611460479988\n",
      "train loss:0.014290824725736862\n",
      "train loss:0.05612692590919275\n",
      "train loss:0.0065966831802522765\n",
      "train loss:0.04449818289062591\n",
      "train loss:0.014975745387742078\n",
      "train loss:0.03780978010404299\n",
      "train loss:0.009778513025941267\n",
      "train loss:0.055552658893585655\n",
      "train loss:0.019995639435049985\n",
      "train loss:0.014509402865884573\n",
      "train loss:0.015840790341544172\n",
      "train loss:0.06116096825621855\n",
      "train loss:0.01901195511452075\n",
      "train loss:0.010899810873063487\n",
      "train loss:0.016998118951291684\n",
      "train loss:0.005295381803885541\n",
      "train loss:0.03578183541426581\n",
      "train loss:0.028122117868343705\n",
      "train loss:0.031248302201041498\n",
      "train loss:0.032533141348921296\n",
      "train loss:0.03154315047990162\n",
      "train loss:0.007100338305192481\n",
      "train loss:0.015199665915239748\n",
      "train loss:0.01751811608453263\n",
      "train loss:0.0018767945005927669\n",
      "train loss:0.012013859333278578\n",
      "train loss:0.030471338742900422\n",
      "train loss:0.008955893599738\n",
      "train loss:0.008970795789641984\n",
      "train loss:0.034593786103735744\n",
      "train loss:0.015699868157702083\n",
      "train loss:0.02742596165080592\n",
      "train loss:0.03883266241853021\n",
      "train loss:0.030458812358120074\n",
      "train loss:0.002627958794786129\n",
      "train loss:0.012981894881921611\n",
      "train loss:0.019427082341997047\n",
      "train loss:0.021807542941645733\n",
      "train loss:0.04309098018017947\n",
      "train loss:0.005325995647052518\n",
      "train loss:0.024855012992333173\n",
      "train loss:0.00753809852172367\n",
      "train loss:0.006331622090867207\n",
      "train loss:0.04666725556065065\n",
      "train loss:0.01638792886653688\n",
      "train loss:0.008536822952441812\n",
      "train loss:0.01092021687248964\n",
      "train loss:0.013388667042799635\n",
      "train loss:0.009045103278143774\n",
      "train loss:0.03864198596262846\n",
      "train loss:0.011032996883092823\n",
      "train loss:0.03502561554965661\n",
      "train loss:0.0064051634004654015\n",
      "train loss:0.07841168650189877\n",
      "train loss:0.0027064872838525778\n",
      "train loss:0.008468539401736561\n",
      "train loss:0.03036777553179883\n",
      "train loss:0.005491856221773061\n",
      "train loss:0.01596309581362149\n",
      "train loss:0.02452901628819533\n",
      "train loss:0.010477192198874418\n",
      "train loss:0.01065085678636966\n",
      "train loss:0.010985733086133103\n",
      "train loss:0.0031069056736181848\n",
      "train loss:0.022276385806344598\n",
      "train loss:0.016389769926459005\n",
      "train loss:0.005079426998699404\n",
      "train loss:0.03205849368191892\n",
      "train loss:0.008724107403170995\n",
      "train loss:0.012809587453644218\n",
      "train loss:0.017951101906893602\n",
      "train loss:0.023604603071764062\n",
      "train loss:0.023509316868825954\n",
      "train loss:0.023995638500544952\n",
      "train loss:0.0030256621769687024\n",
      "train loss:0.008581713121334489\n",
      "train loss:0.005658834860405083\n",
      "train loss:0.019300678926244704\n",
      "train loss:0.011503625946090903\n",
      "train loss:0.01178904400855053\n",
      "train loss:0.012584518719010904\n",
      "train loss:0.043391631992220486\n",
      "train loss:0.004710576965603125\n",
      "train loss:0.03409397560090048\n",
      "train loss:0.013462720085205658\n",
      "train loss:0.012705926680677866\n",
      "train loss:0.00921960627645745\n",
      "train loss:0.013902227988695136\n",
      "train loss:0.02146866929582249\n",
      "train loss:0.0289360465961167\n",
      "train loss:0.004577539553675925\n",
      "train loss:0.009800087143077589\n",
      "train loss:0.017666376120581837\n",
      "train loss:0.024636900964084538\n",
      "train loss:0.007113996175634514\n",
      "train loss:0.011973039610820519\n",
      "train loss:0.017382623148978933\n",
      "train loss:0.011493803982309338\n",
      "train loss:0.003289030973554597\n",
      "train loss:0.003104023958462874\n",
      "train loss:0.0070226213497197\n",
      "train loss:0.010372614348215984\n",
      "train loss:0.011541192699633223\n",
      "train loss:0.005273148465386735\n",
      "train loss:0.01171494889581103\n",
      "train loss:0.02371188170840341\n",
      "train loss:0.04423666077621035\n",
      "train loss:0.013520463432409917\n",
      "train loss:0.048630368868977286\n",
      "train loss:0.011576702383514175\n",
      "train loss:0.005280612145034109\n",
      "train loss:0.037007270540247814\n",
      "train loss:0.014908962350063107\n",
      "train loss:0.004614300027681504\n",
      "train loss:0.010463541619357286\n",
      "train loss:0.03679911989761259\n",
      "train loss:0.014226341982066784\n",
      "train loss:0.021362585058844506\n",
      "train loss:0.04002601448343298\n",
      "train loss:0.0048167199201117495\n",
      "train loss:0.016642508785611188\n",
      "train loss:0.08828989808912198\n",
      "train loss:0.07021044044835435\n",
      "train loss:0.02645290442669106\n",
      "train loss:0.0057336534599535085\n",
      "train loss:0.02045221960844841\n",
      "train loss:0.03191534318660344\n",
      "train loss:0.02756652094810943\n",
      "train loss:0.020971304598023256\n",
      "train loss:0.003728495313225213\n",
      "train loss:0.0243581073288599\n",
      "train loss:0.03025306363920507\n",
      "train loss:0.04138576954306225\n",
      "train loss:0.02092605667056615\n",
      "train loss:0.03632671497739235\n",
      "train loss:0.010272937526025674\n",
      "train loss:0.0033688699312421043\n",
      "train loss:0.01692298929537039\n",
      "train loss:0.14324627891879477\n",
      "train loss:0.05483443021305388\n",
      "train loss:0.010761262802133124\n",
      "train loss:0.007246506576269018\n",
      "train loss:0.044996624875614115\n",
      "train loss:0.04667898287414655\n",
      "train loss:0.06360034371026034\n",
      "train loss:0.02428934903484365\n",
      "train loss:0.024867990609431175\n",
      "train loss:0.027967863094155176\n",
      "train loss:0.03137331853022396\n",
      "train loss:0.03445919642411449\n",
      "train loss:0.005856561781993493\n",
      "train loss:0.01779652361343915\n",
      "train loss:0.011934203762079865\n",
      "train loss:0.05741780707377893\n",
      "train loss:0.01609540162560688\n",
      "train loss:0.022224434815190555\n",
      "train loss:0.00908987558594827\n",
      "train loss:0.012718054516942398\n",
      "train loss:0.07870725777304877\n",
      "train loss:0.024967394705722477\n",
      "train loss:0.009615546739477154\n",
      "train loss:0.06395138820751194\n",
      "train loss:0.035215342428011366\n",
      "train loss:0.022688424049429726\n",
      "train loss:0.020243246146331984\n",
      "train loss:0.0449058339869961\n",
      "train loss:0.04930246717208699\n",
      "train loss:0.02110112462513499\n",
      "train loss:0.01733744320358309\n",
      "train loss:0.020589966341075537\n",
      "train loss:0.03584348236089322\n",
      "train loss:0.025081059830888965\n",
      "train loss:0.00674656787112057\n",
      "train loss:0.01002074199020989\n",
      "train loss:0.02396806025242391\n",
      "train loss:0.019505431649792522\n",
      "train loss:0.004780634890510619\n",
      "train loss:0.009972331501914929\n",
      "train loss:0.045283153567527885\n",
      "train loss:0.0324626597245131\n",
      "train loss:0.034362019068602916\n",
      "train loss:0.027509909658592156\n",
      "train loss:0.006073679419974394\n",
      "train loss:0.00987170055793033\n",
      "train loss:0.037339978524519986\n",
      "train loss:0.015095831366798297\n",
      "train loss:0.02313357002201201\n",
      "train loss:0.006793791311846254\n",
      "train loss:0.002308311544586728\n",
      "train loss:0.00749823682813483\n",
      "train loss:0.036593548471909015\n",
      "train loss:0.016892510907173353\n",
      "train loss:0.027367691096776902\n",
      "train loss:0.006622562814173906\n",
      "train loss:0.05703300401671653\n",
      "train loss:0.00654093571896956\n",
      "train loss:0.013918736832486511\n",
      "train loss:0.010907096929336817\n",
      "train loss:0.009126316555215399\n",
      "train loss:0.012652923013494357\n",
      "train loss:0.09543195265721356\n",
      "train loss:0.03803354088455486\n",
      "train loss:0.12775513362328994\n",
      "train loss:0.0066511340309700455\n",
      "train loss:0.007937126831009897\n",
      "train loss:0.004674129179068271\n",
      "train loss:0.019589724413415565\n",
      "train loss:0.03113831832594977\n",
      "train loss:0.016457093772979464\n",
      "train loss:0.022027180443684662\n",
      "train loss:0.009020342831038461\n",
      "train loss:0.009087492236497241\n",
      "train loss:0.009006986367459916\n",
      "train loss:0.006117103196900893\n",
      "train loss:0.03134301532596853\n",
      "train loss:0.006649337818278263\n",
      "train loss:0.035431647802264\n",
      "train loss:0.009594507588282824\n",
      "train loss:0.020943766897636724\n",
      "train loss:0.027004400465157268\n",
      "train loss:0.0037341253133826376\n",
      "train loss:0.016204649447842653\n",
      "train loss:0.03748175876781335\n",
      "train loss:0.04966693930592297\n",
      "train loss:0.0071682842290308\n",
      "train loss:0.016454280178257404\n",
      "train loss:0.03325270460780379\n",
      "train loss:0.026570940634325618\n",
      "train loss:0.016702886304020434\n",
      "train loss:0.019993402094984517\n",
      "train loss:0.016421986280097216\n",
      "train loss:0.060767362467136744\n",
      "train loss:0.011389935870444776\n",
      "train loss:0.030072440142479534\n",
      "train loss:0.008252321670789986\n",
      "train loss:0.015484407583597282\n",
      "train loss:0.03124946507947373\n",
      "train loss:0.008140918097302953\n",
      "train loss:0.03360019834291725\n",
      "train loss:0.036509201334921616\n",
      "train loss:0.008129198042611429\n",
      "train loss:0.004697123137628131\n",
      "train loss:0.014208241693060859\n",
      "train loss:0.014225244435834151\n",
      "train loss:0.010293429248835178\n",
      "train loss:0.015526455701030792\n",
      "train loss:0.004046667029797488\n",
      "train loss:0.01618736332957302\n",
      "train loss:0.015219411850406612\n",
      "train loss:0.019851233028321723\n",
      "train loss:0.0384178348358238\n",
      "train loss:0.003008747943399965\n",
      "train loss:0.04755683207385877\n",
      "train loss:0.011548334046997117\n",
      "train loss:0.011271886351549226\n",
      "train loss:0.008972114044985668\n",
      "train loss:0.011392123649790389\n",
      "train loss:0.0205422075455516\n",
      "train loss:0.06891198550668995\n",
      "train loss:0.0008852020108873279\n",
      "train loss:0.004274939003845838\n",
      "train loss:0.019108799419089287\n",
      "train loss:0.04734537323762748\n",
      "train loss:0.019275432246943853\n",
      "train loss:0.04194123669803585\n",
      "train loss:0.007722878193901345\n",
      "train loss:0.007257819527135335\n",
      "train loss:0.010129964079279331\n",
      "train loss:0.03555329361884218\n",
      "train loss:0.007318964997827026\n",
      "train loss:0.032794323736639194\n",
      "train loss:0.03546488572800011\n",
      "train loss:0.06158707611343209\n",
      "train loss:0.03595586770697949\n",
      "train loss:0.04262225761788633\n",
      "train loss:0.009479066673565534\n",
      "train loss:0.019020770930767178\n",
      "train loss:0.022225455268634266\n",
      "train loss:0.02534522140009516\n",
      "train loss:0.019456609799387955\n",
      "train loss:0.00120855230988799\n",
      "train loss:0.0387712895624343\n",
      "train loss:0.006520509615096038\n",
      "train loss:0.015266392792463987\n",
      "train loss:0.01140086175811183\n",
      "train loss:0.008422272126891726\n",
      "train loss:0.03099567037175125\n",
      "train loss:0.00959761622064056\n",
      "train loss:0.010059684866824617\n",
      "train loss:0.013516771354539839\n",
      "train loss:0.011896350091871793\n",
      "train loss:0.04091005832396284\n",
      "train loss:0.05266800595922797\n",
      "train loss:0.01664980801292051\n",
      "train loss:0.025851791437203023\n",
      "train loss:0.03009543605878919\n",
      "train loss:0.009337673012964245\n",
      "train loss:0.023730256417214997\n",
      "train loss:0.008749868998433784\n",
      "train loss:0.01115582933277223\n",
      "train loss:0.013739462138409883\n",
      "train loss:0.014772563217137076\n",
      "train loss:0.005550211495633889\n",
      "train loss:0.011385570124571087\n",
      "train loss:0.0071886969902169315\n",
      "train loss:0.024406075672292405\n",
      "train loss:0.020636167197988442\n",
      "train loss:0.007026598204170906\n",
      "train loss:0.013983272228339385\n",
      "train loss:0.0042389555758010145\n",
      "train loss:0.00469110189454055\n",
      "train loss:0.011806885925349621\n",
      "train loss:0.038690985743564305\n",
      "train loss:0.008645007227162636\n",
      "train loss:0.019378414209448556\n",
      "train loss:0.04027590848020582\n",
      "train loss:0.021535493141810816\n",
      "train loss:0.0014512707391202287\n",
      "train loss:0.019403255460089464\n",
      "train loss:0.013807293122083699\n",
      "train loss:0.008121861602885793\n",
      "train loss:0.025813178575513643\n",
      "train loss:0.0073100324384235615\n",
      "train loss:0.01642880625543132\n",
      "train loss:0.03351756914845665\n",
      "train loss:0.018073362515192367\n",
      "train loss:0.0358372949321859\n",
      "train loss:0.02027898326917142\n",
      "train loss:0.1001169183971498\n",
      "train loss:0.018781361844060287\n",
      "train loss:0.012907013165551763\n",
      "train loss:0.014472014305622226\n",
      "train loss:0.007103977613548819\n",
      "train loss:0.021280321888874763\n",
      "train loss:0.027929958517072848\n",
      "train loss:0.006496309511485302\n",
      "train loss:0.0034269376664947367\n",
      "train loss:0.048062483020222195\n",
      "train loss:0.014385310330835475\n",
      "train loss:0.11319641772212935\n",
      "train loss:0.08733896816620199\n",
      "train loss:0.005317555364215545\n",
      "train loss:0.07458014432753077\n",
      "train loss:0.008121923431982384\n",
      "train loss:0.01741678171027064\n",
      "train loss:0.012996084881875586\n",
      "train loss:0.009652817710494149\n",
      "train loss:0.00713597880939829\n",
      "train loss:0.0029764459661563124\n",
      "train loss:0.006335782502454344\n",
      "train loss:0.04372747791986976\n",
      "train loss:0.013663636641404736\n",
      "train loss:0.02034822592742461\n",
      "train loss:0.008897835028076197\n",
      "train loss:0.015330854702844838\n",
      "train loss:0.031006354898638608\n",
      "train loss:0.018671838173386676\n",
      "train loss:0.01768649078845279\n",
      "train loss:0.028119926037886104\n",
      "train loss:0.016756253108825024\n",
      "train loss:0.04991059912059228\n",
      "train loss:0.0077302106051317255\n",
      "train loss:0.029145685819535902\n",
      "train loss:0.0021189036475748623\n",
      "train loss:0.01790827428697153\n",
      "train loss:0.06626330410604335\n",
      "train loss:0.00307360736264288\n",
      "train loss:0.03674842721780393\n",
      "train loss:0.04458538846550922\n",
      "train loss:0.006020425053632561\n",
      "train loss:0.009856339611822869\n",
      "train loss:0.01522931320441826\n",
      "train loss:0.009809426698381042\n",
      "train loss:0.029246690975160715\n",
      "train loss:0.01724769757826641\n",
      "train loss:0.004576959785432256\n",
      "train loss:0.00905054736692184\n",
      "train loss:0.06168487788087308\n",
      "train loss:0.08931144659024054\n",
      "train loss:0.00548108498802195\n",
      "train loss:0.0546567451229863\n",
      "train loss:0.023695770106386838\n",
      "train loss:0.005803538375971571\n",
      "train loss:0.0261225401262051\n",
      "train loss:0.022408090422221437\n",
      "train loss:0.004739380169361014\n",
      "train loss:0.0073873783755765456\n",
      "train loss:0.004993695774647614\n",
      "train loss:0.009824140355342436\n",
      "train loss:0.023919538217059916\n",
      "train loss:0.011690329715227654\n",
      "train loss:0.01592403709535653\n",
      "train loss:0.0650566817265424\n",
      "train loss:0.011589598650481238\n",
      "train loss:0.0050947007913786846\n",
      "train loss:0.00247705248078919\n",
      "train loss:0.003437471394360084\n",
      "train loss:0.006954950221683957\n",
      "train loss:0.003921495764369578\n",
      "train loss:0.018239235814322657\n",
      "train loss:0.0070560957578222625\n",
      "train loss:0.009280512363390036\n",
      "train loss:0.008729092170560358\n",
      "train loss:0.023224328884840766\n",
      "train loss:0.01910164102320856\n",
      "train loss:0.00909476273797902\n",
      "train loss:0.01675201586632041\n",
      "train loss:0.005729149567407421\n",
      "train loss:0.013074605180986132\n",
      "train loss:0.012500164481278058\n",
      "train loss:0.042230728044406446\n",
      "train loss:0.022356958990008783\n",
      "train loss:0.022866873599903276\n",
      "train loss:0.009684562947626391\n",
      "train loss:0.027691175609996982\n",
      "train loss:0.030345859064058383\n",
      "train loss:0.00813816255275641\n",
      "train loss:0.0651610560526806\n",
      "train loss:0.010209156994731511\n",
      "train loss:0.007388383812423632\n",
      "train loss:0.020412604035567078\n",
      "train loss:0.021401705859020945\n",
      "train loss:0.007934074471623764\n",
      "train loss:0.04315676266395301\n",
      "train loss:0.021148421336042108\n",
      "train loss:0.005105353988725091\n",
      "train loss:0.010540752891657805\n",
      "train loss:0.012556580209564485\n",
      "train loss:0.053426107617591104\n",
      "train loss:0.011332165940404895\n",
      "train loss:0.011937763413235887\n",
      "train loss:0.00220950169176985\n",
      "train loss:0.05311567562816247\n",
      "train loss:0.010524761684238392\n",
      "train loss:0.005383454137037986\n",
      "train loss:0.012541336558965823\n",
      "train loss:0.00177862141284957\n",
      "train loss:0.06014820014074457\n",
      "train loss:0.03612850267392495\n",
      "train loss:0.027087783926415684\n",
      "train loss:0.01314778604946909\n",
      "train loss:0.027348269683030245\n",
      "train loss:0.008091496387877824\n",
      "train loss:0.004044754650406313\n",
      "train loss:0.02596367658833267\n",
      "train loss:0.006571870373127392\n",
      "train loss:0.01897941067590639\n",
      "train loss:0.01836697037922751\n",
      "train loss:0.019483827192301253\n",
      "train loss:0.008916075134825034\n",
      "train loss:0.020187039805955456\n",
      "train loss:0.00745888087396747\n",
      "train loss:0.015074750835613483\n",
      "train loss:0.02249797451441048\n",
      "train loss:0.009153235203481472\n",
      "train loss:0.02652292777935025\n",
      "train loss:0.016766702641358874\n",
      "train loss:0.010873535100789202\n",
      "train loss:0.05254816025557704\n",
      "train loss:0.016887759466361382\n",
      "train loss:0.041730267352755854\n",
      "train loss:0.06531958624370922\n",
      "train loss:0.009277266673623196\n",
      "train loss:0.009201701933173902\n",
      "train loss:0.013644034692420601\n",
      "train loss:0.009255110574636434\n",
      "train loss:0.0033992570396327794\n",
      "train loss:0.02107014596650092\n",
      "train loss:0.003449386497298396\n",
      "train loss:0.01700357226395059\n",
      "train loss:0.0067726397968654204\n",
      "train loss:0.06616091572965019\n",
      "train loss:0.024978100850626524\n",
      "train loss:0.019363327180395004\n",
      "train loss:0.005719433760471179\n",
      "train loss:0.036943426976083424\n",
      "train loss:0.0031825124441664547\n",
      "train loss:0.04207022495986239\n",
      "train loss:0.008862514503882798\n",
      "train loss:0.03361599140180908\n",
      "train loss:0.016966388545697875\n",
      "train loss:0.010548136480338748\n",
      "train loss:0.00840539155211214\n",
      "train loss:0.05621301995152468\n",
      "train loss:0.005826366912613372\n",
      "train loss:0.006879474567280553\n",
      "train loss:0.00809235184456048\n",
      "train loss:0.015691566574111512\n",
      "train loss:0.0029627930781284894\n",
      "train loss:0.026598902860090586\n",
      "train loss:0.011837360115625169\n",
      "train loss:0.00395488668833845\n",
      "train loss:0.12347424643852781\n",
      "train loss:0.03458075735637168\n",
      "train loss:0.02895493323203832\n",
      "train loss:0.010395115303000548\n",
      "train loss:0.022067238067970157\n",
      "train loss:0.014017584252438289\n",
      "train loss:0.03111382799918489\n",
      "train loss:0.013716232811470696\n",
      "train loss:0.05858290153353272\n",
      "train loss:0.0028624640841661496\n",
      "train loss:0.010867509460882592\n",
      "train loss:0.004240438022985647\n",
      "train loss:0.020628945331590227\n",
      "train loss:0.006917895850027988\n",
      "train loss:0.011624995464843668\n",
      "train loss:0.010131905523990065\n",
      "train loss:0.034975546983211564\n",
      "train loss:0.004275134023848244\n",
      "train loss:0.013749747367860254\n",
      "train loss:0.019215943250750017\n",
      "train loss:0.008090346542122439\n",
      "train loss:0.036081933442655055\n",
      "train loss:0.009236480141781588\n",
      "train loss:0.006779819494158707\n",
      "train loss:0.00410498434358496\n",
      "train loss:0.01107183563667805\n",
      "train loss:0.06976463963304882\n",
      "train loss:0.06398355286685921\n",
      "train loss:0.0035287011483469666\n",
      "train loss:0.012130505959934548\n",
      "=== epoch:7, train acc:0.991, test acc:0.983 ===\n",
      "train loss:0.005161401059596469\n",
      "train loss:0.03600791271194491\n",
      "train loss:0.015634191982925965\n",
      "train loss:0.012468105489505561\n",
      "train loss:0.007261364615610301\n",
      "train loss:0.020336791488843386\n",
      "train loss:0.029974860216058663\n",
      "train loss:0.021049005386172194\n",
      "train loss:0.04063536288170938\n",
      "train loss:0.04358204875155998\n",
      "train loss:0.00866572948465544\n",
      "train loss:0.010045396136399676\n",
      "train loss:0.008543915623026728\n",
      "train loss:0.040904120053272604\n",
      "train loss:0.013142861405700002\n",
      "train loss:0.01705057083383774\n",
      "train loss:0.013388218055739826\n",
      "train loss:0.013832246745785357\n",
      "train loss:0.008524922765600549\n",
      "train loss:0.0037416035350861577\n",
      "train loss:0.013536255761803754\n",
      "train loss:0.01280688257321195\n",
      "train loss:0.010742343568579655\n",
      "train loss:0.020076564071406303\n",
      "train loss:0.001563673716983007\n",
      "train loss:0.009393654094967568\n",
      "train loss:0.008340731929935652\n",
      "train loss:0.012151765906524718\n",
      "train loss:0.03814253797063435\n",
      "train loss:0.053479272822170826\n",
      "train loss:0.0270900945820883\n",
      "train loss:0.028296187402191662\n",
      "train loss:0.010982554061400031\n",
      "train loss:0.0038005271961316154\n",
      "train loss:0.006983560235928328\n",
      "train loss:0.01447200600709877\n",
      "train loss:0.0707642755999968\n",
      "train loss:0.010148134659724117\n",
      "train loss:0.014224878688879419\n",
      "train loss:0.041063804654728156\n",
      "train loss:0.005195249848457057\n",
      "train loss:0.0027877721888832797\n",
      "train loss:0.016258574732298203\n",
      "train loss:0.0217414777882408\n",
      "train loss:0.03387608292424599\n",
      "train loss:0.029056089254684\n",
      "train loss:0.008133108392326975\n",
      "train loss:0.012545192867561198\n",
      "train loss:0.012929667855194907\n",
      "train loss:0.036699746230323405\n",
      "train loss:0.0021960270518906967\n",
      "train loss:0.008451805130022725\n",
      "train loss:0.031107356932123072\n",
      "train loss:0.031074963912899118\n",
      "train loss:0.006608236809978214\n",
      "train loss:0.0020693522907556934\n",
      "train loss:0.014006754373303681\n",
      "train loss:0.032282329021011885\n",
      "train loss:0.0048889582200511246\n",
      "train loss:0.0041739715412211385\n",
      "train loss:0.026578378586134023\n",
      "train loss:0.10876652328503261\n",
      "train loss:0.005371882403410582\n",
      "train loss:0.009728002219260127\n",
      "train loss:0.01950046030117668\n",
      "train loss:0.0024315615972892025\n",
      "train loss:0.04017171281494747\n",
      "train loss:0.006893219692300255\n",
      "train loss:0.005349804210474798\n",
      "train loss:0.10095168378000106\n",
      "train loss:0.010633844488839085\n",
      "train loss:0.1102866513694844\n",
      "train loss:0.031149799089031247\n",
      "train loss:0.015980180217648795\n",
      "train loss:0.003691281833294375\n",
      "train loss:0.0055153309992999775\n",
      "train loss:0.004891300040495341\n",
      "train loss:0.0026543580777886235\n",
      "train loss:0.03884600386513501\n",
      "train loss:0.02104201384632779\n",
      "train loss:0.03668917404504273\n",
      "train loss:0.016370775487859743\n",
      "train loss:0.0712050279233264\n",
      "train loss:0.0400349454864806\n",
      "train loss:0.005491398919873737\n",
      "train loss:0.008467370161789865\n",
      "train loss:0.0041893347683482195\n",
      "train loss:0.005146267942181505\n",
      "train loss:0.016412274984700385\n",
      "train loss:0.03845601463971327\n",
      "train loss:0.051740862471592465\n",
      "train loss:0.004145856064194407\n",
      "train loss:0.017486035471584407\n",
      "train loss:0.03185621258190587\n",
      "train loss:0.03727575548353353\n",
      "train loss:0.013325095252408983\n",
      "train loss:0.004596174376106539\n",
      "train loss:0.0497837577002847\n",
      "train loss:0.03614610185871034\n",
      "train loss:0.004455897011516307\n",
      "train loss:0.011123548819511529\n",
      "train loss:0.02914220996235521\n",
      "train loss:0.01810145409852195\n",
      "train loss:0.012955707973155055\n",
      "train loss:0.004320608987231283\n",
      "train loss:0.03317465622066052\n",
      "train loss:0.006933534628249633\n",
      "train loss:0.011133432206673899\n",
      "train loss:0.0305244011601311\n",
      "train loss:0.011883647199612674\n",
      "train loss:0.04351586167040985\n",
      "train loss:0.014494003671921525\n",
      "train loss:0.00717400040574051\n",
      "train loss:0.023452595962289574\n",
      "train loss:0.03300284051886754\n",
      "train loss:0.02443574539796365\n",
      "train loss:0.015225561299615865\n",
      "train loss:0.13678368981988343\n",
      "train loss:0.008775918935673586\n",
      "train loss:0.027730271371495956\n",
      "train loss:0.035017260128004615\n",
      "train loss:0.015552605449318656\n",
      "train loss:0.037476330002184775\n",
      "train loss:0.043878580219325614\n",
      "train loss:0.034153494019105134\n",
      "train loss:0.017902819984868465\n",
      "train loss:0.011056006597041701\n",
      "train loss:0.018037652776027674\n",
      "train loss:0.012654728283660956\n",
      "train loss:0.013068534511907528\n",
      "train loss:0.011979801228853192\n",
      "train loss:0.002505434919247376\n",
      "train loss:0.02464513279783225\n",
      "train loss:0.01141328255202007\n",
      "train loss:0.05547543775797271\n",
      "train loss:0.007312857043098885\n",
      "train loss:0.029395334241088186\n",
      "train loss:0.014644989553306308\n",
      "train loss:0.04877908134313422\n",
      "train loss:0.008862539687554329\n",
      "train loss:0.010707471349698293\n",
      "train loss:0.011942143254259494\n",
      "train loss:0.07508827835775357\n",
      "train loss:0.015418842289048672\n",
      "train loss:0.05526998230138914\n",
      "train loss:0.014881037233729976\n",
      "train loss:0.012855645988067252\n",
      "train loss:0.04469340683045719\n",
      "train loss:0.025194588678687495\n",
      "train loss:0.029994856749073505\n",
      "train loss:0.06091772793939707\n",
      "train loss:0.010583095710358958\n",
      "train loss:0.08968469378122905\n",
      "train loss:0.011985736380161873\n",
      "train loss:0.022660409505715524\n",
      "train loss:0.033494470215792285\n",
      "train loss:0.012372663683516371\n",
      "train loss:0.01259026141284644\n",
      "train loss:0.006672249947789024\n",
      "train loss:0.04318075496342726\n",
      "train loss:0.009556676201983572\n",
      "train loss:0.01203740303812937\n",
      "train loss:0.015203446440616619\n",
      "train loss:0.022228320636975357\n",
      "train loss:0.0064835383536888825\n",
      "train loss:0.005940381109225107\n",
      "train loss:0.012027158201732254\n",
      "train loss:0.012510595608077092\n",
      "train loss:0.01917863208796332\n",
      "train loss:0.012629983531803925\n",
      "train loss:0.034463712562168757\n",
      "train loss:0.05253934636932857\n",
      "train loss:0.012294537248274881\n",
      "train loss:0.021641982997155203\n",
      "train loss:0.005850212689140592\n",
      "train loss:0.020958353821541067\n",
      "train loss:0.027015824572802983\n",
      "train loss:0.050954942247218596\n",
      "train loss:0.019227265892294624\n",
      "train loss:0.013071682332988378\n",
      "train loss:0.009160210873889893\n",
      "train loss:0.0090765577519482\n",
      "train loss:0.010632548624371187\n",
      "train loss:0.018529126913675257\n",
      "train loss:0.031102848820266708\n",
      "train loss:0.0072725418637077655\n",
      "train loss:0.034368401528065806\n",
      "train loss:0.05076858929761365\n",
      "train loss:0.013916357115351306\n",
      "train loss:0.0036324027452535905\n",
      "train loss:0.02551323526798112\n",
      "train loss:0.008287032589357191\n",
      "train loss:0.03598537562877698\n",
      "train loss:0.025779810168199996\n",
      "train loss:0.006866885748211574\n",
      "train loss:0.008969621358810102\n",
      "train loss:0.012642222953282416\n",
      "train loss:0.010046139640507481\n",
      "train loss:0.008542195666082716\n",
      "train loss:0.05047823265952674\n",
      "train loss:0.011789610220754391\n",
      "train loss:0.009474819913880515\n",
      "train loss:0.017129036645062115\n",
      "train loss:0.014457394384317005\n",
      "train loss:0.003403211594534593\n",
      "train loss:0.0060826165834220155\n",
      "train loss:0.004387056017046409\n",
      "train loss:0.015525365975509521\n",
      "train loss:0.028618097372154594\n",
      "train loss:0.008163458473935554\n",
      "train loss:0.009584333730795072\n",
      "train loss:0.004091619494237729\n",
      "train loss:0.0014916191426309982\n",
      "train loss:0.0036798579077548994\n",
      "train loss:0.0017531095836202692\n",
      "train loss:0.0275245390364704\n",
      "train loss:0.005655168182352812\n",
      "train loss:0.031236323750444214\n",
      "train loss:0.06632520908087351\n",
      "train loss:0.02480891393792723\n",
      "train loss:0.0036865001417132827\n",
      "train loss:0.010224451705886933\n",
      "train loss:0.006647102633303676\n",
      "train loss:0.006104248695637617\n",
      "train loss:0.010689257064353282\n",
      "train loss:0.012857132293569525\n",
      "train loss:0.012025546698480515\n",
      "train loss:0.021378190744849333\n",
      "train loss:0.00501162887547676\n",
      "train loss:0.001664311804393665\n",
      "train loss:0.014873493473407077\n",
      "train loss:0.0047102317800782004\n",
      "train loss:0.015899667916454297\n",
      "train loss:0.00797728181771183\n",
      "train loss:0.007074182145155714\n",
      "train loss:0.0025552876505773136\n",
      "train loss:0.00602198728974184\n",
      "train loss:0.0049448747958698\n",
      "train loss:0.0020462536643391237\n",
      "train loss:0.014802682353617893\n",
      "train loss:0.0046121452916716415\n",
      "train loss:0.004239527081126851\n",
      "train loss:0.0031670419010414123\n",
      "train loss:0.0025930776771328646\n",
      "train loss:0.05618795830563663\n",
      "train loss:0.011644720641946844\n",
      "train loss:0.011095558881764127\n",
      "train loss:0.02319426242256392\n",
      "train loss:0.02935939257800859\n",
      "train loss:0.027249872407003037\n",
      "train loss:0.006324778769436735\n",
      "train loss:0.035391763990573526\n",
      "train loss:0.06004260978269049\n",
      "train loss:0.00521845244120386\n",
      "train loss:0.010072547437498571\n",
      "train loss:0.004120234227278815\n",
      "train loss:0.005359713249525528\n",
      "train loss:0.021451799115986908\n",
      "train loss:0.03027515784238238\n",
      "train loss:0.02718306162234039\n",
      "train loss:0.010892588701904526\n",
      "train loss:0.0014696787218975104\n",
      "train loss:0.059924703188056344\n",
      "train loss:0.008112584340999084\n",
      "train loss:0.020571226018206233\n",
      "train loss:0.007639618790199734\n",
      "train loss:0.023746494470427825\n",
      "train loss:0.02097586826544091\n",
      "train loss:0.005152056902273902\n",
      "train loss:0.00450135615528707\n",
      "train loss:0.0030536973042435194\n",
      "train loss:0.012160063214811122\n",
      "train loss:0.021637396405345143\n",
      "train loss:0.03305358902744817\n",
      "train loss:0.1127650812391099\n",
      "train loss:0.008989782670120067\n",
      "train loss:0.09664342371874643\n",
      "train loss:0.007084169000154385\n",
      "train loss:0.0075472572870207665\n",
      "train loss:0.002778382041186612\n",
      "train loss:0.041304241846424866\n",
      "train loss:0.010875877454985155\n",
      "train loss:0.0150597122091302\n",
      "train loss:0.013461337950260898\n",
      "train loss:0.02564241180420895\n",
      "train loss:0.009762923352478449\n",
      "train loss:0.007362168709520207\n",
      "train loss:0.011441703794188669\n",
      "train loss:0.008827739809686269\n",
      "train loss:0.015879683232045493\n",
      "train loss:0.014606473418565462\n",
      "train loss:0.013850887844169095\n",
      "train loss:0.013116050737850486\n",
      "train loss:0.014354241523027245\n",
      "train loss:0.022249629412610966\n",
      "train loss:0.008403286566100262\n",
      "train loss:0.004987674687087279\n",
      "train loss:0.003660951543137651\n",
      "train loss:0.021256285709945456\n",
      "train loss:0.011575666325530564\n",
      "train loss:0.011036120436344053\n",
      "train loss:0.008731342734999986\n",
      "train loss:0.03732029665964746\n",
      "train loss:0.009386601669059507\n",
      "train loss:0.023911388066481333\n",
      "train loss:0.0013004021693638636\n",
      "train loss:0.006250655398916529\n",
      "train loss:0.01734283536281002\n",
      "train loss:0.01607795609164947\n",
      "train loss:0.033512276544760834\n",
      "train loss:0.0058276660801136575\n",
      "train loss:0.04259204114567784\n",
      "train loss:0.02031388381177118\n",
      "train loss:0.009016371665839623\n",
      "train loss:0.0017588170538360933\n",
      "train loss:0.0016274294209938246\n",
      "train loss:0.002388726274207645\n",
      "train loss:0.007089934209343054\n",
      "train loss:0.0028324334342077224\n",
      "train loss:0.014033200228723852\n",
      "train loss:0.020795369975825633\n",
      "train loss:0.015715731419345007\n",
      "train loss:0.008734844824607449\n",
      "train loss:0.013690503364432859\n",
      "train loss:0.02124847723361899\n",
      "train loss:0.007711253962089603\n",
      "train loss:0.01651491062021674\n",
      "train loss:0.006126746349907336\n",
      "train loss:0.00788153024683571\n",
      "train loss:0.009723865532521322\n",
      "train loss:0.004686048414583167\n",
      "train loss:0.11373464746133974\n",
      "train loss:0.004017813532467456\n",
      "train loss:0.02210680969130127\n",
      "train loss:0.024194177763559695\n",
      "train loss:0.04154397772790303\n",
      "train loss:0.00583433095200751\n",
      "train loss:0.0642120668347868\n",
      "train loss:0.007952766046280068\n",
      "train loss:0.006508010055379185\n",
      "train loss:0.008583815779518968\n",
      "train loss:0.019522049333119778\n",
      "train loss:0.008960898710091165\n",
      "train loss:0.012372783161640801\n",
      "train loss:0.013664813928415629\n",
      "train loss:0.008801761961417087\n",
      "train loss:0.020030020425703586\n",
      "train loss:0.013438070659234922\n",
      "train loss:0.03283702638282259\n",
      "train loss:0.022195852339106134\n",
      "train loss:0.02067462842984409\n",
      "train loss:0.004079516977090541\n",
      "train loss:0.018993573760840998\n",
      "train loss:0.0069916025396951865\n",
      "train loss:0.031145552013197575\n",
      "train loss:0.010158228535742421\n",
      "train loss:0.03165313736119835\n",
      "train loss:0.00663875177314656\n",
      "train loss:0.021307674105614058\n",
      "train loss:0.005298595849803595\n",
      "train loss:0.03243343777647213\n",
      "train loss:0.012707022324708545\n",
      "train loss:0.04622070539983467\n",
      "train loss:0.024500848242567156\n",
      "train loss:0.011417139591414676\n",
      "train loss:0.01296218225158133\n",
      "train loss:0.10571573849442803\n",
      "train loss:0.055120709741086404\n",
      "train loss:0.008570686257573098\n",
      "train loss:0.022958029452052636\n",
      "train loss:0.011242081044174868\n",
      "train loss:0.020346845325045552\n",
      "train loss:0.01474056959877992\n",
      "train loss:0.04794245419626937\n",
      "train loss:0.0036236079371246175\n",
      "train loss:0.018538348250171397\n",
      "train loss:0.010123601688554069\n",
      "train loss:0.030420156053225365\n",
      "train loss:0.019300229310575238\n",
      "train loss:0.012155225081085566\n",
      "train loss:0.008325539563127458\n",
      "train loss:0.02131350460102577\n",
      "train loss:0.04864000096713027\n",
      "train loss:0.0017053767643995326\n",
      "train loss:0.013478005030898514\n",
      "train loss:0.002850890879577966\n",
      "train loss:0.017536522427778752\n",
      "train loss:0.031457101336240786\n",
      "train loss:0.0014397439509157425\n",
      "train loss:0.01840569739027546\n",
      "train loss:0.017981739606801144\n",
      "train loss:0.0029353043335877582\n",
      "train loss:0.0071057509530898285\n",
      "train loss:0.026152993842454727\n",
      "train loss:0.04159199530171708\n",
      "train loss:0.006769249478441854\n",
      "train loss:0.0024427559232011935\n",
      "train loss:0.03881661757569088\n",
      "train loss:0.01068476630449797\n",
      "train loss:0.004526928332998346\n",
      "train loss:0.07829418815708017\n",
      "train loss:0.010509135302833433\n",
      "train loss:0.05038690116244385\n",
      "train loss:0.00979180052200144\n",
      "train loss:0.009280599691508995\n",
      "train loss:0.0022241529169452707\n",
      "train loss:0.004325895650362645\n",
      "train loss:0.01685723073622909\n",
      "train loss:0.00958129524069049\n",
      "train loss:0.004257899183427663\n",
      "train loss:0.012399762817641742\n",
      "train loss:0.016580730209979154\n",
      "train loss:0.005277026333016052\n",
      "train loss:0.0026434541108919882\n",
      "train loss:0.004553637236305404\n",
      "train loss:0.004446524114171885\n",
      "train loss:0.015400336413672085\n",
      "train loss:0.0040024538329793405\n",
      "train loss:0.01379825122734106\n",
      "train loss:0.015977443869110767\n",
      "train loss:0.008474225835591775\n",
      "train loss:0.003990693563455367\n",
      "train loss:0.010328216179612054\n",
      "train loss:0.003353380904608483\n",
      "train loss:0.013047859408131674\n",
      "train loss:0.040557321692046296\n",
      "train loss:0.017408427775152644\n",
      "train loss:0.006289690435276338\n",
      "train loss:0.012551943024872973\n",
      "train loss:0.014181541227578753\n",
      "train loss:0.007428731558192806\n",
      "train loss:0.010516259995446626\n",
      "train loss:0.01729138172335008\n",
      "train loss:0.0015740473873916523\n",
      "train loss:0.01928357758479289\n",
      "train loss:0.004457946714835244\n",
      "train loss:0.005425735682553751\n",
      "train loss:0.014270405449070163\n",
      "train loss:0.004057009978749174\n",
      "train loss:0.03160515946953793\n",
      "train loss:0.0016951068234420713\n",
      "train loss:0.01794210118977054\n",
      "train loss:0.003064435974454861\n",
      "train loss:0.009431590516415005\n",
      "train loss:0.017110910413882335\n",
      "train loss:0.003227589282206711\n",
      "train loss:0.010003307229036962\n",
      "train loss:0.0013951285867880117\n",
      "train loss:0.03585691491836051\n",
      "train loss:0.00742083042716042\n",
      "train loss:0.028909106260810044\n",
      "train loss:0.013024606524345865\n",
      "train loss:0.004836821270247169\n",
      "train loss:0.005296070922204573\n",
      "train loss:0.022689671517176112\n",
      "train loss:0.003319703266809933\n",
      "train loss:0.018930640496697255\n",
      "train loss:0.007724446164605533\n",
      "train loss:0.006773765886193414\n",
      "train loss:0.053327098756130106\n",
      "train loss:0.008119327574033828\n",
      "train loss:0.056441713098813254\n",
      "train loss:0.012013809810543175\n",
      "train loss:0.0067942231275864365\n",
      "train loss:0.007278809218978939\n",
      "train loss:0.019989851456413337\n",
      "train loss:0.00639048675804513\n",
      "train loss:0.015681883687397674\n",
      "train loss:0.010087673560703257\n",
      "train loss:0.005122453595837767\n",
      "train loss:0.0027175733068053915\n",
      "train loss:0.009518176698714471\n",
      "train loss:0.005130222368278345\n",
      "train loss:0.02883564996601391\n",
      "train loss:0.013387063333703528\n",
      "train loss:0.01698282841970614\n",
      "train loss:0.009067721675133613\n",
      "train loss:0.012617028845604174\n",
      "train loss:0.02072906773679857\n",
      "train loss:0.00796385571741871\n",
      "train loss:0.004809582929889659\n",
      "train loss:0.036871881909615524\n",
      "train loss:0.009669229981555835\n",
      "train loss:0.01557945576576677\n",
      "train loss:0.012181492178127931\n",
      "train loss:0.0035362389084857664\n",
      "train loss:0.0020727794270352334\n",
      "train loss:0.04067431825232862\n",
      "train loss:0.01693732943208792\n",
      "train loss:0.013992062023393695\n",
      "train loss:0.017409973080231806\n",
      "train loss:0.010802669429248443\n",
      "train loss:0.01125776086113118\n",
      "train loss:0.009663265368237072\n",
      "train loss:0.0009630089033509481\n",
      "train loss:0.005648773018824098\n",
      "train loss:0.01053041457635709\n",
      "train loss:0.00678913995589102\n",
      "train loss:0.012624399089581856\n",
      "train loss:0.016913966789737823\n",
      "train loss:0.012139381731686723\n",
      "train loss:0.020787946030411328\n",
      "train loss:0.006685065570902431\n",
      "train loss:0.01788800343292326\n",
      "train loss:0.007466969793814552\n",
      "train loss:0.004152171211878563\n",
      "train loss:0.025877276526629602\n",
      "train loss:0.002532414873092681\n",
      "train loss:0.003728170046502568\n",
      "train loss:0.010468896893820457\n",
      "train loss:0.020308729505142557\n",
      "train loss:0.014160599334658013\n",
      "train loss:0.0034537492675324063\n",
      "train loss:0.03153220177655516\n",
      "train loss:0.015758803104676936\n",
      "train loss:0.023652708428162396\n",
      "train loss:0.015454099438603642\n",
      "train loss:0.018770139210029096\n",
      "train loss:0.05280069127459837\n",
      "train loss:0.006155581063699677\n",
      "train loss:0.04005791966646693\n",
      "train loss:0.0069641108565097\n",
      "train loss:0.009919021449879665\n",
      "train loss:0.01600997356620706\n",
      "train loss:0.005519664913042121\n",
      "train loss:0.008136596314853792\n",
      "train loss:0.004880713481006417\n",
      "train loss:0.010509038802021968\n",
      "train loss:0.004934672989306222\n",
      "train loss:0.003618590001992897\n",
      "train loss:0.0024888924849410504\n",
      "train loss:0.004924442396879496\n",
      "train loss:0.01048998749726286\n",
      "train loss:0.027793966659065243\n",
      "train loss:0.03620309667985152\n",
      "train loss:0.011188007860811353\n",
      "train loss:0.029889950311570455\n",
      "train loss:0.010531287139502883\n",
      "train loss:0.003546680283799271\n",
      "train loss:0.016600661831364508\n",
      "train loss:0.008089878338427246\n",
      "train loss:0.013091746421180082\n",
      "train loss:0.04058996752602986\n",
      "train loss:0.014674802866073852\n",
      "train loss:0.006605738886377504\n",
      "train loss:0.006750542124882657\n",
      "train loss:0.007152001802889393\n",
      "train loss:0.004385839121142977\n",
      "train loss:0.011382508161689968\n",
      "train loss:0.01706207137021705\n",
      "train loss:0.003659079061099992\n",
      "train loss:0.0024512563564392767\n",
      "train loss:0.03584986283097895\n",
      "train loss:0.008594401034048585\n",
      "train loss:0.011802200142942148\n",
      "train loss:0.038710242753926295\n",
      "train loss:0.018534067281288754\n",
      "train loss:0.015414902798329671\n",
      "train loss:0.0036630609867220605\n",
      "train loss:0.015919812326262422\n",
      "train loss:0.0022755788701312755\n",
      "train loss:0.007123792008226279\n",
      "train loss:0.012595095034525706\n",
      "train loss:0.023787197315701905\n",
      "train loss:0.004422537486984378\n",
      "train loss:0.04831681826588752\n",
      "train loss:0.04673954310484413\n",
      "train loss:0.07039921401283976\n",
      "train loss:0.07048601178186406\n",
      "train loss:0.013366357942638539\n",
      "train loss:0.013318452786866159\n",
      "train loss:0.03554880652878306\n",
      "train loss:0.02424144312129255\n",
      "train loss:0.005122026133865227\n",
      "train loss:0.004389876347124876\n",
      "train loss:0.011700067494534944\n",
      "train loss:0.05732650969586513\n",
      "train loss:0.006731562380178005\n",
      "train loss:0.018276293890044457\n",
      "train loss:0.029264247452144822\n",
      "train loss:0.06693833324303344\n",
      "train loss:0.005835964684822975\n",
      "train loss:0.0029982029825623186\n",
      "train loss:0.02903950849151003\n",
      "train loss:0.019002448168873743\n",
      "train loss:0.011858823792697624\n",
      "train loss:0.01084446896658512\n",
      "train loss:0.017674229605038307\n",
      "train loss:0.038781059928707375\n",
      "train loss:0.003860320678761032\n",
      "train loss:0.02201482604675104\n",
      "train loss:0.006409834741029886\n",
      "train loss:0.003725768079735706\n",
      "train loss:0.0065888512711129886\n",
      "train loss:0.008225871509059204\n",
      "train loss:0.05910447661179417\n",
      "train loss:0.010997279959991278\n",
      "train loss:0.0332551625260271\n",
      "train loss:0.012189178387344657\n",
      "train loss:0.020863282172509358\n",
      "=== epoch:8, train acc:0.993, test acc:0.986 ===\n",
      "train loss:0.016142572699057764\n",
      "train loss:0.007065546493858707\n",
      "train loss:0.004282156000968322\n",
      "train loss:0.019366853614797074\n",
      "train loss:0.03014845964573676\n",
      "train loss:0.029945735473772554\n",
      "train loss:0.00883547931814855\n",
      "train loss:0.014560495778542766\n",
      "train loss:0.019766374761860928\n",
      "train loss:0.005395241251257958\n",
      "train loss:0.013139385673112411\n",
      "train loss:0.012841684784683767\n",
      "train loss:0.05574749920245981\n",
      "train loss:0.018726793463286498\n",
      "train loss:0.004599541887830506\n",
      "train loss:0.013029680352157891\n",
      "train loss:0.014295494746494069\n",
      "train loss:0.004289053475476993\n",
      "train loss:0.003997692807283443\n",
      "train loss:0.004119404278455964\n",
      "train loss:0.011626775264281493\n",
      "train loss:0.014432717022525347\n",
      "train loss:0.06812721457225858\n",
      "train loss:0.01912709428702709\n",
      "train loss:0.001988308557731832\n",
      "train loss:0.0014969314183622725\n",
      "train loss:0.005867555145459189\n",
      "train loss:0.02173734663656612\n",
      "train loss:0.02925629041453294\n",
      "train loss:0.025171301215494588\n",
      "train loss:0.006063106481458228\n",
      "train loss:0.00939015959471508\n",
      "train loss:0.02326044828245301\n",
      "train loss:0.007511833293201983\n",
      "train loss:0.015702128816354022\n",
      "train loss:0.02053142849063803\n",
      "train loss:0.0128008762298778\n",
      "train loss:0.0033747099698887132\n",
      "train loss:0.004689213067837638\n",
      "train loss:0.024858700222217922\n",
      "train loss:0.007181565303958909\n",
      "train loss:0.013134723439994176\n",
      "train loss:0.021425739913665406\n",
      "train loss:0.014112052349501065\n",
      "train loss:0.005855106638783593\n",
      "train loss:0.01057335477236198\n",
      "train loss:0.003858098265192704\n",
      "train loss:0.0063283402313603744\n",
      "train loss:0.0018530778775052211\n",
      "train loss:0.027319427256902885\n",
      "train loss:0.06628652186788808\n",
      "train loss:0.015723072166657626\n",
      "train loss:0.023237253609371657\n",
      "train loss:0.013015803836275\n",
      "train loss:0.017521525205041355\n",
      "train loss:0.006089042582265538\n",
      "train loss:0.021099723741764057\n",
      "train loss:0.012368349785463872\n",
      "train loss:0.08363673796544972\n",
      "train loss:0.01968109937441103\n",
      "train loss:0.0029117036020371414\n",
      "train loss:0.014417994880357181\n",
      "train loss:0.016488468291419152\n",
      "train loss:0.0037421987696747983\n",
      "train loss:0.027977108698643994\n",
      "train loss:0.009119099141103622\n",
      "train loss:0.022477211149598074\n",
      "train loss:0.004624954487162301\n",
      "train loss:0.002765220911553449\n",
      "train loss:0.003792686222810604\n",
      "train loss:0.01204597339345008\n",
      "train loss:0.023575909782852178\n",
      "train loss:0.0034196689635518566\n",
      "train loss:0.0017024302799847613\n",
      "train loss:0.0054414512252805225\n",
      "train loss:0.011564801616888266\n",
      "train loss:0.008443271402176745\n",
      "train loss:0.014633266775688623\n",
      "train loss:0.0011090274729648392\n",
      "train loss:0.006849821462038267\n",
      "train loss:0.007640649112713539\n",
      "train loss:0.009571984460321642\n",
      "train loss:0.022281912643115825\n",
      "train loss:0.0036640329561693995\n",
      "train loss:0.010267883649128179\n",
      "train loss:0.0425132782174293\n",
      "train loss:0.0125671116023674\n",
      "train loss:0.012084774155493106\n",
      "train loss:0.020836184827942358\n",
      "train loss:0.059775259091237626\n",
      "train loss:0.004295288410974654\n",
      "train loss:0.004337792254932632\n",
      "train loss:0.01343283183025054\n",
      "train loss:0.019291235239461017\n",
      "train loss:0.04423361147228584\n",
      "train loss:0.0657537182342076\n",
      "train loss:0.017825141149318845\n",
      "train loss:0.006140007486813672\n",
      "train loss:0.003170459981780295\n",
      "train loss:0.011972530864981169\n",
      "train loss:0.06333007597276899\n",
      "train loss:0.004044176930707937\n",
      "train loss:0.011378407043784644\n",
      "train loss:0.059223023277384865\n",
      "train loss:0.0020498997167646706\n",
      "train loss:0.0051016112659675\n",
      "train loss:0.03251045917492947\n",
      "train loss:0.008673010204015877\n",
      "train loss:0.0035615749317718408\n",
      "train loss:0.05617572014616662\n",
      "train loss:0.013166206193217402\n",
      "train loss:0.037213661125210525\n",
      "train loss:0.0033492233194291864\n",
      "train loss:0.00474933382731108\n",
      "train loss:0.04039736308681205\n",
      "train loss:0.01584980695676574\n",
      "train loss:0.028958693366920526\n",
      "train loss:0.006608050353339619\n",
      "train loss:0.0061790354710756215\n",
      "train loss:0.002823684802441766\n",
      "train loss:0.004816956175438814\n",
      "train loss:0.016774866114318998\n",
      "train loss:0.014306446350098556\n",
      "train loss:0.004665470725236121\n",
      "train loss:0.009966477872272127\n",
      "train loss:0.014692332239890723\n",
      "train loss:0.01296632289157215\n",
      "train loss:0.01573271841449073\n",
      "train loss:0.004767425298581428\n",
      "train loss:0.001768351835359288\n",
      "train loss:0.01528394549902842\n",
      "train loss:0.012318827875789709\n",
      "train loss:0.006106457168393038\n",
      "train loss:0.0020389955940243055\n",
      "train loss:0.015193879459366656\n",
      "train loss:0.00209153823948539\n",
      "train loss:0.020314987453484104\n",
      "train loss:0.004507196146648865\n",
      "train loss:0.018972660182040104\n",
      "train loss:0.003625315305199371\n",
      "train loss:0.004779359082649686\n",
      "train loss:0.011620098051218379\n",
      "train loss:0.008224946458969196\n",
      "train loss:0.011674824584420635\n",
      "train loss:0.009403119116271867\n",
      "train loss:0.0048243643482651\n",
      "train loss:0.014441666286881281\n",
      "train loss:0.006845360261764712\n",
      "train loss:0.01956439542273797\n",
      "train loss:0.009488933040940482\n",
      "train loss:0.008809619781379485\n",
      "train loss:0.009304383445797515\n",
      "train loss:0.006399586337250978\n",
      "train loss:0.029710768018495832\n",
      "train loss:0.007759384187263181\n",
      "train loss:0.019639924956585147\n",
      "train loss:0.032682024560438915\n",
      "train loss:0.006722512640977312\n",
      "train loss:0.02061061095831248\n",
      "train loss:0.021352952535446036\n",
      "train loss:0.007121922266318881\n",
      "train loss:0.002860968241516638\n",
      "train loss:0.00533215709952489\n",
      "train loss:0.0011524908690139484\n",
      "train loss:0.0020514220707850274\n",
      "train loss:0.0013382263035388337\n",
      "train loss:0.010838963659998169\n",
      "train loss:0.0040024554527893\n",
      "train loss:0.0012255288513640197\n",
      "train loss:0.009978587145654918\n",
      "train loss:0.009187533593529737\n",
      "train loss:0.0022197160315074755\n",
      "train loss:0.005745421654517743\n",
      "train loss:0.00284221876536434\n",
      "train loss:0.003842880017244744\n",
      "train loss:0.003994844502260015\n",
      "train loss:0.015376740299974363\n",
      "train loss:0.029531919221995176\n",
      "train loss:0.0022017358679784953\n",
      "train loss:0.0027453690683829535\n",
      "train loss:0.009112166437493057\n",
      "train loss:0.014931690935122206\n",
      "train loss:0.0015874128150626817\n",
      "train loss:0.006053171012535845\n",
      "train loss:0.0011095802167327123\n",
      "train loss:0.019114740762862074\n",
      "train loss:0.0016929584911719421\n",
      "train loss:0.007409681463340213\n",
      "train loss:0.02220505388982616\n",
      "train loss:0.005577677888222578\n",
      "train loss:0.0031594023627930586\n",
      "train loss:0.005063257185716906\n",
      "train loss:0.0068263380595397025\n",
      "train loss:0.0059074735134349455\n",
      "train loss:0.03318654767326629\n",
      "train loss:0.006327096455907759\n",
      "train loss:0.0023802100553980336\n",
      "train loss:0.005170517820617084\n",
      "train loss:0.029729491860647057\n",
      "train loss:0.0019518633365118102\n",
      "train loss:0.003531332778064938\n",
      "train loss:0.014817939361212357\n",
      "train loss:0.009322240585658286\n",
      "train loss:0.02877023244884528\n",
      "train loss:0.0016587875791703357\n",
      "train loss:0.01189593460851413\n",
      "train loss:0.003869612356212613\n",
      "train loss:0.0077448354194428115\n",
      "train loss:0.07303467406277728\n",
      "train loss:0.0021306211760414476\n",
      "train loss:0.015828591348943807\n",
      "train loss:0.008513502383562887\n",
      "train loss:0.004784461293702995\n",
      "train loss:0.012845809243819782\n",
      "train loss:0.017291621292576834\n",
      "train loss:0.009677053817000649\n",
      "train loss:0.04773941024746454\n",
      "train loss:0.025516031328496957\n",
      "train loss:0.009250620338458327\n",
      "train loss:0.00238802228140717\n",
      "train loss:0.01732186627892235\n",
      "train loss:0.0027186428681396867\n",
      "train loss:0.02810214352137673\n",
      "train loss:0.035545876850081065\n",
      "train loss:0.001725281454195455\n",
      "train loss:0.01887565912041441\n",
      "train loss:0.01287046319264904\n",
      "train loss:0.0018421939053038167\n",
      "train loss:0.0009358766796270403\n",
      "train loss:0.00418567731007059\n",
      "train loss:0.007793121336437117\n",
      "train loss:0.02792083589455411\n",
      "train loss:0.034761818547961364\n",
      "train loss:0.001491145215999342\n",
      "train loss:0.004292912324022677\n",
      "train loss:0.016212494254410837\n",
      "train loss:0.0052254353053706095\n",
      "train loss:0.001441613563824782\n",
      "train loss:0.012169529572513776\n",
      "train loss:0.005822594849478728\n",
      "train loss:0.0037695598303114823\n",
      "train loss:0.0017307284183060078\n",
      "train loss:0.005745569073854767\n",
      "train loss:0.032515875602008894\n",
      "train loss:0.00053405714269785\n",
      "train loss:0.004099679912035383\n",
      "train loss:0.0022175955184262257\n",
      "train loss:0.0015200818021416\n",
      "train loss:0.004264678561559232\n",
      "train loss:0.0035464924345644483\n",
      "train loss:0.02462617098626939\n",
      "train loss:0.01930088230156345\n",
      "train loss:0.007782556194066377\n",
      "train loss:0.0132126472325141\n",
      "train loss:0.08300424884990645\n",
      "train loss:0.016854080932582453\n",
      "train loss:0.0032753424987881307\n",
      "train loss:0.021635624362689464\n",
      "train loss:0.004977228300550623\n",
      "train loss:0.0023171753689591114\n",
      "train loss:0.007568720329762791\n",
      "train loss:0.013985239831583013\n",
      "train loss:0.009715526961568282\n",
      "train loss:0.002025913658781092\n",
      "train loss:0.002335871042598612\n",
      "train loss:0.006024473519306181\n",
      "train loss:0.0028034015151620286\n",
      "train loss:0.011074193530881517\n",
      "train loss:0.060019135445197734\n",
      "train loss:0.011014989801013507\n",
      "train loss:0.014595066440158096\n",
      "train loss:0.003621054223185003\n",
      "train loss:0.005716616132025001\n",
      "train loss:0.007163197791849282\n",
      "train loss:0.0060817183501562745\n",
      "train loss:0.0042858394751348035\n",
      "train loss:0.004458042411426976\n",
      "train loss:0.004694173211978632\n",
      "train loss:0.01033309635198194\n",
      "train loss:0.023619990469679112\n",
      "train loss:0.02403935576477053\n",
      "train loss:0.008901199427216103\n",
      "train loss:0.004006615016053638\n",
      "train loss:0.011589262975136143\n",
      "train loss:0.0035267304271329406\n",
      "train loss:0.007931785532335582\n",
      "train loss:0.017523119945529474\n",
      "train loss:0.004358737118440528\n",
      "train loss:0.0013346756811269352\n",
      "train loss:0.023725086801820193\n",
      "train loss:0.016326701457936226\n",
      "train loss:0.016050985678023533\n",
      "train loss:0.004440546020457636\n",
      "train loss:0.005768727132855288\n",
      "train loss:0.01285919297861987\n",
      "train loss:0.013559479174409673\n",
      "train loss:0.008775720419680519\n",
      "train loss:0.022922730687218455\n",
      "train loss:0.0013419255048905826\n",
      "train loss:0.003413646366589247\n",
      "train loss:0.006691789636307024\n",
      "train loss:0.01335359121456378\n",
      "train loss:0.0006253030240930134\n",
      "train loss:0.04822678394341442\n",
      "train loss:0.018985647450479856\n",
      "train loss:0.0028223966858196507\n",
      "train loss:0.01826075754361287\n",
      "train loss:0.01403591611894604\n",
      "train loss:0.010243857916256628\n",
      "train loss:0.0065541126290547345\n",
      "train loss:0.017442386466573548\n",
      "train loss:0.021619678764414382\n",
      "train loss:0.012044693848336474\n",
      "train loss:0.00196442069871956\n",
      "train loss:0.00446123331842176\n",
      "train loss:0.028188456621548372\n",
      "train loss:0.004421098522799957\n",
      "train loss:0.0015846743057695994\n",
      "train loss:0.00664888271391281\n",
      "train loss:0.005965558385647604\n",
      "train loss:0.012002236932004836\n",
      "train loss:0.011327717996491366\n",
      "train loss:0.00901427539187271\n",
      "train loss:0.03463497713158099\n",
      "train loss:0.00489895931675114\n",
      "train loss:0.004609820526176496\n",
      "train loss:0.0061810374151054556\n",
      "train loss:0.005341777190978283\n",
      "train loss:0.010944495622183303\n",
      "train loss:0.041986730870292185\n",
      "train loss:0.002101548558100974\n",
      "train loss:0.003092467920232091\n",
      "train loss:0.002940839174174116\n",
      "train loss:0.00564281502087406\n",
      "train loss:0.0036974822241361855\n",
      "train loss:0.006903202967567534\n",
      "train loss:0.0031113130297557375\n",
      "train loss:0.007861551004842269\n",
      "train loss:0.0032202560080579245\n",
      "train loss:0.002241552572629131\n",
      "train loss:0.024255687815712898\n",
      "train loss:0.021282693011531063\n",
      "train loss:0.0019694479403419454\n",
      "train loss:0.0062942793937018\n",
      "train loss:0.004788154303844297\n",
      "train loss:0.0025214419947050502\n",
      "train loss:0.004439460361174249\n",
      "train loss:0.003726142812075204\n",
      "train loss:0.012275422187861356\n",
      "train loss:0.004117060011369511\n",
      "train loss:0.00140500152746914\n",
      "train loss:0.003700414409571352\n",
      "train loss:0.015652335325931358\n",
      "train loss:0.0018642000626573516\n",
      "train loss:0.0013584666684934945\n",
      "train loss:0.0021713848036971654\n",
      "train loss:0.02803144749068233\n",
      "train loss:0.039688875437002676\n",
      "train loss:0.018176446602241034\n",
      "train loss:0.002922742606364118\n",
      "train loss:0.0012114534483342481\n",
      "train loss:0.027665291875343764\n",
      "train loss:0.022042491753795343\n",
      "train loss:0.00956431591283097\n",
      "train loss:0.004567450420364493\n",
      "train loss:0.0025778116707492454\n",
      "train loss:0.003763058573394469\n",
      "train loss:0.007055878195687875\n",
      "train loss:0.0144917341773046\n",
      "train loss:0.01365631652918296\n",
      "train loss:0.002376654419133643\n",
      "train loss:0.017939434866794658\n",
      "train loss:0.005193547923666228\n",
      "train loss:0.10059810787033059\n",
      "train loss:0.004350817589566522\n",
      "train loss:0.004914911564714694\n",
      "train loss:0.011652712296839205\n",
      "train loss:0.0074434574201917795\n",
      "train loss:0.013090280781375762\n",
      "train loss:0.023835621783502122\n",
      "train loss:0.031381083027092166\n",
      "train loss:0.0013192903765223555\n",
      "train loss:0.0039090199348744605\n",
      "train loss:0.005565165833268505\n",
      "train loss:0.0340834964884115\n",
      "train loss:0.011515789957028337\n",
      "train loss:0.004148300138701306\n",
      "train loss:0.02776230085818616\n",
      "train loss:0.006493118299112267\n",
      "train loss:0.0030903753117333347\n",
      "train loss:0.02681675687007886\n",
      "train loss:0.019261623077175532\n",
      "train loss:0.01882852012314111\n",
      "train loss:0.004009932928879972\n",
      "train loss:0.00744836356951212\n",
      "train loss:0.010187403213911697\n",
      "train loss:0.0022921950444059615\n",
      "train loss:0.008361013260318361\n",
      "train loss:0.008216791358326881\n",
      "train loss:0.010418596045881485\n",
      "train loss:0.028355726370880175\n",
      "train loss:0.002722108397498576\n",
      "train loss:0.010396589009687509\n",
      "train loss:0.002113855091710094\n",
      "train loss:0.023672993868407755\n",
      "train loss:0.018726305839356\n",
      "train loss:0.014161168101420617\n",
      "train loss:0.009726827623406344\n",
      "train loss:0.021710567934244186\n",
      "train loss:0.014769769783739181\n",
      "train loss:0.007197346245633784\n",
      "train loss:0.001238851695115084\n",
      "train loss:0.011938833701854843\n",
      "train loss:0.060828862052287574\n",
      "train loss:0.008930312997660493\n",
      "train loss:0.01378353372754485\n",
      "train loss:0.009612620693473213\n",
      "train loss:0.009130247080457218\n",
      "train loss:0.003976602933648887\n",
      "train loss:0.023989073045302158\n",
      "train loss:0.0025662282494470097\n",
      "train loss:0.010100900326496045\n",
      "train loss:0.0062785229427085\n",
      "train loss:0.031091901342151534\n",
      "train loss:0.002756004122035952\n",
      "train loss:0.03852436465659007\n",
      "train loss:0.0074015147861269705\n",
      "train loss:0.0013061849970519169\n",
      "train loss:0.002685970346635017\n",
      "train loss:0.01727045236749391\n",
      "train loss:0.006724163232165275\n",
      "train loss:0.007592216300797547\n",
      "train loss:0.009362306410325842\n",
      "train loss:0.04049164169701146\n",
      "train loss:0.0017146821257579326\n",
      "train loss:0.010994167385605212\n",
      "train loss:0.004141032279283242\n",
      "train loss:0.014284435054026563\n",
      "train loss:0.001530467367188134\n",
      "train loss:0.018744820692845637\n",
      "train loss:0.004585098013516601\n",
      "train loss:0.001941008177377251\n",
      "train loss:0.09737554870619007\n",
      "train loss:0.009178761192319647\n",
      "train loss:0.008408607540608286\n",
      "train loss:0.023463967578302256\n",
      "train loss:0.005316561055032856\n",
      "train loss:0.0036416842811855116\n",
      "train loss:0.005540972016092299\n",
      "train loss:0.006608571773917849\n",
      "train loss:0.008133285012372609\n",
      "train loss:0.028245492584657014\n",
      "train loss:0.032601962989430126\n",
      "train loss:0.030784568362715477\n",
      "train loss:0.007540809989313227\n",
      "train loss:0.018646452988683074\n",
      "train loss:0.035373664290655166\n",
      "train loss:0.006207429419244734\n",
      "train loss:0.022489384888608865\n",
      "train loss:0.007506365174028069\n",
      "train loss:0.012138785134974088\n",
      "train loss:0.006562560569938109\n",
      "train loss:0.006512420688520344\n",
      "train loss:0.010122343737093658\n",
      "train loss:0.010429700449007602\n",
      "train loss:0.01252589682068335\n",
      "train loss:0.013171727943173119\n",
      "train loss:0.02409801187495457\n",
      "train loss:0.004721276024000008\n",
      "train loss:0.003302784132695849\n",
      "train loss:0.0020210903271482327\n",
      "train loss:0.010791876643433394\n",
      "train loss:0.006699993977358246\n",
      "train loss:0.007705637639310679\n",
      "train loss:0.014165830107781795\n",
      "train loss:0.004948102774627924\n",
      "train loss:0.04340136984036417\n",
      "train loss:0.005759930935188777\n",
      "train loss:0.009210912380116433\n",
      "train loss:0.014498119086464504\n",
      "train loss:0.03555453657447947\n",
      "train loss:0.014218908651295013\n",
      "train loss:0.008403747351666234\n",
      "train loss:0.014606146620720198\n",
      "train loss:0.015012808652722476\n",
      "train loss:0.04596601382765795\n",
      "train loss:0.0018673701456655473\n",
      "train loss:0.014414730329771974\n",
      "train loss:0.0049423373729160286\n",
      "train loss:0.005641829234554546\n",
      "train loss:0.004754810867869478\n",
      "train loss:0.008619782298158147\n",
      "train loss:0.0016486224473817063\n",
      "train loss:0.018187332478678236\n",
      "train loss:0.012591123677846974\n",
      "train loss:0.0016458475913801898\n",
      "train loss:0.013491403226808103\n",
      "train loss:0.003240474385978655\n",
      "train loss:0.004029137091253612\n",
      "train loss:0.018850046919602924\n",
      "train loss:0.02118378626842198\n",
      "train loss:0.03510354049592939\n",
      "train loss:0.008411625999684959\n",
      "train loss:0.004951720333645794\n",
      "train loss:0.004270315748734802\n",
      "train loss:0.014174846174891374\n",
      "train loss:0.004662149838158021\n",
      "train loss:0.000707448735475328\n",
      "train loss:0.0008271384669966884\n",
      "train loss:0.006199109783579659\n",
      "train loss:0.004813954145866386\n",
      "train loss:0.005276148839982001\n",
      "train loss:0.01914519432776873\n",
      "train loss:0.0025321089963645854\n",
      "train loss:0.0035844911280182735\n",
      "train loss:0.020828036954759824\n",
      "train loss:0.014532430572430059\n",
      "train loss:0.0027183410461615936\n",
      "train loss:0.0046446933058244405\n",
      "train loss:0.007834656294862914\n",
      "train loss:0.008308280411263462\n",
      "train loss:0.009112749517414498\n",
      "train loss:0.010849695151965575\n",
      "train loss:0.004939125245056339\n",
      "train loss:0.003977780590791404\n",
      "train loss:0.024414203310832647\n",
      "train loss:0.022059858254794583\n",
      "train loss:0.004600791436110663\n",
      "train loss:0.021552510439064032\n",
      "train loss:0.0051396235272981585\n",
      "train loss:0.004282749445009802\n",
      "train loss:0.004681172326691753\n",
      "train loss:0.016496848594809092\n",
      "train loss:0.01824557221470349\n",
      "train loss:0.03668533725710077\n",
      "train loss:0.004863341847510459\n",
      "train loss:0.003299655179235573\n",
      "train loss:0.005172349836838913\n",
      "train loss:0.01890028928020601\n",
      "train loss:0.004562177606332554\n",
      "train loss:0.0023996716455935278\n",
      "train loss:0.054260622561270164\n",
      "train loss:0.005310810541518277\n",
      "train loss:0.0037139989101892747\n",
      "train loss:0.009451210008333742\n",
      "train loss:0.009439501672697007\n",
      "train loss:0.011712901951550902\n",
      "train loss:0.011651158936662019\n",
      "train loss:0.03109790635451935\n",
      "train loss:0.012772442321430474\n",
      "train loss:0.026128890474132214\n",
      "train loss:0.011545790871220221\n",
      "train loss:0.0038021683727295363\n",
      "train loss:0.013590998856459154\n",
      "train loss:0.007567979249006306\n",
      "train loss:0.01405293727642592\n",
      "train loss:0.007241168160228552\n",
      "train loss:0.012579869487487804\n",
      "train loss:0.005746189161140214\n",
      "train loss:0.00675337349543589\n",
      "train loss:0.0060026994857867175\n",
      "train loss:0.04791676342116816\n",
      "train loss:0.07212258227079599\n",
      "train loss:0.016303449647957026\n",
      "train loss:0.0034951186433949876\n",
      "train loss:0.02577233291551964\n",
      "train loss:0.002629893509645983\n",
      "train loss:0.00761578231079661\n",
      "train loss:0.03396093384289286\n",
      "train loss:0.012515826462032637\n",
      "train loss:0.05409745757772187\n",
      "train loss:0.0033145810117009137\n",
      "train loss:0.01132878231565418\n",
      "train loss:0.01510267205930747\n",
      "train loss:0.02124846924859852\n",
      "train loss:0.0025573542421533303\n",
      "train loss:0.0036699999745774294\n",
      "train loss:0.008804382461080507\n",
      "train loss:0.0049722207408440825\n",
      "train loss:0.010865981134630436\n",
      "train loss:0.015037269994230627\n",
      "train loss:0.0070900105077327654\n",
      "train loss:0.02427842931786972\n",
      "train loss:0.017483959896289605\n",
      "train loss:0.04064081668112557\n",
      "train loss:0.01626192781480411\n",
      "train loss:0.015002865202063936\n",
      "train loss:0.004458432742610036\n",
      "train loss:0.030263449910607067\n",
      "train loss:0.012353260795215124\n",
      "train loss:0.0055895155712500375\n",
      "train loss:0.021597958235659354\n",
      "train loss:0.002518583446388392\n",
      "train loss:0.00683194017237845\n",
      "train loss:0.016335674307012573\n",
      "train loss:0.02852647143514366\n",
      "train loss:0.008517188634084828\n",
      "train loss:0.007077524011835349\n",
      "train loss:0.001663175381351808\n",
      "train loss:0.017251918016872404\n",
      "=== epoch:9, train acc:0.99, test acc:0.987 ===\n",
      "train loss:0.00419841162373748\n",
      "train loss:0.0010066622462644008\n",
      "train loss:0.01518301688843689\n",
      "train loss:0.007638342427904466\n",
      "train loss:0.010188223326383467\n",
      "train loss:0.038761840203302374\n",
      "train loss:0.007883032988652682\n",
      "train loss:0.011249439878790068\n",
      "train loss:0.00417434281369241\n",
      "train loss:0.011373939843018719\n",
      "train loss:0.000567783411542548\n",
      "train loss:0.0031988363559117745\n",
      "train loss:0.008683182206367882\n",
      "train loss:0.009079184719610455\n",
      "train loss:0.03528141051054897\n",
      "train loss:0.023284161718920185\n",
      "train loss:0.009948513329837962\n",
      "train loss:0.0031843172842625704\n",
      "train loss:0.00522248350471995\n",
      "train loss:0.007596877969040698\n",
      "train loss:0.006459304084471687\n",
      "train loss:0.013531904756004387\n",
      "train loss:0.006111819659256639\n",
      "train loss:0.016684325334983412\n",
      "train loss:0.007099363037424023\n",
      "train loss:0.013516075382359454\n",
      "train loss:0.010162451069004895\n",
      "train loss:0.012626228577163678\n",
      "train loss:0.0010529830529457005\n",
      "train loss:0.008628099047474913\n",
      "train loss:0.00762568255973023\n",
      "train loss:0.015155743351928033\n",
      "train loss:0.006070629774139771\n",
      "train loss:0.0068193961670525724\n",
      "train loss:0.003481216936136633\n",
      "train loss:0.0008230884786444064\n",
      "train loss:0.026310944592085272\n",
      "train loss:0.006838412828170763\n",
      "train loss:0.002071845999721029\n",
      "train loss:0.008210507116592077\n",
      "train loss:0.030755937590117716\n",
      "train loss:0.004283625564996375\n",
      "train loss:0.0046283677496009815\n",
      "train loss:0.018994194426036542\n",
      "train loss:0.0023764163790301095\n",
      "train loss:0.0041149801182006115\n",
      "train loss:0.03628974224602369\n",
      "train loss:0.00913531791038099\n",
      "train loss:0.008616526825138164\n",
      "train loss:0.00362148409517628\n",
      "train loss:0.0032810540117933114\n",
      "train loss:0.016814934200105176\n",
      "train loss:0.02286425117434008\n",
      "train loss:0.009492501303947975\n",
      "train loss:0.00680295918732334\n",
      "train loss:0.00784083375243282\n",
      "train loss:0.008556347998430408\n",
      "train loss:0.011983477229089084\n",
      "train loss:0.0018457259107722518\n",
      "train loss:0.006075244437255958\n",
      "train loss:0.008399225464719036\n",
      "train loss:0.0067213461688864825\n",
      "train loss:0.0018246659006955453\n",
      "train loss:0.005418256091143005\n",
      "train loss:0.006538468339287817\n",
      "train loss:0.012354256604947964\n",
      "train loss:0.008008993340124126\n",
      "train loss:0.0019038526079321642\n",
      "train loss:0.007875544222982638\n",
      "train loss:0.0037726507960144486\n",
      "train loss:0.0031455851795422333\n",
      "train loss:0.018748201025660115\n",
      "train loss:0.0016824691096557127\n",
      "train loss:0.0033581731722087532\n",
      "train loss:0.01842153576580923\n",
      "train loss:0.004207449439890884\n",
      "train loss:0.010948129633753901\n",
      "train loss:0.017540301026718025\n",
      "train loss:0.004207939914124091\n",
      "train loss:0.032200275830223425\n",
      "train loss:0.0014413088627023606\n",
      "train loss:0.02366344906796817\n",
      "train loss:0.00784994936591732\n",
      "train loss:0.00885435791370199\n",
      "train loss:0.019364410406626877\n",
      "train loss:0.014880033436723934\n",
      "train loss:0.029898641181927758\n",
      "train loss:0.00293911375132295\n",
      "train loss:0.00893000243409605\n",
      "train loss:0.006604860639252981\n",
      "train loss:0.0006180652929575544\n",
      "train loss:0.0032736392339896623\n",
      "train loss:0.0029880327303636895\n",
      "train loss:0.004207745615339115\n",
      "train loss:0.00713041870320127\n",
      "train loss:0.026978462567698966\n",
      "train loss:0.015772383127543352\n",
      "train loss:0.08584383495606472\n",
      "train loss:0.0020441935260052694\n",
      "train loss:0.012397009626461524\n",
      "train loss:0.011713414542202281\n",
      "train loss:0.06370121090622315\n",
      "train loss:0.001596984231537249\n",
      "train loss:0.005663413582688161\n",
      "train loss:0.006238382523948316\n",
      "train loss:0.011069662394672946\n",
      "train loss:0.029477992921315355\n",
      "train loss:0.006527072942451294\n",
      "train loss:0.006802871625221558\n",
      "train loss:0.0043268175996475375\n",
      "train loss:0.007987749311435277\n",
      "train loss:0.009822038243471935\n",
      "train loss:0.005360811861916694\n",
      "train loss:0.009952605156814322\n",
      "train loss:0.018825894875965882\n",
      "train loss:0.011878805171709972\n",
      "train loss:0.005642078381123949\n",
      "train loss:0.0023477405801506372\n",
      "train loss:0.010838066200711515\n",
      "train loss:0.004755767830577962\n",
      "train loss:0.008123557659653639\n",
      "train loss:0.0022478269296043814\n",
      "train loss:0.002973151137189858\n",
      "train loss:0.005728848144534285\n",
      "train loss:0.005153714583104695\n",
      "train loss:0.008522993150823778\n",
      "train loss:0.009642815176749073\n",
      "train loss:0.0034347699365510644\n",
      "train loss:0.012083060616127194\n",
      "train loss:0.0024836341024965466\n",
      "train loss:0.022851683426478905\n",
      "train loss:0.005128917972242461\n",
      "train loss:0.02058584093723405\n",
      "train loss:0.0007774697607186844\n",
      "train loss:0.008287922687074836\n",
      "train loss:0.0039723738308175185\n",
      "train loss:0.00951564846737784\n",
      "train loss:0.008164590633613916\n",
      "train loss:0.0034917221211725373\n",
      "train loss:0.025523063568868052\n",
      "train loss:0.005536858635719438\n",
      "train loss:0.003165648066594057\n",
      "train loss:0.005976085392463546\n",
      "train loss:0.01354695868790952\n",
      "train loss:0.010466618755086447\n",
      "train loss:0.00793755088270877\n",
      "train loss:0.007916470966275563\n",
      "train loss:0.004121250367722837\n",
      "train loss:0.002752393484566531\n",
      "train loss:0.0014813589345976596\n",
      "train loss:0.007690619644925837\n",
      "train loss:0.0009636909586486598\n",
      "train loss:0.0015865054542402027\n",
      "train loss:0.03888363726802031\n",
      "train loss:0.021252425987951497\n",
      "train loss:0.002620792929978865\n",
      "train loss:0.009092209057276408\n",
      "train loss:0.014746752263695158\n",
      "train loss:0.0016959357112989724\n",
      "train loss:0.006081117181248056\n",
      "train loss:0.008529003360901018\n",
      "train loss:0.0022082667220185764\n",
      "train loss:0.021746936876286867\n",
      "train loss:0.006847454216538807\n",
      "train loss:0.0018153681393899659\n",
      "train loss:0.008120929356505671\n",
      "train loss:0.0011867203441850939\n",
      "train loss:0.042713533389320324\n",
      "train loss:0.0041880751145690785\n",
      "train loss:0.0092231000303794\n",
      "train loss:0.003930262526242663\n",
      "train loss:0.001276604821986761\n",
      "train loss:0.016738939773419937\n",
      "train loss:0.012391087254460397\n",
      "train loss:0.0030532141493650918\n",
      "train loss:0.006032129049619015\n",
      "train loss:0.025970110402812847\n",
      "train loss:0.030504951022480253\n",
      "train loss:0.011253056357299666\n",
      "train loss:0.023585520576512736\n",
      "train loss:0.001800012298056442\n",
      "train loss:0.0026757251594764675\n",
      "train loss:0.053531455135220056\n",
      "train loss:0.0012330647087690305\n",
      "train loss:0.003142203847623139\n",
      "train loss:0.0030980849916483395\n",
      "train loss:0.060057009166249155\n",
      "train loss:0.0025823929459979533\n",
      "train loss:0.01780073662103026\n",
      "train loss:0.014826241280527333\n",
      "train loss:0.015264930971836923\n",
      "train loss:0.004450362909967986\n",
      "train loss:0.0020504812618508065\n",
      "train loss:0.0027521358999008814\n",
      "train loss:0.008861160946304186\n",
      "train loss:0.06167724378550116\n",
      "train loss:0.029162911650744215\n",
      "train loss:0.032433003332239634\n",
      "train loss:0.0027364733640445425\n",
      "train loss:0.0029662090240312623\n",
      "train loss:0.0019756449461993918\n",
      "train loss:0.007754732374707809\n",
      "train loss:0.0079330980262438\n",
      "train loss:0.0037716774562088274\n",
      "train loss:0.0045338040272658794\n",
      "train loss:0.08123679624236647\n",
      "train loss:0.0030131226603254003\n",
      "train loss:0.0065969222639116355\n",
      "train loss:0.010068288634278192\n",
      "train loss:0.009733970198903466\n",
      "train loss:0.0037017853577469536\n",
      "train loss:0.003666115578087183\n",
      "train loss:0.0020529371055851044\n",
      "train loss:0.007992539930415946\n",
      "train loss:0.012462200879719826\n",
      "train loss:0.00353164451364531\n",
      "train loss:0.0164180395752854\n",
      "train loss:0.010143210153146098\n",
      "train loss:0.005349282481373352\n",
      "train loss:0.001821539518721644\n",
      "train loss:0.08207427140712208\n",
      "train loss:0.003197372768637005\n",
      "train loss:0.016175051795561498\n",
      "train loss:0.0015246431804848853\n",
      "train loss:0.039533035637395096\n",
      "train loss:0.006905138500155895\n",
      "train loss:0.04734294299412034\n",
      "train loss:0.01059616264236617\n",
      "train loss:0.00196980710030008\n",
      "train loss:0.01635764052000429\n",
      "train loss:0.04267077855355952\n",
      "train loss:0.017936236291554657\n",
      "train loss:0.009080499547750314\n",
      "train loss:0.030084622654268273\n",
      "train loss:0.007561154515192469\n",
      "train loss:0.006879960175563734\n",
      "train loss:0.005833514575437106\n",
      "train loss:0.014472580698368924\n",
      "train loss:0.006858041886650162\n",
      "train loss:0.01619415871639968\n",
      "train loss:0.0005674134391830027\n",
      "train loss:0.0039092736789845825\n",
      "train loss:0.011914049038770781\n",
      "train loss:0.004775420408925788\n",
      "train loss:0.002400843946098206\n",
      "train loss:0.012117937084522377\n",
      "train loss:0.0019913564335857646\n",
      "train loss:0.005079708011726845\n",
      "train loss:0.02523445122235473\n",
      "train loss:0.016681041509928447\n",
      "train loss:0.010420592864247836\n",
      "train loss:0.009894634015399776\n",
      "train loss:0.003265882778139846\n",
      "train loss:0.010016848451011195\n",
      "train loss:0.007309736771455521\n",
      "train loss:0.014457098185212543\n",
      "train loss:0.009246719662811763\n",
      "train loss:0.011216705539221416\n",
      "train loss:0.015035223601410632\n",
      "train loss:0.0017475958436467722\n",
      "train loss:0.013363463170321365\n",
      "train loss:0.01667365872021046\n",
      "train loss:0.015259734739634016\n",
      "train loss:0.006970185640865117\n",
      "train loss:0.013094683330129362\n",
      "train loss:0.008843066968857399\n",
      "train loss:0.022698831182109643\n",
      "train loss:0.005863782902284481\n",
      "train loss:0.0017670869418016598\n",
      "train loss:0.003918004776053851\n",
      "train loss:0.005189327297268815\n",
      "train loss:0.01635810182286196\n",
      "train loss:0.0010722711484058916\n",
      "train loss:0.011872485709149944\n",
      "train loss:0.008178230385655311\n",
      "train loss:0.00279578651368358\n",
      "train loss:0.008985547709896671\n",
      "train loss:0.02166969587152334\n",
      "train loss:0.0011009495659153564\n",
      "train loss:0.013800640008674198\n",
      "train loss:0.002588122692689458\n",
      "train loss:0.006090091197035606\n",
      "train loss:0.008546966718207654\n",
      "train loss:0.022024693160058686\n",
      "train loss:0.0015103068830120982\n",
      "train loss:0.010423452593964067\n",
      "train loss:0.00238392002796254\n",
      "train loss:0.001253911602150644\n",
      "train loss:0.012574219323813016\n",
      "train loss:0.002637465811903504\n",
      "train loss:0.0010078130304069606\n",
      "train loss:0.030880912559012215\n",
      "train loss:0.010611504962158251\n",
      "train loss:0.005754818608146285\n",
      "train loss:0.0013766258218970082\n",
      "train loss:0.0027308524322694917\n",
      "train loss:0.012370337289514503\n",
      "train loss:0.025870120467066497\n",
      "train loss:0.007980441037579776\n",
      "train loss:0.0011440205293639808\n",
      "train loss:0.011351420701200675\n",
      "train loss:0.006401339161257314\n",
      "train loss:0.002876900578434042\n",
      "train loss:0.0033120003114805313\n",
      "train loss:0.007836603318809646\n",
      "train loss:0.015531525151038634\n",
      "train loss:0.015363692491654203\n",
      "train loss:0.0036477252673799516\n",
      "train loss:0.0062079052603101524\n",
      "train loss:0.0020964438880450987\n",
      "train loss:0.00357584658839507\n",
      "train loss:0.008273830301126746\n",
      "train loss:0.012586999514502505\n",
      "train loss:0.0055997906120029564\n",
      "train loss:0.004130246231988144\n",
      "train loss:0.011830603424445\n",
      "train loss:0.0007307426687424222\n",
      "train loss:0.0023331124809637727\n",
      "train loss:0.0030382584660585583\n",
      "train loss:0.007102145735445682\n",
      "train loss:0.004908074550298453\n",
      "train loss:0.03167094120211869\n",
      "train loss:0.004357435214706391\n",
      "train loss:0.004249270800588508\n",
      "train loss:0.007037414172250333\n",
      "train loss:0.014735360981664766\n",
      "train loss:0.00886330800300035\n",
      "train loss:0.011422241520804444\n",
      "train loss:0.007788809701793223\n",
      "train loss:0.007156737166956506\n",
      "train loss:0.004348742575664099\n",
      "train loss:0.006873552907806499\n",
      "train loss:0.0014057789868036263\n",
      "train loss:0.005304321783941973\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=17'>18</a>\u001b[0m network \u001b[39m=\u001b[39m SimpleConvNet(input_dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=18'>19</a>\u001b[0m                         conv_param \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfilter_num\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m30\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfilter_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=19'>20</a>\u001b[0m                         hidden_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, weight_init_std\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=21'>22</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=22'>23</a>\u001b[0m                   epochs\u001b[39m=\u001b[39mmax_epochs, mini_batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=23'>24</a>\u001b[0m                   optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, optimizer_param\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.001\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=24'>25</a>\u001b[0m                   evaluate_sample_num_per_epoch\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=25'>26</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=27'>28</a>\u001b[0m \u001b[39m# 매개변수 보존\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bandaejun/Desktop/GLC/study/daejun4582/ch7_CNN.ipynb#ch0000022?line=28'>29</a>\u001b[0m network\u001b[39m.\u001b[39msave_params(\u001b[39m\"\u001b[39m\u001b[39mparams.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/GLC/study/daejun4582/../DLS_git_clone/common/trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[0;32m---> 71\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[1;32m     73\u001b[0m     test_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39maccuracy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_test)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Desktop/GLC/study/daejun4582/../DLS_git_clone/common/trainer.py:44\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m x_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_train[batch_mask]\n\u001b[1;32m     42\u001b[0m t_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_train[batch_mask]\n\u001b[0;32m---> 44\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mgradient(x_batch, t_batch)\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparams, grads)\n\u001b[1;32m     47\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mloss(x_batch, t_batch)\n",
      "File \u001b[0;32m~/Desktop/GLC/study/daejun4582/../DLS_git_clone/ch07/simple_convnet.py:139\u001b[0m, in \u001b[0;36mSimpleConvNet.gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    137\u001b[0m layers\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    138\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m layers:\n\u001b[0;32m--> 139\u001b[0m     dout \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(dout)\n\u001b[1;32m    141\u001b[0m \u001b[39m# 결과 저장\u001b[39;00m\n\u001b[1;32m    142\u001b[0m grads \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Desktop/GLC/study/daejun4582/../DLS_git_clone/common/layers.py:62\u001b[0m, in \u001b[0;36mAffine.backward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, dout):\n\u001b[0;32m---> 62\u001b[0m     dx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(dout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdW \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mT, dout)\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dout, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DLS_git_clone.common.trainer import Trainer\n",
    "from DLS_git_clone.dataset.mnist import load_mnist\n",
    "from DLS_git_clone.ch07.simple_convnet import SimpleConvNet\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7eeedbc38737b45b3f3d9c30fb6c381a1d5f1910f05692e0cad4cfbf19e6b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
