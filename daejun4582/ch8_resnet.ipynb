{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 딥러닝\n",
    "딥러닝은 층을 깊게 한 심층 신경망이다.  \n",
    "심층 신경망은 지금까지 설명한 신경망을 바탕으로 뒷닷에 층을 추가하기만 하면 만들 수 있지만, 커다란 문제가 있다.  \n",
    "이번장에서는 딥러닝의 특징과 과제, 그리고 가능성을 살펴볼 것이다.  \n",
    "또 오늘날의 첨단 딥러닝에 대한 설명도 준비했다.  \n",
    "> ## 8-1 더 깊게\n",
    "> 신경망에 관해 그동안 많은 것을 배웠다.  \n",
    "> 신경망을 구성하는 다양한 계층과 학습에 효과적인 기술, 영상 분야에 특히 유효한 CNN과 매개변수 최적화 기법등이 떠오를 것이다.\n",
    "> 이 모두가 딥러닝에서 중요한 기술이다.\n",
    "> 이번 절에서는 그동안 배운 기술을 집약하고 심층 신경망을 만들어 MNIST 데이터셋의 손글씨 숫자 인식에 도전하려 한다.\n",
    ">>### 8-1.1 더 깊은 신경망으로\n",
    ">> 거두절미 하고, 이번절에서는 그림 8-1 과 같이 구성된 CNN을 만들고자 한다.\n",
    ">> 척보아도 지금까지 구현한 신경망보다 층이 깊다. 여기에서 사용하는 합성곱 계층은 모두 3x3크기의 작은 필터로\n",
    ">> 층이 깊어지면서 채널수가 더 늘어나는 것이 특징이다.\n",
    ">> 또 그림과 같이 풀링 계층을 추가하여 중간 데이터의 공간 크기를 점차 줄여나간다.\n",
    ">> 마지막 단의 전 열결 계층에서는 드롭 아웃 계층을 사용한다.\n",
    ">> 가중치 초기값은 He 초기값을 사용하고, 가중치 매개변수 갱신에서는 Adam을 사용한다.\n",
    ">> 결과부터 말하여 이 신경마으이 정확도는 99.38%가 된다. 이 정도면 매우 훌륭하다.\n",
    ">> 그럼 이 신경망이 잘못 인식할 확률은 겨우 0.62% 이다. 그럼 실제로 어떤 이미지를 인식하지 못했는지 살펴보자 [그림 8-2]\n",
    ">> 그림의 사진들은 우리 인간도 판단하기 어려운 이미지이다. 이러한 점에서도 심층 CNN의 잠재력이 크다는 것을 새삼 느낄수 있다.\n",
    ">>### 8-1.2 정확도를 높이려면 \n",
    ">> <what is the class of this image>라는 웹사이트는 다양한 데이터셋을 대상으로 그동안 논문등에서 발표한 기법들의 정확도 순위를 정리해 두었다.\n",
    ">> 사실 상위권 대부분은 CNN을 기초로 한 기법들이 점령하였다. 상위들을 통하여 정확도에 대한 힌트를 얻을 수 있다.\n",
    ">> 예를 들어, 앙상블 학습, 학습률 감소, 데이터 확장등이 정확도 향상에 공헌하고 있다.\n",
    ">> 특히 데이터 확장은 손쉬운 방법이면서도 정확도 개선에 아주 효과적이다.\n",
    ">> 데이터 확장이란 (data augmentation)은 입력 이미지를 알고리즘을 동원해 인위적으로 확장한다는 것이다.\n",
    ">> 그림과 같이 입력 이미지를 회전하거나 세로로 이동하는 등 미세한 변화를 주어 이미지의 개수를 늘리는 것이다.\n",
    ">> 이는 데이터가 부족할때 특히 효과적인 수단이다.\n",
    ">> 데이터 확장은 이미지 일부를 잘라내는 crop 자위를 뒤집는 flip 등이 존재한다.\n",
    ">> ### 8-1.3 깊게 하는 이유\n",
    ">> 층을 깊게 하는 것이 왜 중요한가에 대한 이론적인 근거는 아직 많이 부족한 것이 사실이다.\n",
    ">> 그래도 지금까지의 연구와 실험 결과를 바탕으로 설명할 수 있는 것은 몇가지 있다.  \n",
    ">> 이번 절에서는 츠응ㄹ 깊게하는 것의 중요성에 대하여 이를 뒷받침하는 데이터와 설명을 소개하려 한다.\n",
    ">> 우선 층을 깊게 하는 것의 중요성은 ILSVRC로 대표되는 대규모 이미지 인식 대회의 결과에서 파악할 수 있다.  \n",
    ">> 이 대회에서 최근 상위를 차지만 기법은 대부분은 딥러닝 기반이며, 그 경향은 신경망을 더 깊게 만드는 방향으로 가고 있다.  \n",
    ">> 층이 깊이에 비례해 정확도가 좋아지는 것이다.\n",
    ">> 이어서 층을 깊게 할때의 이점을 설명하겠다.\n",
    ">> 그 이점 하나는 신경망의 매개변수 수가 줄어든다는 것이다.\n",
    ">> 층을 깊게 한 신겸망은 깊지 않은 경우보다 적은 매개변수로 같은 수준의 표현련을 달성할 수 있다.\n",
    ">> 합성곱 연산에서의 필터 크기에 주목해 생각해보면 쉽게 이해가 될것이다.\n",
    ">> 예를 한가지 들어보자 [그림 8-5]\n",
    ">> 5x5의 합성곱 연산 에 필요한 매개변수는 25개이다.\n",
    ">> 하지만 같은 결과를 출력하는 3x3의 합성곱 연산 2번에 필요한 매개변수는 18개이다.\n",
    ">> 즉 층이 깊어질수록 매개변수 개수의 차이는 커지는 것이다.\n",
    ">>  층이 깊어지면서 이룰 수 있는 또 다른 이점은 직관적으로 생가하였을때 학습을 더 세밀하게 할 수있다는 것이다.\n",
    ">>  층이 적으면 단번에 학습을 진행하여 특징을 추출해야 하지만 층이 많으면 그만큼 에지 정보부터 고차원 적인 패턴을 학습할 수 있다는 것이다.\n",
    "\n",
    "\n",
    "\n",
    ">## 8-2 딥러닝의 초기 역사\n",
    "> 딥러닝이 현재 처럼 큰 주목을 받게 된 계기는 이미지 인식 기술을 겨루는 ILSVRC의 2012년 대호 이다.\n",
    "> 그러면 이 대회의 딥러닝 트렌드를 살펴보겠다.\n",
    ">>### 8-2.1  이미지 넷\n",
    ">> 이미지 넷은 100만장이 넘는 이미지를 담고 잇는 데이터 셋이다. \n",
    ">> 그림과 같이 다양한 종류의 이미지를 포함하며 각 이미지에는 레이블이 붙어있다.\n",
    ">> 본 대회에는 시험 항목이 몇가지 있는 데 그중 하나가 분류이다.(classification)\n",
    ">> 분류 부문에서는 100개의 클래스를 제대로 분류하는지를 겨룬다.\n",
    ">> 그럼 최근 ILSVRC의 분류 시험 결과를 살펴보자.\n",
    ">> 그림 8-8은 2010년부터 최근까지 분류 부분 우승팀의 성적이다.\n",
    ">> 여기서 탑 -5 오류를 막대로 나타내었다.\n",
    ">> 톱-5 오류란 확률이 가장 높다고 생각하는 후보 클래스 5개 안에 정답이 포함되지 않은 즉 5개 모두가 틀린 비율이다.\n",
    ">> 주목할 점은 2012년 이후 선두는 항상 딥러닝을 사용하였다는 것이다.\n",
    ">> 특히 2015년 에는 150층이 넘는 심층 신경망인 ResNet이 오류율 3.5% 까지 낮췄다. \n",
    ">> 이결과는 인간을 넘어 섰다고 한다.\n",
    ">> 여기서 우린ㄴ VGG,GoogLeNetm,Resnet을 간단히 알아보고자 한다.\n",
    ">>### 8-2.2 VGG\n",
    ">> VGG는 합성곱 계층과 풀링 계층으로 구성된 기본적인 CNN이다. 다만 그림과 같이 비중있는 층을 모두 16층 혹은 19층으로 심화한게 특징이다.\n",
    ">> 여기서 주목할 점은 3x3 작은 필터를 사용한 합성곱 계층을 연속으로 거친다는 것이다. 글미에서 보듯 합성곱 계층을 2~4회 연속으로 풀링 계층을 두어\n",
    ">> 크기를 절반으로 줄이는 처리르 반복한다. 그리모 마지막에는 전결합 계층을 통과시켜 결과를 출력한다.\n",
    ">>### 8-2.3 GoogLeNet\n",
    ">> 본 모델의 구성은 다음과 같다. 그림의 사각형이 합성곱 계층과 풀링 계층등의 계층으로 나타난다.\n",
    ">> 그림을 보면 구성이 매우 복잡해 보이는데, 기본적으로 지금까지 보아온 CNN과 다르지 않다.\n",
    ">> 단, 본 모델은  세보 방향뿐만 아니라 가로방향도 깊다는 점이 특징이다.\n",
    ">> 본 모델에는 가로 방향에 폭이 있따. 이를 인셉션 구조라 하며, 그 기반 구조는 그림 8-11과 같다.\n",
    ">> 인셉션 구조는 그림과 같이 다른 필터를 여러개 적용하여 그 결과를 결합한다. \n",
    ">> 이 인셉션 구조를 하나의 빌딩 블록으로 사용하는것이 본 모델의 특징이다.\n",
    ">>### 8-2.4 ResNet\n",
    ">> 본 모델은 마이크로소프트의 팀이 개발한 네트워크 이다.\n",
    ">> 그 특징은 지금까지 보다 층이 더 깊게 할 수있는 특별한 장치에 있다.\n",
    ">> 지금까지 층을 깊게 하는 것이 성능 향상에 중요한다는 건 알고 있다.\n",
    ">>  그러나 딥러닝의 학습에서는 지나치게 깊으면 학습이 잘되지 않고 오히려 성능이 떨어지는 경우도 많다.\n",
    ">> ResNet에서는 그런 문제를 해결하기 위해 스킵 연결을 도입한다.\n",
    ">> 스킵 연결이란 [그림 8-12]와 같이 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더하는 구조를  말한다.\n",
    ">> 그림에서는 입력 x를 연속한 두 합성곱 계층을 건너뛰어 출력에 바로 연결한다. 이 단축 경로가 없다면 출력이 F(x)가 된다.\n",
    ">> 하지만 스킵 연결로 인해 F(x)+x가 되는 것이 핵심이다.\n",
    ">> 스킵 연결은 층이 깊어져도 학습을 효율적으로 할 수 있도록 하는데 이는 역전파 때 스킵 연결이 신호 감쇠를 막아주기 때문이다.\n",
    ">> ResNet은 먼저 설명한 VGG 신겨망을 기반으로 스킵 연결을 도입하여 층을 깊게 하였다.\n",
    ">> 그림과 같이 ResNet은 합성곱 계층을 2개 층 마다 건너뛰면서 층을 깊게 한다.\n",
    ">> 실험 결과 150층 이상으로 해도 정확도가 오르는 모습을 확인 할수있었고 3.5%라는 경이적인 결과를 냈다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
