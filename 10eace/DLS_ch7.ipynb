{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Convolutional Neural Network\n",
    ">  \n",
    "> ### **7.1** 전체 구조  \n",
    "> CNN도 지금까지 본 신경망과 같이 레고 블록처럼 계층을 조합하여 만들 수 있음  \n",
    ">  \n",
    "> convolutional layer와 pooling layer가 새롭게 등장  \n",
    ">  \n",
    "> 지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었음  \n",
    "> &nbsp; &nbsp; &rarr; 이를 fully-connected라고 하며, 이러한 계층을 Affine layer라는 이름으로 구현했음  \n",
    ">  \n",
    "> Affine layer를 사용하면 층이 5개인 fully-connected neural network는 다음과 같이 구현할 수 있음  \n",
    ">  \n",
    "> ![pic/affine_layer_network](pic/affine_layer_network.png)  \n",
    ">  \n",
    "> 위와 같이 fully-connected neural network는 Affine layer뒤에 activation function을 갖는 ReLU layer(혹은 Sigmoid layer)가 이어짐  \n",
    ">  \n",
    "> 위 그림에서는 Affine-ReLU 조합이 4개 쌓였고, 마지막 5번째 layer는 Affine layer에 이어 softmax layer에서 최종 결과(확률)를 출력  \n",
    ">  \n",
    "> ![pic/cnn_example](pic/cnn_example.png)  \n",
    ">  \n",
    "> 위와 같이 CNN에서는 Conv layer와 Polling layer가 추가됨  \n",
    ">  \n",
    "> CNN의 layer는 'Conv-ReLU-(Pooling)'흐름으로 연결됨 (Pooling layer는 생략하기도 함)  \n",
    ">  \n",
    "> 지금까지의 'Affine-ReLU'연결이 'Conv-ReLU-(Pooling)'으로 바뀌었다고 생각할 수 있음  \n",
    ">  \n",
    "> 위 그림의 CNN에서 주목할 또 다른 점은 출력에 가까운 layer에서는 지금까지의 'Affine-ReLU'구성을 사용할 수 있다는 것  \n",
    ">  \n",
    "> 마지막 출력 layer에서는 'Affine-Softmax'조합을 그대로 사용함  \n",
    ">  \n",
    "> 이것이 일반적인 CNN에서 흔히 볼 수 있는 구성  \n",
    "\n",
    "> ### **7.2** Convolutional layer\n",
    "> CNN에서는 padding, stride 등 CNN 고유의 용어가 등장  \n",
    ">  \n",
    "> 각 layer 사이에는 3차원 데이터같이 입체적인 데이터가 흐른다는 점에서 fully-connected network와 다름  \n",
    ">>  \n",
    ">> ##### fully-connected layer의 문제점\n",
    ">> 지금까지 본 fully-connected network에서는 fully-connected layer(Affine layer)를 사용했음  \n",
    ">>  \n",
    ">> fully-connected layer에서는 인접하는 layer의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있음  \n",
    ">>  \n",
    ">> fully-connected layer의 문제점은 무엇일까?  \n",
    ">> &nbsp; &nbsp; &rarr; '데이터의 형상이 무시'됨  \n",
    ">>  \n",
    ">> 입력 데이터가 이미지인 경우를 예로 들면, 이미지는 통상 세로 &middot; 가로 &middot; 채널로 구성된 3차원 데이터  \n",
    ">>  \n",
    ">> fully-connected layer에 입력할 때에는 3차원 데이터를 평평한 1차원 데이터로 평탄화해야 함  \n",
    ">>  \n",
    ">> 지금까지의 MNIST 데이터셋을 사용한 사례에서는 shape이 (1, 28, 28)인 이미지(1채널, 세로 28픽셀, 가로 28픽셀)를 1줄로 세운 784개의 데이터를 첫 Affine layer에 입력  \n",
    ">>  \n",
    ">> 이미지는 3차원 shape이며, 이 shape에는 중요한 공간적 정보가 담겨 있음  \n",
    ">> &nbsp; &nbsp; &rarr; 예를 들어 공간적으로 가까운 픽셀은 값이 비슷하거나, RGB의 각 채널은 서로 밀접하게 관련되어 있거나, 거리가 먼 픽셀끼리는 연관이 없는 등, 3차원 속에서 의미를 갖는 본질적인 패턴이 숨어 있을 것  \n",
    ">>  \n",
    ">> 그러나 fully-connected layer는 shape을 무시하고 모든 입력 데이터를 동등한 뉴런(같은 차원의 뉴런)으로 취급하여 shape에 담긴 정보를 살릴 수 없음  \n",
    ">>  \n",
    ">> Conv layer는 shape을 유지함  \n",
    ">>  \n",
    ">> 이미지도 3차원 데이터로 입력받으며, 다음 계층에도 3차원 데이터로 전달함  \n",
    ">> &nbsp; &nbsp; &rarr; CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 가능성이 있는 것  \n",
    ">>  \n",
    ">> CNN에서는 Conv layer의 입출력 데이터를 feature map이라고도 함  \n",
    ">>  \n",
    ">> Conv layer의 입력 데이터를 input feature map, 출력 데이터를 output feature map이라고 함  \n",
    ">>  \n",
    ">> 이 책에서는 '입출력 데이터'와 'feature map'을 같은 의미로 사용  \n",
    "\n",
    ">> ##### 합성곱 연산\n",
    ">> Conv layer에서는 합성곱 연산을 처리함  \n",
    ">>  \n",
    ">> 합성곱 연산은 이미지 처리에서 말하는 필터 연산에 해당  \n",
    ">>  \n",
    ">> ![pic/conv_cal_ex](pic/conv_cal_ex.png)  \n",
    ">>  \n",
    ">> 위 그림과 같이 합성곱 연산은 입력 데이터에 필터를 적용함  \n",
    ">>  \n",
    ">> 이 예에서 입력 데이터는 세로 &middot; 가로 방향의 형상을 가졌고, 필터 역시 세로 &middot; 가로 방향의 차원을 가짐  \n",
    ">>  \n",
    ">> 데이터와 필터의 shape을 (height, width)로 표기하며, 이 예에서 입력은 (4, 4), 필터는 (3, 3), 출력은 (2, 2)가 됨  \n",
    ">>  \n",
    ">> 문헌에 따라 필터를 커널이라 하기도 함  \n",
    ">>  \n",
    ">>  \n",
    ">>  \n",
    ">> 위 그림의 합성곱 연산  \n",
    ">> * 합성곱 연산은 필터의 윈도우를 일정 간격으로 이동해가며 입력 데이터에 적용  \n",
    ">>  \n",
    ">> * 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구함  \n",
    ">> &nbsp; &nbsp; &rarr; 이 계산을 fused multiply-add, FMA라고 함  \n",
    ">> * 연산 결과를 출력의 해당 장소에 저장  \n",
    ">> * 이 과정을 모든 장소에서 수행하면 합성곱 연산의 출력이 완성됨  \n",
    ">> \n",
    ">> ![pic/conv_cal_explain](pic/conv_cal_explain.png)  \n",
    ">>  \n",
    ">> fully-connected network에는 가중치 매개변수와 편향이 존재, CNN에서는 필터의 매개변수가 그동안의 '가중치'에 해당, 편향까지 포함하면 아래와 같이 표현할 수 있음  \n",
    ">> \n",
    ">> ![pic/cnn_bias](pic/cnn_bias.png)  \n",
    ">>  \n",
    ">> 위 그림과 같이 편향은 필터를 적용한 후의 데이터에 더해짐  \n",
    ">>  \n",
    ">> 편향은 항상 하나($1 \\times 1$)만 존재함  \n",
    ">> \n",
    ">> 그 하나의 값을 필터를 적용한 모든 원소에 더함\n",
    "\n",
    ">> ##### padding\n",
    ">> padding: 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(0 등)으로 채우는 일, 합성곱 연산에서 자주 이용하는 기법  \n",
    ">>  \n",
    ">> 아래 그림은 (4, 4) 크기의 입력 데이터에 폭이 1인 padding을 적용한 모습  \n",
    ">>  \n",
    ">> 폭 1짜리 padding은 입력 데이터 사방 1픽셀을 특정 값으로 채우는 것을 말함  \n",
    ">> \n",
    ">> ![pic/padding_ex](pic/padding_ex.png)  \n",
    ">>  \n",
    ">> 위 그림과 같이 처음에 크기가 (4, 4)인 입력 데이터에 padding이 추가되어 (6, 6)이 됨  \n",
    ">> \n",
    ">> 이 입력에 (3, 3) 크기의 필터를 걸면 (4, 4) 크기의 출력 데이터가 생성  \n",
    ">>  \n",
    ">> 이 예에서는 padding을 1로 설정했지만, 2나 3 등 원하는 정수로 설정할 수 있음  \n",
    ">> \n",
    ">> 만약 그림 7-5에 패딩을 2로 설정하면 입력 데이터의 크기는 (8, 8)이 되고, 3으로 설정하면 (10, 10)이 됨  \n",
    ">>  \n",
    ">> padding은 주로 출력 크기를 조정할 목적으로 사용함  \n",
    ">> \n",
    ">> 예를 들어 (4, 4) 입력 데이터에 (3, 3) 필터를 적용하면 출력은 (2, 2)가 되어 입력보다 2만큼 줄어듦  \n",
    ">> &nbsp; &nbsp; &rarr; 이는 합성곱 연산을 몇 번이나 되풀이하는 심층 신경망에서는 문제가 될 수 있음  \n",
    ">> &nbsp; &nbsp; &rarr; 합성곱 연산을 거칠 때마다 크기가 작아지면 어느 시점에서는 출력 크기가 1이 되어버림  \n",
    ">> &nbsp; &nbsp; &rarr; 더 이상은 합성곱 연산을 적용할 수 없음  \n",
    ">>  \n",
    ">> 이러한 사태를 막기 위해 padding을 사용  \n",
    ">> &nbsp; &nbsp; &rarr; 입력 데이터의 공간적 크기를 고정한 채로 다음 layer에 전달할 수 있음  \n",
    "\n",
    ">> ##### stride\n",
    ">> stride: 필터를 적용하는 위치의 간격  \n",
    ">> \n",
    ">> 지금까지 본 예는 모두 stride가 1이었지만, 예를 들어 stride를 2로 하면 필터를 적용하는 윈도우가 두 칸씩 이동함  \n",
    ">> \n",
    ">> ![pic/stride_2_ex](pic/stride_2_ex.png)  \n",
    ">> \n",
    ">> 위 그림에서는 크기가 (7, 7)인 입력 데이터에 stride를 2로 설정한 필터를 적용함  \n",
    ">> \n",
    ">> 이처럼 stride는 필터를 적용하는 간격을 지정함  \n",
    ">> \n",
    ">> stride를 2로 하니 출력은 (3, 3)이 됨  \n",
    ">> &nbsp; &nbsp; &rarr; 이처럼 stride를 키우면 출력 크기는 작아짐\n",
    ">> \n",
    ">> 한편, padding을 크게 하면 출력 크기가 커짐  \n",
    ">> \n",
    ">> 이러한 관계를 수식화하면 다음과 같음\n",
    ">> \n",
    ">> * $ OH = \\frac{H+2P-FH}{S} + 1 $\n",
    ">> \n",
    ">> * $ OW = \\frac{W+wP-FW}{S} + 1 $ &nbsp; &nbsp; &nbsp; [식 7.1]\n",
    ">>      * H, W: 입력 크기 (height, width)\n",
    ">>      * FH, FW: 필터 크기 (Filter Height, Filter Width)\n",
    ">>      * OH, OW: 출력 크기 (Output Height, Output Width)\n",
    ">>      * P: padding\n",
    ">>      * S: stride\n",
    ">>  \n",
    ">> 연습\n",
    ">> * 예1: 그림 7-6  \n",
    ">> &nbsp; &nbsp; &nbsp; &nbsp; 입력: (4,4), padding: 1, stride: 1, 필터: (3,3)  \n",
    ">> $$\n",
    ">> OH = \\frac{4+2 \\cdot 1-3}{1} + 1 = 4\n",
    ">> $$\n",
    ">> $$\n",
    ">> OW = \\frac{4+2 \\cdot 1-3}{1} + 1 = 4\n",
    ">> $$\n",
    ">>  \n",
    ">> * 예2: 그림 7-7  \n",
    ">> &nbsp; &nbsp; &nbsp; &nbsp; 입력: (7,7), padding: 0, stride: 2, 필터: (3,3)  \n",
    ">> $$\n",
    ">> OH = \\frac{7+2 \\cdot 0-3}{2} + 1 = 3\n",
    ">> $$\n",
    ">> $$\n",
    ">> OW = \\frac{7+2 \\cdot 0-3}{2} + 1 = 3\n",
    ">> $$\n",
    ">> \n",
    ">> * 예3  \n",
    ">> &nbsp; &nbsp; &nbsp; &nbsp; 입력: (28,31), padding: 2, stride: 3, 필터: (5,5)  \n",
    ">> $$\n",
    ">> OH = \\frac{28+2 \\cdot 2-5}{3} + 1 = 10\n",
    ">> $$\n",
    ">> $$\n",
    ">> OW = \\frac{31+2 \\cdot 2-5}{3} + 1 = 11\n",
    ">> $$\n",
    ">>  \n",
    ">> 위의 예에서처럼 식 7.1에 단순히 값을 대입하기만 하면 출력 크기를 구할 수 있음  \n",
    ">> \n",
    ">> 단 식 7.1의 $ \\frac{H+2P-FH}{S} $와 $ \\frac{W+wP-FW}{S} $가 정수로 나누어 떨어지는 값이어야 한다는 점에 주의  \n",
    ">> \n",
    ">> 출력 크기가 정수가 아니면 오류를 내는 등의 대응을 해주어야 함  \n",
    ">> \n",
    ">> 딥러닝 프레임워크 중에는 값이 나누어 떨어지지 않을 때는 가장 가까운 정수로 반올림하는 등, 특별히 에러를 내지 않고 진행하도록 구현하는 경우도 있음  \n",
    "\n",
    ">> ##### 3차원 데이터의 합성곱 연산\n",
    ">> 지금까지 2차원 shape을 다루는 합성곱 연산을 살펴봄  \n",
    ">> \n",
    ">> 그러나 이미지만 해도 height &middot; width에 더해 채널까지 고려한 3차원 데이터임  \n",
    ">> \n",
    ">> 지금까지 살펴본 순서대로 채널까지 고려한 3차원 데이터를 다루는 합성곱 연산을 살펴볼 예정  \n",
    ">> \n",
    ">> ![pic/3dim_conv_ex](pic/3dim_conv_ex.png)  \n",
    ">> \n",
    ">> ![pic/order_3dim_conv](pic/order_3dim_conv.png)  \n",
    ">> \n",
    ">> 위 그림은 3차원 데이터의 합성곱 연산 예  \n",
    ">> \n",
    ">> 2차원일 때(그림 7-3)와 비교하면, 길이 방향(채널 방향)으로 feature map이 늘어남  \n",
    ">> \n",
    ">> 채널 방향으로 feature map이 여러 개 있다면 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 그 결과를 더해서 하나의 출력을 얻음\n",
    ">>  \n",
    ">> 3차원 합성곱 연산에서 주의할 점은 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다는 것  \n",
    ">> \n",
    ">> 필터 자체의 크기는 원하는 값으로 설정할 수 있음 (단, 모든 채널의 필터가 같은 크기여야 함)  \n",
    "\n",
    ">> ##### 블록으로 생각하기\n",
    ">> 3차원 합성곱 연산은 데이터와 필터를 직육면체 블록이라고 생각하면 쉬움  \n",
    ">> \n",
    ">> 블록은 3차원 직육면체임  \n",
    ">> \n",
    ">> 3차원 데이터를 다차원 배열로 나타낼 때는 (channel, height, width)로 표현  \n",
    ">> \n",
    ">> ![pic/conv_block](pic/conv_block.png)  \n",
    ">> \n",
    ">> 위 그림에서 출력 데이터는 한 장의 feature map  \n",
    ">> &nbsp; &nbsp; &rarr; 한 장의 feature map = 채널이 1개인 feature map  \n",
    ">> \n",
    ">> 합성곱 연산의 출력으로 다수의 채널을 내보내려면 어떻게 해야 할까?  \n",
    ">> &nbsp; &nbsp; &rarr; 필터(가중치)를 여러 개 사용  \n",
    ">> \n",
    ">> 그림으로 나타내면 아래와 같음  \n",
    ">>  \n",
    ">> ![pic/conv_many_filter](pic/conv_many_filter.png)  \n",
    ">> \n",
    ">> 위 그림과 같이 필터를 FN개 적용하면 출력 feature map도 FN개가 생성  \n",
    ">> \n",
    ">> 그리고 그 FN개의 맵을 모으면 shape이 (FN, OH, OW)인 블록이 완성  \n",
    ">>  \n",
    ">> 이 완성된 블록을 다음 계층으로 넘기는 것이 CNN의 처리 흐름  \n",
    ">>  \n",
    ">> 살펴본 것처럼 합성곱 연산에서는 필터의 수도 고려해야 함  \n",
    ">> \n",
    ">> 그렇기 때문에 필터의 가중치 데이터는 4차원 데이터이며 (output channel, input channel, height, width)로 나타냄  \n",
    ">> &nbsp; &nbsp; &rarr; ex) 채널 수 $3$, 크기 $5 \\times 5$인 필터가 $20$개 있다면 (20, 3, 5, 5)로 나타냄  \n",
    ">> \n",
    ">> 합성곱 연산에도 편향이 쓰임  \n",
    ">> \n",
    ">> 이를 그림으로 나타내면, \n",
    ">> \n",
    ">> ![pic/conv_cal_flow](pic/conv_cal_flow.png)  \n",
    ">> \n",
    ">> 위 그림에서 보듯 편향은 채널 하나에 값 하나씩으로 구성됨  \n",
    ">> \n",
    ">> 이 예에서는 편향의 shape은 (FN, 1, 1)이고, 필터의 출력 결과의 형사은 (FN, OH, OW)임  \n",
    ">> \n",
    ">> 이 두 블록을 더하면 편향의 각 값이 필터의 출력인 (FN, OH, OW) 블록의 대응 채널의 원소 모두에 더해짐  \n",
    "\n",
    ">> ##### 배치 처리\n",
    ">> 신경망 처리에서는 입력 데이터를 한 덩어리로 묶어 배치로 처리했음  \n",
    ">> \n",
    ">> fully-connected network를 구현하면서는 이 방식을 지원하여 처리 효율을 높이고, 미니배치 방식의 학습도 지원하도록 했음  \n",
    ">> \n",
    ">> 합성곱 연산도 마찬가지로 배치 처리를 지원  \n",
    ">> &nbsp; &nbsp; &rarr; 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장함  \n",
    ">> \n",
    ">> 구체적으로는 데이터를 (number of data, channel, height, width)로 저장  \n",
    ">> \n",
    ">> 데이터가 N개일 때 배치 처리한다면 데이터 형태가 다음과 같이 구성  \n",
    ">> \n",
    ">> ![pic/conv_cal_batch](pic/conv_cal_batch.png)  \n",
    ">> \n",
    ">> 배치 처리 시의 데이터 흐름을 나타낸 위 그림을 보면 각 데이터의 선두에 배치용 차원을 추가함  \n",
    ">> \n",
    ">> 이처럼 데이터는 4차원 shape을 가진 채 각 layer를 타고 흐름  \n",
    ">> \n",
    ">> 여기서 주의할 점으로는 신경망에 4차원 데이터가 하나 흐르 때마다 데이터 N개에 대한 합성곱 연산이 이뤄진다는 것  \n",
    ">> \n",
    ">> 즉, N회 분의 처리를 한 번에 수행하는 것\n",
    "\n",
    "> ### **7.3** Pooling layer\n",
    ">> \n",
    ">> pooling: hegiht &middot; width 방향의 공간을 줄이는 연산  \n",
    ">> \n",
    ">> ![pic/max_pooling_flow](pic/max_pooling_flow.png)  \n",
    ">> \n",
    ">> 위 그림과 같이 $2 \\times 2$영역을 원소 하나로 집약하여 공간 크기를 줄임  \n",
    ">>  \n",
    ">> 위 그림은 $ 2 \\times 2$ max pooling을 stride 2로 처리하는 순서\n",
    ">> \n",
    ">> max pooling: 최댓값을 구하는 연산  \n",
    ">> \n",
    ">> '$2 \\times 2$'는 대상 영역의 크기를 뜻함  \n",
    ">> \n",
    ">> 즉 $2 \\times 2$ max pooling은 위 그림과 같이 $2 \\times 2$ 크기의 영역에서 가장 큰 원소 하나를 꺼냄  \n",
    ">> \n",
    ">> stride는 이 예에서는 2로 설정했으므로 $2 \\times 2$ 윈도우가 원소 2칸 간격으로 이동  \n",
    ">> \n",
    ">> 참고로, pooling의 윈도우 크기와 stride는 같은 값으로 설정하는 것이 보통  \n",
    ">> &nbsp; &nbsp; &rarr; ex) 윈도우가 $3 \\times 3$ 이면 stride는 3으로, 윈도우가 $4 \\times 4$이면 stride를 4 설정  \n",
    ">>  \n",
    ">> pooling은 max pooling 외에도 average pooling 등이 있음  \n",
    ">> \n",
    ">> max pooling은 대상 영역에서 최댓값을 취하는 연산인 반면, average pooling은 대상 영역의 평균을 계산함  \n",
    ">> \n",
    ">> 이미지 인식 분야에서는 주로 max pooling을 사용함  \n",
    ">> &nbsp; &nbsp; &rarr; 이 책에서 pooling layer라고 하면 max pooling을 말하는 것  \n",
    ">> \n",
    ">> ##### pooling layer의 특징\n",
    ">> * 학습해야 할 매개변수가 없음  \n",
    ">> &nbsp; &nbsp; pooling layer는 Conv layer와 달리 학습해야 할 매개변수가 없음  \n",
    ">> &nbsp; &nbsp; pooling은 대상 영역에서 최댓값이나 평균을 취하는 명확한 처리이므로 특별히 학습할 것이 없음  \n",
    ">> \n",
    ">> * 채널 수가 변하지 않음  \n",
    ">> &nbsp; &nbsp; pooling 연산은 입력 데이터의 채널 수 그대로 출력 데이터로 내보냄  \n",
    ">> &nbsp; &nbsp; pooling 연산은 아래 그림처럼 채널마다 독립적으로 계산하기 때문  \n",
    ">>\n",
    ">> ![pic/pooling_feature](pic/pooling_feature.png)  \n",
    ">> \n",
    ">> * 입력의 변화에 영향을 적게 받음 (강건함)  \n",
    ">> &nbsp; &nbsp; 입력 데이터가 조금 변해도 pooling의 결과는 잘 변하지 않음  \n",
    ">> &nbsp; &nbsp; 예를 들어 다음 그림은 입력 데이터의 차이(데이터가 오른쪽으로 1칸씩 이동)를 pooling이 흡수해 사라지게 하는 모습을 보여줌  \n",
    ">> \n",
    ">> ![pic/pooling_feature_2](pic/pooling_feature_2.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **7.4** Conv/Pooling layer 구현하기  \n",
    ">>  \n",
    ">> ##### 4차원 배열  \n",
    ">> CNN에서 layer 사이를 흐르는 데이터는 4차원 데이터임  \n",
    ">>  \n",
    ">> 예를 들어 데이터의 shape이 (10, 1, 28, 28)이라면, 이는 height 28, width 28, 채널이 1개인 데이터가 10개인 데이터  \n",
    ">> \n",
    ">> 이를 파이썬으로 구현하면 다음과 같음  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10, 1, 28, 28)   # 무작위로 데이터 생성\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> 위 구현에서 첫 번째 데이터에 접근하려면 단순히 `x[0]`로 접근, 마찬가지로 두 번째 데이터는 `x[1]`위치에 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x[0].shape)\n",
    "print(x[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> 첫 번째 데이터의 첫 채널의 공간 데이터에 접근하려면 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.63654930e-01, 2.70133878e-01, 5.90513631e-01, 1.33889394e-01,\n",
       "        6.64864921e-01, 3.10319697e-02, 8.08453449e-01, 1.64234921e-01,\n",
       "        9.55767513e-01, 9.88709104e-01, 1.47094740e-01, 1.68825915e-01,\n",
       "        3.78001754e-01, 6.90078879e-01, 7.65159579e-01, 1.33405168e-01,\n",
       "        9.59389183e-01, 1.07629077e-01, 5.26163258e-01, 1.20124731e-01,\n",
       "        5.25486419e-01, 1.98785515e-01, 2.51981004e-01, 1.63028671e-01,\n",
       "        7.82706250e-01, 2.24891233e-01, 1.17122457e-01, 7.12211826e-01],\n",
       "       [4.03633082e-01, 8.31191712e-01, 9.75136653e-01, 6.11191461e-01,\n",
       "        7.77244066e-01, 2.46218227e-01, 8.91685996e-01, 9.56385741e-01,\n",
       "        1.95838934e-01, 6.25321943e-01, 2.36697700e-01, 2.27950900e-01,\n",
       "        7.86215455e-01, 2.84020518e-01, 7.82213755e-01, 4.26332978e-01,\n",
       "        4.97740524e-01, 3.39565178e-01, 8.46438964e-01, 8.55048275e-01,\n",
       "        1.03897374e-03, 7.95177931e-01, 8.12788332e-01, 5.69023300e-01,\n",
       "        5.37043180e-01, 3.69549232e-01, 7.02434371e-01, 5.13798369e-01],\n",
       "       [3.92190043e-01, 8.81135731e-01, 1.71658607e-02, 2.77534796e-01,\n",
       "        4.74904173e-02, 9.91977559e-01, 2.94718166e-01, 7.79276357e-01,\n",
       "        1.74317901e-01, 3.04192679e-01, 4.04555034e-01, 5.78092796e-01,\n",
       "        8.59874398e-01, 1.45129088e-01, 3.54132639e-02, 4.70701816e-01,\n",
       "        3.00738822e-02, 9.90354056e-01, 9.93109962e-01, 8.95524915e-01,\n",
       "        9.48643407e-01, 1.60554398e-01, 1.84171704e-02, 2.61862107e-01,\n",
       "        2.76567119e-01, 9.48778678e-01, 8.51320472e-02, 5.21024808e-01],\n",
       "       [2.97069909e-01, 4.44221039e-01, 3.97294573e-01, 5.30922566e-01,\n",
       "        5.48427593e-01, 8.62701638e-01, 7.91393019e-01, 7.08483276e-01,\n",
       "        8.98550060e-01, 5.80663457e-01, 1.75454502e-01, 1.21012676e-01,\n",
       "        1.57269233e-01, 9.25675701e-01, 2.46546678e-01, 4.99383891e-01,\n",
       "        1.83675177e-01, 6.00549686e-01, 6.53534947e-01, 6.43485203e-01,\n",
       "        8.65094894e-01, 4.13403152e-01, 7.59489952e-01, 5.61802041e-01,\n",
       "        7.93779936e-01, 7.62455942e-01, 6.64239964e-01, 4.67430286e-01],\n",
       "       [6.31354442e-01, 1.96861155e-01, 5.14490763e-01, 6.51769862e-01,\n",
       "        3.63019451e-01, 3.91551418e-01, 6.03912804e-01, 2.42516459e-01,\n",
       "        9.88734110e-01, 1.20358721e-01, 7.04606321e-02, 9.46818239e-01,\n",
       "        4.59196592e-02, 1.21610356e-01, 4.94926877e-01, 6.97030721e-01,\n",
       "        3.94025629e-01, 9.79407254e-01, 8.43076388e-01, 5.62271779e-04,\n",
       "        7.67199597e-01, 1.06253976e-01, 3.48511263e-01, 8.84244608e-01,\n",
       "        7.14190799e-01, 3.43354661e-01, 8.87565871e-01, 4.92964810e-01],\n",
       "       [4.42740971e-01, 6.51364867e-01, 7.19519048e-02, 2.51690129e-01,\n",
       "        5.96131416e-02, 9.83647980e-01, 1.00382961e-02, 4.80308978e-01,\n",
       "        6.17135119e-01, 5.59488844e-01, 9.05135617e-01, 4.43932231e-01,\n",
       "        6.04629291e-01, 9.46612905e-01, 9.19757853e-01, 3.03895160e-01,\n",
       "        9.72344683e-01, 2.66403462e-01, 2.11567303e-01, 4.06064788e-02,\n",
       "        2.72626953e-01, 6.06132122e-01, 1.28058346e-01, 6.54004393e-01,\n",
       "        3.68244456e-01, 8.57780365e-02, 2.68956138e-01, 9.14764767e-01],\n",
       "       [3.34406084e-01, 9.61128766e-01, 8.30553198e-01, 7.82877869e-01,\n",
       "        6.02186087e-01, 8.01837549e-01, 3.32921875e-01, 4.85136992e-01,\n",
       "        7.07725110e-01, 1.61805497e-01, 2.25496647e-01, 7.33123395e-01,\n",
       "        6.36047671e-01, 9.62207589e-01, 3.75825392e-01, 1.68712194e-01,\n",
       "        8.47101434e-02, 3.93184625e-01, 9.87775532e-01, 3.26381445e-01,\n",
       "        8.02355532e-01, 1.82876193e-01, 2.34946572e-01, 2.88447954e-01,\n",
       "        4.40817742e-01, 5.90987036e-01, 3.09886978e-01, 2.50041805e-01],\n",
       "       [7.79433500e-01, 2.20804996e-01, 1.56505816e-01, 5.63145488e-01,\n",
       "        3.09435339e-02, 3.51107920e-02, 5.11190186e-01, 5.09429067e-02,\n",
       "        1.24251741e-01, 8.33797187e-01, 2.84487306e-01, 2.82615833e-01,\n",
       "        5.59038140e-01, 1.19202712e-02, 4.19139197e-01, 3.75176961e-01,\n",
       "        8.69947741e-01, 6.23685441e-01, 3.75104878e-01, 4.56971587e-01,\n",
       "        6.29276387e-01, 9.52989436e-01, 5.13614104e-01, 9.55804337e-01,\n",
       "        9.57296450e-01, 2.22930986e-02, 6.75526332e-01, 3.34655656e-01],\n",
       "       [6.28535320e-01, 4.91155398e-01, 3.66138599e-01, 4.67312190e-01,\n",
       "        3.52633268e-01, 8.10143822e-01, 9.62615006e-01, 5.00526946e-01,\n",
       "        4.35864943e-01, 8.84788615e-01, 3.36246726e-01, 9.38284751e-01,\n",
       "        8.60923665e-01, 9.73957058e-01, 3.36211268e-01, 5.15302030e-01,\n",
       "        5.70672015e-01, 8.37633846e-01, 5.72159946e-01, 3.91384836e-01,\n",
       "        4.81328312e-01, 4.91042352e-01, 6.18936361e-02, 2.65521436e-01,\n",
       "        5.22102698e-01, 4.10325255e-01, 6.90750002e-01, 5.36716751e-01],\n",
       "       [1.06652022e-02, 6.87295238e-01, 7.11167449e-01, 1.95447215e-01,\n",
       "        6.41245678e-01, 7.37622172e-01, 8.88334789e-01, 6.28095369e-01,\n",
       "        4.40003320e-01, 1.42144971e-01, 2.48331843e-01, 8.10272421e-01,\n",
       "        6.10135614e-01, 7.53888939e-01, 1.54053430e-01, 6.96760060e-01,\n",
       "        4.74743507e-01, 9.85920930e-01, 2.28412400e-02, 7.78670244e-01,\n",
       "        3.35821077e-01, 6.18159740e-01, 4.61806389e-01, 5.02201132e-01,\n",
       "        6.78955395e-01, 6.64531435e-02, 3.66214965e-01, 8.73386270e-01],\n",
       "       [9.31688172e-01, 6.84594481e-01, 4.25648742e-01, 7.45525338e-01,\n",
       "        9.42222810e-02, 6.70796851e-02, 4.74243549e-01, 5.89622035e-01,\n",
       "        9.89071435e-01, 4.95954721e-01, 8.16331604e-01, 3.08171184e-01,\n",
       "        4.26814506e-02, 2.60231154e-01, 9.91995089e-01, 5.20535842e-01,\n",
       "        8.11457709e-01, 8.63719391e-01, 6.02217018e-01, 7.37907498e-01,\n",
       "        2.78470706e-01, 1.24974358e-01, 1.85647382e-01, 6.33099120e-01,\n",
       "        2.57888218e-01, 5.49668390e-01, 6.52407287e-01, 8.53232845e-01],\n",
       "       [5.81461781e-01, 2.03999558e-01, 6.15880204e-01, 9.94475709e-01,\n",
       "        8.60003704e-01, 8.36856329e-01, 3.51668654e-01, 7.96633670e-01,\n",
       "        4.07358008e-01, 6.24934845e-01, 4.49348627e-01, 2.35520897e-01,\n",
       "        6.66171229e-01, 7.69268638e-01, 7.26060382e-01, 2.34250653e-01,\n",
       "        8.00822021e-01, 9.40143050e-01, 6.38500837e-01, 7.71654407e-01,\n",
       "        7.19164006e-01, 5.33460937e-01, 7.70987350e-01, 9.57068823e-02,\n",
       "        6.48359349e-02, 3.16068482e-01, 3.04712928e-01, 7.10711472e-01],\n",
       "       [2.65998496e-01, 6.24086942e-01, 2.79324988e-02, 8.60539186e-01,\n",
       "        8.97780704e-02, 9.57794817e-02, 9.64114920e-01, 6.94847867e-01,\n",
       "        7.15840739e-01, 5.36577550e-01, 2.90367222e-01, 9.11791698e-01,\n",
       "        8.00208023e-01, 1.56424447e-02, 2.76623427e-01, 9.48098856e-01,\n",
       "        4.93973071e-01, 8.45444597e-01, 9.62960535e-01, 1.42762368e-01,\n",
       "        6.11189889e-01, 3.25812224e-01, 1.96333878e-02, 4.35236387e-01,\n",
       "        2.92019320e-01, 2.79653625e-02, 1.45664971e-01, 2.84204943e-01],\n",
       "       [2.58021751e-01, 9.63492096e-01, 4.82218378e-01, 7.85749846e-01,\n",
       "        7.98230162e-01, 3.54937448e-01, 8.69192666e-01, 9.39174959e-01,\n",
       "        2.70604520e-02, 4.82082267e-03, 1.04533391e-02, 4.90689331e-01,\n",
       "        7.32003675e-01, 9.09594205e-01, 3.27589407e-01, 2.03639979e-01,\n",
       "        1.38871714e-01, 8.05091315e-01, 1.45528886e-01, 6.24354488e-01,\n",
       "        4.30782438e-01, 8.85070132e-01, 8.82953806e-01, 6.35004655e-01,\n",
       "        5.92973678e-01, 4.27946383e-01, 8.38888162e-01, 5.65237121e-01],\n",
       "       [2.38335233e-01, 8.70941890e-01, 2.64284328e-01, 2.39856232e-01,\n",
       "        1.56396116e-02, 6.98217540e-01, 8.29462660e-02, 2.95739434e-01,\n",
       "        5.36947459e-01, 3.94881913e-01, 9.30646639e-01, 4.27703675e-01,\n",
       "        1.31304588e-01, 4.61949229e-01, 3.60507219e-01, 6.09947321e-01,\n",
       "        5.47877408e-01, 5.71811719e-01, 4.45181363e-02, 8.48955672e-02,\n",
       "        1.13480622e-01, 6.17856983e-01, 5.67224687e-01, 2.55276988e-01,\n",
       "        8.35565948e-01, 6.51184230e-01, 2.43442629e-01, 3.23836343e-01],\n",
       "       [9.59259329e-01, 8.13272402e-01, 7.89149794e-01, 9.73704995e-01,\n",
       "        7.65775720e-01, 3.50874353e-01, 7.11602824e-01, 1.23767006e-01,\n",
       "        3.41461526e-02, 9.52222861e-01, 4.61393272e-01, 4.15873936e-01,\n",
       "        6.26651460e-01, 8.44935578e-01, 6.18613362e-01, 1.85710647e-01,\n",
       "        1.68191749e-01, 1.92474914e-01, 2.14124093e-01, 8.96027141e-01,\n",
       "        1.28809173e-01, 4.40563580e-01, 8.83498654e-01, 2.84326094e-02,\n",
       "        7.09542762e-01, 1.72099802e-01, 8.72957214e-01, 5.85130132e-01],\n",
       "       [1.94517386e-01, 2.72190309e-01, 9.35116923e-01, 3.76166095e-02,\n",
       "        5.62056733e-01, 5.67061896e-01, 8.83619618e-01, 4.49374904e-01,\n",
       "        2.11615222e-01, 9.40583070e-01, 1.74954298e-01, 7.59501793e-01,\n",
       "        3.47166830e-01, 2.80741468e-01, 6.86929093e-01, 6.56153303e-01,\n",
       "        7.52844342e-01, 3.95498539e-01, 1.07527132e-01, 4.29280715e-01,\n",
       "        6.36484047e-01, 6.00534874e-01, 2.45973218e-01, 8.06804238e-01,\n",
       "        8.59535676e-01, 2.80818308e-01, 3.66905266e-01, 8.57205941e-01],\n",
       "       [4.84359812e-01, 8.94308400e-01, 1.14873846e-01, 7.51558138e-01,\n",
       "        4.07865697e-01, 6.61274301e-01, 5.39716689e-01, 9.62673255e-01,\n",
       "        2.09171763e-01, 8.74416067e-01, 6.78318201e-02, 9.16248001e-01,\n",
       "        4.48078061e-01, 8.25416596e-01, 9.86367287e-01, 2.31691498e-01,\n",
       "        6.84342513e-01, 4.27574640e-02, 4.60239893e-01, 4.88288998e-01,\n",
       "        5.29774711e-01, 8.07036572e-01, 7.36461445e-01, 1.32648688e-01,\n",
       "        4.75654186e-01, 3.12725197e-01, 7.72634168e-01, 8.48308676e-01],\n",
       "       [4.75103719e-01, 2.39612179e-01, 1.36698254e-01, 1.09466668e-01,\n",
       "        4.93754452e-01, 6.26312135e-01, 4.57390941e-02, 9.15461584e-01,\n",
       "        3.78233634e-01, 7.95398075e-01, 9.89904903e-01, 4.45345280e-01,\n",
       "        6.06206237e-01, 1.02629811e-01, 4.00860983e-01, 7.87871567e-01,\n",
       "        4.97681758e-01, 2.73979365e-01, 5.05885929e-01, 6.77309343e-02,\n",
       "        9.75418267e-01, 9.30306252e-01, 7.36468158e-01, 1.28015105e-01,\n",
       "        3.61703859e-01, 3.64206109e-01, 2.31684257e-02, 1.95163268e-01],\n",
       "       [7.24216091e-01, 2.19958511e-01, 6.15097149e-01, 1.78119359e-01,\n",
       "        3.28796392e-01, 1.72008435e-02, 4.56728442e-01, 6.81453036e-01,\n",
       "        5.86990009e-01, 5.01900593e-01, 9.66711487e-01, 4.97440786e-01,\n",
       "        4.96564824e-01, 1.46471260e-01, 5.93447690e-01, 1.42247661e-01,\n",
       "        3.95002355e-01, 1.26544488e-01, 9.05238923e-01, 2.78390816e-01,\n",
       "        1.81547022e-01, 1.08108910e-01, 9.87345358e-01, 6.94606889e-01,\n",
       "        7.14942586e-01, 3.15968723e-01, 6.18468964e-01, 2.41610944e-01],\n",
       "       [5.68179488e-01, 2.68508152e-01, 8.88495792e-01, 5.92769443e-01,\n",
       "        1.16075910e-01, 7.90157734e-01, 8.27280412e-01, 1.40450510e-01,\n",
       "        2.79495425e-03, 7.46896797e-01, 6.66241971e-01, 9.89487849e-01,\n",
       "        1.65867200e-01, 9.42393800e-01, 1.76137724e-01, 7.73115790e-01,\n",
       "        5.42230809e-01, 7.76845110e-01, 3.89101752e-01, 9.36095148e-01,\n",
       "        8.76803912e-01, 9.28159554e-01, 7.48086004e-01, 8.60589527e-01,\n",
       "        4.44965904e-01, 9.27796999e-01, 4.83050569e-02, 3.27887406e-01],\n",
       "       [5.14476983e-01, 1.68021800e-01, 3.10074963e-02, 2.84140534e-01,\n",
       "        9.76224625e-01, 4.33781593e-01, 1.28686313e-01, 4.91391852e-01,\n",
       "        9.87773770e-01, 5.39885287e-01, 9.44257707e-01, 8.44810644e-01,\n",
       "        1.78013913e-01, 6.52588466e-01, 4.69948936e-01, 9.66631866e-02,\n",
       "        1.88584106e-02, 9.93374383e-02, 6.05436359e-01, 7.51422348e-01,\n",
       "        2.44156572e-01, 8.53941162e-01, 7.86923325e-01, 9.35253564e-01,\n",
       "        5.12720310e-01, 9.18508849e-01, 3.21182978e-01, 7.75770143e-01],\n",
       "       [1.35489578e-01, 8.14967021e-01, 1.03071475e-01, 2.44693176e-01,\n",
       "        2.73280836e-01, 6.18887299e-01, 7.67477127e-01, 2.66860517e-01,\n",
       "        7.16146477e-01, 2.04075749e-01, 5.29701265e-01, 1.35642055e-01,\n",
       "        5.49932549e-01, 4.60064865e-01, 8.64293349e-01, 9.90255659e-02,\n",
       "        2.68441874e-01, 8.07565403e-01, 4.39688358e-01, 4.63950284e-01,\n",
       "        9.43666307e-01, 9.49161296e-01, 9.20202875e-01, 2.08540087e-01,\n",
       "        5.42512297e-01, 9.69830817e-01, 5.62051956e-01, 6.99442871e-01],\n",
       "       [9.64942096e-01, 5.54196548e-01, 7.32998210e-01, 9.34289550e-01,\n",
       "        4.83817988e-01, 6.29219121e-01, 4.33966625e-01, 7.51273115e-01,\n",
       "        6.37702761e-01, 9.46612537e-01, 3.20681138e-01, 6.11113532e-01,\n",
       "        9.82561459e-01, 3.31232849e-01, 6.78333053e-01, 7.10251296e-01,\n",
       "        2.76547930e-01, 7.84377984e-01, 7.78999174e-01, 2.75268928e-01,\n",
       "        1.14042456e-01, 9.17321726e-02, 1.95163697e-01, 8.76184468e-01,\n",
       "        3.05002970e-01, 2.83043561e-01, 2.55753891e-02, 1.62084712e-01],\n",
       "       [6.73895296e-01, 5.70834588e-01, 4.89540351e-01, 8.98550292e-01,\n",
       "        9.75084038e-01, 8.05983552e-01, 2.96477991e-01, 9.55089712e-01,\n",
       "        8.80256096e-01, 2.27061253e-01, 6.47613358e-01, 5.92492789e-01,\n",
       "        7.60991033e-01, 9.11152855e-01, 9.12410924e-01, 1.10876918e-01,\n",
       "        6.85187667e-01, 2.67877352e-02, 6.33275002e-01, 8.57159942e-01,\n",
       "        9.90624506e-01, 9.06982127e-01, 8.00752282e-01, 1.40083904e-01,\n",
       "        6.81641186e-01, 8.75888056e-01, 2.77847984e-01, 1.39205838e-01],\n",
       "       [7.12174261e-01, 8.87978255e-01, 1.58496323e-01, 3.05535751e-01,\n",
       "        1.47553118e-01, 2.40894395e-01, 1.95034601e-01, 8.55218997e-01,\n",
       "        9.61455249e-02, 6.54828599e-01, 5.67684093e-01, 7.49905020e-01,\n",
       "        5.01135677e-01, 5.45970769e-01, 2.07696247e-01, 3.41520399e-01,\n",
       "        4.50301466e-01, 3.32123362e-01, 5.73934684e-01, 3.51677497e-01,\n",
       "        6.68468822e-01, 1.16983447e-01, 3.82446121e-01, 8.10791194e-01,\n",
       "        4.73156383e-02, 4.11102749e-01, 9.20818097e-01, 1.34603046e-01],\n",
       "       [3.59458313e-01, 4.47820146e-01, 1.54585463e-01, 4.17506266e-01,\n",
       "        7.59407297e-01, 1.04771155e-01, 6.71720487e-01, 8.55320890e-01,\n",
       "        3.65840051e-02, 9.23214807e-01, 4.76234843e-01, 9.24869059e-01,\n",
       "        6.35582963e-01, 8.66370315e-01, 5.28608346e-01, 6.31201358e-01,\n",
       "        5.76490976e-01, 9.38942846e-01, 3.80725740e-01, 3.41206395e-01,\n",
       "        2.58836956e-01, 5.40662096e-01, 7.58500289e-01, 2.89144408e-01,\n",
       "        6.03378577e-01, 9.49085597e-01, 5.19532966e-01, 1.24932973e-01],\n",
       "       [4.95923888e-02, 2.62852140e-01, 8.23949986e-01, 1.11290204e-01,\n",
       "        4.04223520e-01, 6.24052895e-01, 9.50023261e-01, 1.89415796e-01,\n",
       "        3.83300365e-01, 7.23999734e-01, 8.90663559e-01, 1.11693383e-01,\n",
       "        2.55117706e-01, 6.69171054e-01, 5.87299423e-01, 3.24851747e-01,\n",
       "        8.85511692e-01, 7.83372249e-01, 2.05043211e-01, 1.51149495e-01,\n",
       "        4.31685639e-01, 2.66201323e-01, 7.08575838e-01, 9.32305484e-01,\n",
       "        2.19299920e-01, 4.09585875e-01, 5.83333487e-01, 2.95221561e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### im2col로 데이터 전개하기\n",
    ">> 합성곱 연산을 곧이곧대로 구현하려면 for 문을 겹겹이 써야함  \n",
    ">> &nbsp; &nbsp; &rarr; 너무 귀찮고, numpy에 for문을 사용하면 성능이 떨어짐  \n",
    ">> &nbsp; &nbsp; &rarr; numpy에서는 원소에 접근할 때 for문을 사용하지 않는 것이 바람직함  \n",
    ">> \n",
    ">> for문 대신 im2col이라는 편의 함수를 사용해 간단하게 구현  \n",
    ">> \n",
    ">> im2col은 입력 데이터를 필터링(가중치 계산)하기 좋게 전개하는(펼치는) 함수  \n",
    ">> \n",
    ">> 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀜  \n",
    ">> &nbsp; &nbsp; &rarr; 정확히는 배치 안의 데이터 수까지 포함한 4차원 데이터를 2차원으로 변환  \n",
    ">> \n",
    ">> ![pic/im2col_do](pic/im2col_do.png)  \n",
    ">> \n",
    ">> im2col은 필터링하기 좋게 입력 데이터를 전개함  \n",
    ">> &nbsp; &nbsp; &rarr; 입력 데이터에서 필터를 적용하는 영역(3차원 블록)을 한 줄로 늘어놓음  \n",
    ">> \n",
    ">> 이러한 전개를 필터를 적용하는 모든 영역에서 수행하는 것이 im2col 함수임  \n",
    ">> \n",
    ">> im2col은 'image to column', 즉 '이미지에서 행렬로'라는 뜻  \n",
    ">> \n",
    ">> ![pic/im2col](pic/im2col_do_2.png)  \n",
    ">> \n",
    ">> 위 그림에서는 보기 좋게끔 stride를 크게 잡아 필터의 적용 영역이 겹치지 않도록 했지만, 실제 상황에서는 영역이 겹치는 경우가 대부분  \n",
    ">> \n",
    ">> 필터 적용 영역이 겹치게 되면 im2col로 전개한 후의 원소 수가 원래 블록의 원소 수보다 많아짐  \n",
    ">> &nbsp; &nbsp; &rarr; im2col을 사용해 구현하면 메모리를 더 많이 소비한다는 단점이 있음  \n",
    ">> \n",
    ">> 하지만 컴퓨터는 큰 행렬을 묶어서 계산하는 데 탁월함  \n",
    ">> &nbsp; &nbsp; &rarr; 예를 들어 행렬 계산 라이브러리(선형 대수 라이브러리) 등은 행렬 계산에 고도로 최적화되어 큰 행렬의 곱셈을 빠르게 계산할 수 있음  \n",
    ">> &nbsp; &nbsp; &rarr; 문제를 행렬 계산으로 만들면 선형 대수 라이브러리를 활용해 효율을 높일 수 있음  \n",
    ">>  \n",
    ">> im2col로 입력 데이터를 전개한 후에는 Conv layer의 필터(가중치)를 1열로 전개하고, 두 행렬의 곱을 계산  \n",
    ">> &nbsp; &nbsp; &rarr; 이는 fully-connected layer의 Affine layer에서 한 것과 거의 같음  \n",
    ">> \n",
    ">> ![pic/after_im2col](pic/after_im2col.png)  \n",
    ">> \n",
    ">> 그림 7-19와 같이 im2col 방식으로 출력한 결과는 2차원 행렬  \n",
    ">> \n",
    ">> CNN은 데이터를 4차원 배열로 저장하므로 2차원 출력 데이터를 4차원으로 변형(reshape)함\n",
    ">>  \n",
    ">> 이상이 Conv layer의 구현 흐름  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### Conv layer 구현하기\n",
    ">> 이 책에서는 im2col 함수를 미리 만들어 제공 (common/util.py)  \n",
    ">> \n",
    ">> im2col의 구현은 간단한 함수 10개 정도를 묶은 것  \n",
    ">>  \n",
    ">> im2col의 인터페이스는 다음과 같음  \n",
    ">> ```\n",
    ">> im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    ">> ```  \n",
    ">> * `input_data` - (number of data, channel, height, width)의 4차원 배열로 이뤄진 입력 데이터  \n",
    ">> \n",
    ">> * `filter_h` - 필터의 높이\n",
    ">> * `filter_w` - 필터의 너비\n",
    ">> * `stride` - stride\n",
    ">> * `pad` - padding  \n",
    ">>  \n",
    ">> im2col은 '필터 크기', 'stride', 'padding'을 고려하여 입력 데이터를 2차원 배열로 전개  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# im2col 사용\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from DLS_git_clone.common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> 위 구현에서는 두 가지 예를 보여주고 있음  \n",
    ">> * 배치 크기:1 (데이터 1개), 채널:3, height &middot; width:$7 \\times 7$  \n",
    ">> * 배치 크기:10 (데이터 10개), 채널:3, height &middot; width:$7 \\times 7$  \n",
    ">>  \n",
    ">> im2col을 적용한 두 경우 모두 2번째 차원의 원소는 75개, 이 값은 필터의 원소 수와 같음 (채널 3개, $5 \\times 5$데이터)  \n",
    ">> \n",
    ">> 배치 크기가 1일 때는 im2col의 결과의 크기가 (9, 75)이고 10일 때는 그 10배인 (90, 75)크기의 데이터가 저장됨  \n",
    ">> \n",
    ">> 이제 이 im2col을 사용하여 Conv layer 구현, Conv layer를 Convolution이라는 클래스로 구현  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1+(H+2*self.pad-FH)/self.stride)\n",
    "        out_w = int(1+(W+2*self.pad-FW)/self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Conv layer는 필터(가중치), 편향, stride, padding을 인수로 받아 초기화  \n",
    ">> \n",
    ">> 필터는 (FN, C, FH, FW)의 4차원 형상  \n",
    ">> * FN: 필터 개수  \n",
    ">> \n",
    ">> * C: 채널  \n",
    ">> * FH: 필터 높이  \n",
    ">> * FW: 필터 너비  \n",
    ">> \n",
    ">> 위 구현에서 입력 데이터를 im2col로 전개하고 필터도 reshape을 사용해 2차원 배열로 전개함  \n",
    ">> \n",
    ">> 그리고 이렇게 전개한 두 행렬의 곱을 구함  \n",
    ">> \n",
    ">> 필터를 전개하는 부분은 그림 7-19에서 보듯 각 필터 블록을 1줄로 펼쳐 세움  \n",
    ">> \n",
    ">> 이때 reshape의 두 번째 인수를 -1로 지정  \n",
    ">> &nbsp; &nbsp; &rarr; 이는 reshape이 제공하는 편의 기능으로, reshape에 -1을 지정하면 다차원 배열의 원소 수가 변환 후에도 똑같이 유지되도록 적절히 묶어줌  \n",
    ">> &nbsp; &nbsp; &rarr; 위의 코드에서 (10, 3, 5, 5) 형상을 한 다차원 배열 W의 원소 수는 총 750개, 이 배열에 reshape(10, -1)을 호출하면 750개의 원소를 10묶음으로, 즉 shape이 (10, 75)인 배열로 만들어줌  \n",
    ">> \n",
    ">> 다음으로 forward 구현의 마지막에서는 출력 데이터를 적절한 형상으로 바꿔줌  \n",
    ">> \n",
    ">> 이때 numpy의 transpose 함수를 사용  \n",
    ">> &nbsp; &nbsp; &rarr; 다차원 배열의 축 순서를 바꿔주는 함수  \n",
    ">> \n",
    ">> 그림 7-20과 같이 인덱스(0부터 시작)를 지정하여 축의 순서를 변경  \n",
    ">>  \n",
    ">> ![pic/transpose](pic/transpose.png)  \n",
    ">> \n",
    ">> 이상이 Conv Layer의 forward 구현  \n",
    ">> \n",
    ">> im2col로 전개한 덕분에 fully-connected layer의 Affine layer와 거의 똑같이 구현할 수 있었음  \n",
    ">> \n",
    ">> 다음은 Conv layer의 역전파를 구현할 차례이지만, Affine layer의 구현과 공통점이 많아 따로 설명하지 않음  \n",
    ">> \n",
    ">> 주의할 것이 하나 있는데, Conv layer의 역전파에서는 im2col을 역으로 처리해야 함  \n",
    ">> &nbsp; &nbsp; &rarr; 이는 이 책이 제공하는 col2im 함수를 사용 (common/util.py)  \n",
    ">> \n",
    ">> col2im을 사용한다는 점을 제외하면 Conv layer의 역전파는 Affine layer와 같음  \n",
    ">> \n",
    ">> Conv layer의 역전파 구현은 common/layer.py에 있음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### Pooling layer 구현하기\n",
    ">> pooling layer 구현도 Conv layer와 마찬가지로 m2col을 사용해 입력 데이터를 전개함  \n",
    ">> \n",
    ">> 단, pooling의 경우 채널 쪽이 독립적이라는 점이 Conv layer 때와 다름  \n",
    ">> &nbsp; &nbsp; &rarr; 구체적으로는 그림 7-21과 같이 pooling 적용 영역을 채널마다 독립적으로 전개함  \n",
    ">> \n",
    ">> ![pic/pooling](pic/pooling.png)  \n",
    ">> \n",
    ">> 일단 이렇게 전개한 후, 전개한 행렬에서 행별 최댓값을 구하고 적절한 shape으로 성형하기만 하면 됨  \n",
    ">> \n",
    ">> ![pic/pooling_layer_flow](pic/pooling_layer_flow.png)  \n",
    ">> \n",
    ">> 이상이 pooling layer의 forward 처리 흐름, 이를 파이썬으로 구현하면 다음과 같음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling layer 구현\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1+(H-self.poo_h)/self.stride)\n",
    "        out_w = int(1+(W-self.pool_w)/self.stride)\n",
    "\n",
    "        # 전개\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        # 최댓값\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        # 성형\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Pooling layer의 구현은 다음의 세 단계로 진행  \n",
    ">> &nbsp; &nbsp; &rarr; 1. 입력 데이터를 전개함  \n",
    ">> &nbsp; &nbsp; &rarr; 2. 행별 최댓값을 구함  \n",
    ">> &nbsp; &nbsp; &rarr; 3. 적절한 모양으로 성형  \n",
    ">> \n",
    ">> 최댓값 계산에는 numpy의 `np.max` 메서드를 사용할 수 있음  \n",
    ">> \n",
    ">> `np.max`는 인수로 축(axis)을 지정할 수 있는데, 이 인수로 지정한 축마다 최댓값을 구할 수 있음  \n",
    ">> &nbsp; &nbsp; &rarr; `np.max(x, axis=1)`과 같이 쓰면, 입력 x의 1번째 차원의 축마다 최댓값을 구함  \n",
    ">> \n",
    ">> 이상이 Pooling layer의 forward 처리  \n",
    ">> \n",
    ">> 지금까지 설명한 전략을 따라 입력 데이터를 pooling하기 쉬운 형태로 전개하면 그 후의 구현은 간단함  \n",
    ">> \n",
    ">> Pooling layer의 backward 처리는 관련 사항을 이미 설명했으니 설명을 생략  \n",
    ">> \n",
    ">> ReLU layer를 구현할 때 사용한 max의 역전파 참고 (5.5.1 ReLU layer)  \n",
    ">> \n",
    ">> Pooling layer의 전체 구현은 common/layer.py에 있음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **7.5** CNN 구현하기\n",
    "> Conv layer와 Pooling layer를 구현했으니, 이 layer들을 조합하여 손글씨 숫자를 인식하는 CNN을 조립할 예정 \n",
    ">  \n",
    "> 그림 7-23과 같은 CNN을 구현  \n",
    "> \n",
    "> ![pic/simple_cnn](pic/simple_cnn.png)  \n",
    "> \n",
    "> 위 그림의 CNN network는 'Convolution-ReLU-Pooing-Affine-ReLU-Affine-Softmax' 순으로 흐름  \n",
    "> \n",
    "> 이를 SimpleConvNet이라는 이름의 클래스로 구현  \n",
    "> \n",
    "> SimpleConvNet의 초기화(`__init__`), 초기화 때 받는 인수  \n",
    "> * `input_dim` - 입력 데이터(channel, height, width)의 차원  \n",
    "> \n",
    "> * `conv_param` - Conv layer의 hyper parameter(딕셔너리)  \n",
    "> &nbsp; &nbsp; &nbsp; &nbsp; 딕셔너리의 키  \n",
    ">       * `filter_num`- 필터 수  \n",
    ">       * `filter_siz`- 필터 크기  \n",
    ">       * `stride`- stride  \n",
    ">       * `pad` - padding  \n",
    "> * `hidden_size`- 은닉층(fully-connected)의 뉴런 수\n",
    "> * `output_size`- 출력층(fully-connected)의 뉴런 수\n",
    "> * `weight_init_std` - 초기화 때의 가중치 표준편차  \n",
    "> \n",
    "> 여기에서 Conv layer의 hyper parameter는 딕셔너리 형태로 주어짐(`conv_param`)  \n",
    "> \n",
    "> 이것은 필요한 hyper parameter값이 예컨대 `{'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}`처럼 저장된다는 뜻  \n",
    "> \n",
    "> SimpleConvNet의 초기화는 코드가 길어지므로 세 부분으로 나누어 설명  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "from DLS_git_clone.common.layers import Relu, SoftmaxWithLoss, Affine, Pooling\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        # 여기에서는 초기화 인수로 주어진 Conv layer의 hyper parameter를 딕셔너리에서 꺼냄(나중에 쓰기 쉽도록)  \n",
    "        # 그리고 Conv layer의 출력 크기를 계산\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size-filter_size+2*filter_pad)/filter_stride+1\n",
    "        pool_output_size = int(filter_num*(conv_output_size/2)*(conv_output_size/2))\n",
    "\n",
    "        # 가중치 매개변수를 초기화하는 부분\n",
    "        # 학습에 필요한 매개변수는 1번째 층의 Conv layer와 나머지 두 fully-connected layer의 가중치와 편향\n",
    "        # 이 매개변수들을 인스턴스 변수 params 딕셔너리에 저장\n",
    "        # 1번째 층의 합성곱 계층의 가중치를 W1, 편향을 b1이라는 키로 저장\n",
    "        # 마찬가지로 2번째 층의 fully-connected layer의 가중치와 편향을 W2와 b2, 마지막 3번째 층의 fully-connected layer의 가중치와 편향을 W3, b3라는 키로 저장함\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std* np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std* np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std* np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 마지막으로 CNN을 구성하는 계층들을 생성\n",
    "        # 순서가 있는 딕셔너리(OrderedDict)인 layers에 계층들을 차례로 추가\n",
    "        # 마지막 SoftmaxWithLoss 계층만큼은 last_layer라는 별도 변수에 저장해둠\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 이상이 SimpleConvNet의 초기화\n",
    "        # 초기화를 마친 후에는 추론을 수행하는 prediect 메서드와 손실 함수의 값을 구하는 loss 메서드를 다음과 같이 구현\n",
    "        # 이 코드에서 인수 x는 입력 데이터, t는 정답 레이블\n",
    "        # 추론을 수행하는 predict 메서드는 초기화 때 layers에 추가한 계층을 맨 앞에서부터 차례로 forward 메서드를 호출하며 그 결과를 다음 계층에 전달\n",
    "        # 손실 함수를 구하는 loss 메서드는 predict 메서드의 결과를 인수로 마지막 층의 forward 메서드를 호출\n",
    "        # 즉, 첫 계층부터 마지막 계층까지 forward를 처리함\n",
    "        def predict(self, x):\n",
    "            for layer in self.layers.values():\n",
    "                x = layer.forward(x)\n",
    "            return x\n",
    "        \n",
    "        def loss(self, x, t):\n",
    "            y = self.predict(x)\n",
    "            return self.last_layer.forward(y, t)\n",
    "\n",
    "        # 이어서 오차역전파법으로 기울기 구하는 구현\n",
    "        # 매개변수의 기울기는 오차역전파법으로 구함\n",
    "        # 이 과정은 순전파와 역전파를 반복\n",
    "        # 지금까지 각 계층의 순전파와 역전파 기능을 제대로 구현했다면, 여기에서는 단지 그것들을 적절한 순서로 호출만 해주면 됨\n",
    "        # 마지막으로 grads라는 딕셔너리 변수에 각 가중치 매개변수의 기울기를 저장\n",
    "        # 이상이 SimpleConvNet의 구현\n",
    "        def gradient(self, x, t):\n",
    "            # 순전파\n",
    "            self.loss(x, t)\n",
    "\n",
    "            # 역전파\n",
    "            dout = 1\n",
    "            dout = self.last_layer.backward(dout)\n",
    "\n",
    "            layers = list(self.layers.values())\n",
    "            layers.reverse()\n",
    "            for layer in layers:\n",
    "                dout = layer.backward(dout)\n",
    "            \n",
    "            # 결과 저장\n",
    "            grads = {}\n",
    "            grads['W1'] = self.layers['Conv1'].dw\n",
    "            grads['b1'] = self.layers['Conv1'].db\n",
    "            grads['W2'] = self.layers['Affine1'].dw\n",
    "            grads['b2'] = self.layers['Affine1'].db\n",
    "            grads['W3'] = self.layers['Affine2'].dw\n",
    "            grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "            return grads\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이제 이 SimpleConvNet으로 MNIST 데이터셋을 학습할 차례  \n",
    "> \n",
    "> 학습을 위한 코드는 4.5 학습 알고리즘 구현히기에서 설명한 것과 거의 같으니 코드는 생략 (소스코드는 ch07/train_convnet.py에 있음)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299158550656108\n",
      "=== epoch:1, train acc:0.121, test acc:0.15 ===\n",
      "train loss:2.296337646990855\n",
      "train loss:2.291616994033077\n",
      "train loss:2.2839854892011315\n",
      "train loss:2.272756873833349\n",
      "train loss:2.2629060584288214\n",
      "train loss:2.2529295182560385\n",
      "train loss:2.2178468329097187\n",
      "train loss:2.2116757439546024\n",
      "train loss:2.1914768829757314\n",
      "train loss:2.1469071128455077\n",
      "train loss:2.0988140278815726\n",
      "train loss:2.0853688186240205\n",
      "train loss:1.9679299489658235\n",
      "train loss:1.9565112715902502\n",
      "train loss:1.8832801568257702\n",
      "train loss:1.8813644157128508\n",
      "train loss:1.8319139000786375\n",
      "train loss:1.73387926750832\n",
      "train loss:1.67373724596842\n",
      "train loss:1.570842908955031\n",
      "train loss:1.444893828227135\n",
      "train loss:1.4770289385130146\n",
      "train loss:1.2552690842926604\n",
      "train loss:1.4561401185861629\n",
      "train loss:1.049497047194964\n",
      "train loss:1.1090595316517933\n",
      "train loss:1.0269994681126458\n",
      "train loss:0.9790485904717139\n",
      "train loss:0.8955882159664905\n",
      "train loss:0.8761884360862368\n",
      "train loss:0.7488071076022094\n",
      "train loss:0.823207966138878\n",
      "train loss:0.7663074987591869\n",
      "train loss:0.850906622756945\n",
      "train loss:0.6602020155755821\n",
      "train loss:0.7246376121252573\n",
      "train loss:0.7010984319094298\n",
      "train loss:0.7423610093435351\n",
      "train loss:0.7904572684851017\n",
      "train loss:0.6125304541875083\n",
      "train loss:0.6019588268991044\n",
      "train loss:0.5566391481673412\n",
      "train loss:0.6380162888242983\n",
      "train loss:0.4760000192620317\n",
      "train loss:0.5335448513179087\n",
      "train loss:0.4503062162375241\n",
      "train loss:0.4956316584458134\n",
      "train loss:0.4905439408706229\n",
      "train loss:0.4593117239672593\n",
      "train loss:0.5422169310021875\n",
      "train loss:0.5036610895867579\n",
      "train loss:0.46709177496961085\n",
      "train loss:0.475000789699999\n",
      "train loss:0.65485638442062\n",
      "train loss:0.40108375453055417\n",
      "train loss:0.5327176074824693\n",
      "train loss:0.537009438533726\n",
      "train loss:0.5166662849951217\n",
      "train loss:0.4334526497164342\n",
      "train loss:0.39453737331218597\n",
      "train loss:0.6276767802872392\n",
      "train loss:0.4690337523024333\n",
      "train loss:0.3685276160406281\n",
      "train loss:0.48961539594921716\n",
      "train loss:0.4216109613122565\n",
      "train loss:0.508798277971165\n",
      "train loss:0.4450950793359829\n",
      "train loss:0.44377738225997804\n",
      "train loss:0.7129325612547319\n",
      "train loss:0.6111084032493871\n",
      "train loss:0.5194738331930269\n",
      "train loss:0.47392878572991065\n",
      "train loss:0.36530983911963916\n",
      "train loss:0.38168140636081943\n",
      "train loss:0.3591858350091921\n",
      "train loss:0.45863015442638216\n",
      "train loss:0.3277323007592025\n",
      "train loss:0.49288456130442315\n",
      "train loss:0.6081474403372649\n",
      "train loss:0.415601300554032\n",
      "train loss:0.5397588367966898\n",
      "train loss:0.3695164452264443\n",
      "train loss:0.4493998273809837\n",
      "train loss:0.33647853630196023\n",
      "train loss:0.5023735808872994\n",
      "train loss:0.3293613920507326\n",
      "train loss:0.29229193221091043\n",
      "train loss:0.31550792531625127\n",
      "train loss:0.25805753483528787\n",
      "train loss:0.4195192281591184\n",
      "train loss:0.6398293141696092\n",
      "train loss:0.392117297712419\n",
      "train loss:0.3890982497417589\n",
      "train loss:0.3920520866800029\n",
      "train loss:0.26151446256725047\n",
      "train loss:0.3128832096405084\n",
      "train loss:0.3914913524224556\n",
      "train loss:0.33990696179985264\n",
      "train loss:0.36803616003490364\n",
      "train loss:0.3823518827921461\n",
      "train loss:0.4437477194217858\n",
      "train loss:0.3699160627331979\n",
      "train loss:0.3732268702951413\n",
      "train loss:0.5979024953917794\n",
      "train loss:0.35444503402518485\n",
      "train loss:0.46578448337241946\n",
      "train loss:0.303226520735164\n",
      "train loss:0.4311487630976963\n",
      "train loss:0.2990792979988166\n",
      "train loss:0.23074633124991828\n",
      "train loss:0.31405317314329734\n",
      "train loss:0.44575261086309786\n",
      "train loss:0.32057460007247135\n",
      "train loss:0.5498386170794681\n",
      "train loss:0.4072063130114406\n",
      "train loss:0.36410217904433556\n",
      "train loss:0.23859148974935007\n",
      "train loss:0.286757650606928\n",
      "train loss:0.36279985193604447\n",
      "train loss:0.26928720889671065\n",
      "train loss:0.24904113820131954\n",
      "train loss:0.4187801574949622\n",
      "train loss:0.431026437678349\n",
      "train loss:0.29052915615673325\n",
      "train loss:0.33841767033620224\n",
      "train loss:0.2537923241797704\n",
      "train loss:0.3293598370021239\n",
      "train loss:0.26168706811085857\n",
      "train loss:0.2531830494144204\n",
      "train loss:0.4118314293214663\n",
      "train loss:0.3127851341471469\n",
      "train loss:0.20600373878861183\n",
      "train loss:0.40425151611686966\n",
      "train loss:0.24158871895551504\n",
      "train loss:0.2735680364813091\n",
      "train loss:0.4591137491545965\n",
      "train loss:0.20729028792501566\n",
      "train loss:0.3446503866567673\n",
      "train loss:0.2774275304012656\n",
      "train loss:0.2872220268306509\n",
      "train loss:0.308076161248232\n",
      "train loss:0.30138429437271774\n",
      "train loss:0.3795715474723443\n",
      "train loss:0.3488733967873241\n",
      "train loss:0.2964757555111107\n",
      "train loss:0.3052450450558908\n",
      "train loss:0.19690541123479444\n",
      "train loss:0.31110806963561993\n",
      "train loss:0.300890010507816\n",
      "train loss:0.3958741497330995\n",
      "train loss:0.407382478868478\n",
      "train loss:0.4247110627150012\n",
      "train loss:0.22552851718222516\n",
      "train loss:0.3063356244819015\n",
      "train loss:0.31802297211869723\n",
      "train loss:0.26765808334147884\n",
      "train loss:0.22907736632911554\n",
      "train loss:0.33772191654868744\n",
      "train loss:0.4058179413985332\n",
      "train loss:0.33472463252828144\n",
      "train loss:0.2405552351906633\n",
      "train loss:0.34534285791017294\n",
      "train loss:0.35054509618398844\n",
      "train loss:0.1952812245682305\n",
      "train loss:0.33708789358044855\n",
      "train loss:0.37063616931722543\n",
      "train loss:0.37289474420261187\n",
      "train loss:0.2800661506663767\n",
      "train loss:0.3978136686879388\n",
      "train loss:0.24364215820198395\n",
      "train loss:0.29725512801675164\n",
      "train loss:0.44361321544930804\n",
      "train loss:0.2609715588650339\n",
      "train loss:0.3864974653277941\n",
      "train loss:0.21841367659593078\n",
      "train loss:0.25091661796458486\n",
      "train loss:0.3252917818450751\n",
      "train loss:0.26240805009488855\n",
      "train loss:0.2746864274733232\n",
      "train loss:0.24619062306764541\n",
      "train loss:0.1939976396554453\n",
      "train loss:0.37312143875626697\n",
      "train loss:0.2696577195612719\n",
      "train loss:0.236139365729902\n",
      "train loss:0.22506764372837867\n",
      "train loss:0.26859018736418255\n",
      "train loss:0.26176558072699785\n",
      "train loss:0.3152140003234806\n",
      "train loss:0.3342653937302795\n",
      "train loss:0.2845942627165412\n",
      "train loss:0.35464133123782526\n",
      "train loss:0.2561083612194201\n",
      "train loss:0.16662926513586793\n",
      "train loss:0.30366337501043816\n",
      "train loss:0.18070827364257525\n",
      "train loss:0.26218055311036736\n",
      "train loss:0.28987821507217526\n",
      "train loss:0.25551773881318757\n",
      "train loss:0.2894243557112624\n",
      "train loss:0.19688680650603757\n",
      "train loss:0.3837736576208527\n",
      "train loss:0.24247996487150839\n",
      "train loss:0.2422938887949924\n",
      "train loss:0.3283674460737985\n",
      "train loss:0.2589047385885008\n",
      "train loss:0.15105540553565217\n",
      "train loss:0.1938330679530698\n",
      "train loss:0.2779788138261286\n",
      "train loss:0.20107350829341888\n",
      "train loss:0.13234180125918088\n",
      "train loss:0.16682514475162183\n",
      "train loss:0.2141465748013745\n",
      "train loss:0.11975059071766003\n",
      "train loss:0.1552374592672402\n",
      "train loss:0.2832600785651729\n",
      "train loss:0.22422297427701257\n",
      "train loss:0.216810656299348\n",
      "train loss:0.16892717832140625\n",
      "train loss:0.34486409469235463\n",
      "train loss:0.15072678461393957\n",
      "train loss:0.2800315914281697\n",
      "train loss:0.19431193780014916\n",
      "train loss:0.2935682386073421\n",
      "train loss:0.2652376350223985\n",
      "train loss:0.22582953735070496\n",
      "train loss:0.208789406565278\n",
      "train loss:0.441217574549025\n",
      "train loss:0.3025618965566878\n",
      "train loss:0.21501663628229145\n",
      "train loss:0.21787957006608696\n",
      "train loss:0.16758324759361048\n",
      "train loss:0.26219806684035496\n",
      "train loss:0.175616259741343\n",
      "train loss:0.2571555569247822\n",
      "train loss:0.2939439650821626\n",
      "train loss:0.18142796256901658\n",
      "train loss:0.26913872975251574\n",
      "train loss:0.31790820348008764\n",
      "train loss:0.3747124467834868\n",
      "train loss:0.17603113383705002\n",
      "train loss:0.31188087792830727\n",
      "train loss:0.19837698816136778\n",
      "train loss:0.19804054252160477\n",
      "train loss:0.2766686277225066\n",
      "train loss:0.16854197990079045\n",
      "train loss:0.20567679327902613\n",
      "train loss:0.1125818584488185\n",
      "train loss:0.23865282268162105\n",
      "train loss:0.27805874463856284\n",
      "train loss:0.34747230765186177\n",
      "train loss:0.17909147581377016\n",
      "train loss:0.2150462955767168\n",
      "train loss:0.2589608948464247\n",
      "train loss:0.35175803295204844\n",
      "train loss:0.27460432388658723\n",
      "train loss:0.1578571418237022\n",
      "train loss:0.40328278596603184\n",
      "train loss:0.29784426124200836\n",
      "train loss:0.20625234642504334\n",
      "train loss:0.324821075226521\n",
      "train loss:0.22935878010537558\n",
      "train loss:0.19814123176378515\n",
      "train loss:0.3266531548539159\n",
      "train loss:0.36993514758276613\n",
      "train loss:0.20357491368638048\n",
      "train loss:0.20004269876569103\n",
      "train loss:0.25767403971140135\n",
      "train loss:0.15672700425371183\n",
      "train loss:0.28702559818539475\n",
      "train loss:0.360896949292774\n",
      "train loss:0.1770376977268144\n",
      "train loss:0.1518970889100844\n",
      "train loss:0.3718469328460713\n",
      "train loss:0.27266529512267473\n",
      "train loss:0.23378200218373046\n",
      "train loss:0.20069826032523255\n",
      "train loss:0.21039002229135195\n",
      "train loss:0.32155188857544\n",
      "train loss:0.2956857489226903\n",
      "train loss:0.1955278600791926\n",
      "train loss:0.16964882469521014\n",
      "train loss:0.21273276673149766\n",
      "train loss:0.1413311665726484\n",
      "train loss:0.15732045313697446\n",
      "train loss:0.15578489079410685\n",
      "train loss:0.1713996209131337\n",
      "train loss:0.16782384712720969\n",
      "train loss:0.21005614411325946\n",
      "train loss:0.14559138527763196\n",
      "train loss:0.18522033720044706\n",
      "train loss:0.09058613313561031\n",
      "train loss:0.17715915806046278\n",
      "train loss:0.15840305859023668\n",
      "train loss:0.17380204807850352\n",
      "train loss:0.1020115891005856\n",
      "train loss:0.10785498323376753\n",
      "train loss:0.16124730177393062\n",
      "train loss:0.12053156174983357\n",
      "train loss:0.143998301963458\n",
      "train loss:0.216474901345641\n",
      "train loss:0.28382120203055033\n",
      "train loss:0.18490101536166292\n",
      "train loss:0.20563337441725466\n",
      "train loss:0.170782506624562\n",
      "train loss:0.09856132050545402\n",
      "train loss:0.2023944064990843\n",
      "train loss:0.13313053392304025\n",
      "train loss:0.1803575931121782\n",
      "train loss:0.2076854659382148\n",
      "train loss:0.17098637226838254\n",
      "train loss:0.24910741180115764\n",
      "train loss:0.13695464864111137\n",
      "train loss:0.14228915700340164\n",
      "train loss:0.07983027384910203\n",
      "train loss:0.15786906901584635\n",
      "train loss:0.20279699821953176\n",
      "train loss:0.24394884352249666\n",
      "train loss:0.1280255905783302\n",
      "train loss:0.26426420880944007\n",
      "train loss:0.2807802776211428\n",
      "train loss:0.04587242242230836\n",
      "train loss:0.07253449385962094\n",
      "train loss:0.15426854305054355\n",
      "train loss:0.10070970325252337\n",
      "train loss:0.07094645441284085\n",
      "train loss:0.0710831809478123\n",
      "train loss:0.1407974188809372\n",
      "train loss:0.13481485280857697\n",
      "train loss:0.15101008830163162\n",
      "train loss:0.16351575087838285\n",
      "train loss:0.2514137874901853\n",
      "train loss:0.17271982480541312\n",
      "train loss:0.2726191135049581\n",
      "train loss:0.12130824590169097\n",
      "train loss:0.11062213325107631\n",
      "train loss:0.08404097052108257\n",
      "train loss:0.3235703311694379\n",
      "train loss:0.1572588806356851\n",
      "train loss:0.21336051974854342\n",
      "train loss:0.16215320561062133\n",
      "train loss:0.09506803143951313\n",
      "train loss:0.235698862066758\n",
      "train loss:0.2177225899979477\n",
      "train loss:0.2129241577616922\n",
      "train loss:0.21643912816895652\n",
      "train loss:0.2747179668296229\n",
      "train loss:0.2490143358782489\n",
      "train loss:0.11897092556985596\n",
      "train loss:0.20808107480028773\n",
      "train loss:0.14928759205785985\n",
      "train loss:0.30784214522930103\n",
      "train loss:0.20088614313883899\n",
      "train loss:0.07785572649723366\n",
      "train loss:0.18876972128443023\n",
      "train loss:0.1661161340905993\n",
      "train loss:0.16151316410003158\n",
      "train loss:0.1409449618833844\n",
      "train loss:0.07245890971431952\n",
      "train loss:0.20516122990919466\n",
      "train loss:0.0723780032525777\n",
      "train loss:0.17120510525348023\n",
      "train loss:0.10900820217736885\n",
      "train loss:0.10187384588076238\n",
      "train loss:0.2936029142592738\n",
      "train loss:0.12510361820523708\n",
      "train loss:0.12138037731739006\n",
      "train loss:0.15698063207984375\n",
      "train loss:0.1589038209924067\n",
      "train loss:0.2039696108250097\n",
      "train loss:0.19458813346852263\n",
      "train loss:0.10221904615094315\n",
      "train loss:0.16590809020027017\n",
      "train loss:0.20842607821661666\n",
      "train loss:0.14913908467490888\n",
      "train loss:0.2363874269052548\n",
      "train loss:0.09476777536416253\n",
      "train loss:0.1641630028376404\n",
      "train loss:0.09839726069791299\n",
      "train loss:0.08926380535004202\n",
      "train loss:0.2359310854417587\n",
      "train loss:0.18598320350328645\n",
      "train loss:0.15871067532729582\n",
      "train loss:0.18408797466633214\n",
      "train loss:0.12693159872348117\n",
      "train loss:0.1839199984081912\n",
      "train loss:0.14334591714963815\n",
      "train loss:0.0782338240477395\n",
      "train loss:0.08992473558898055\n",
      "train loss:0.1289867766532868\n",
      "train loss:0.15901866457345187\n",
      "train loss:0.10267294137029223\n",
      "train loss:0.07195092828386387\n",
      "train loss:0.11883200437304171\n",
      "train loss:0.09007530249937938\n",
      "train loss:0.18247987555185557\n",
      "train loss:0.1007204884180473\n",
      "train loss:0.23856617179397488\n",
      "train loss:0.09773625845245677\n",
      "train loss:0.07371482641838037\n",
      "train loss:0.1961428553916857\n",
      "train loss:0.1560541804316081\n",
      "train loss:0.13728012848788718\n",
      "train loss:0.10925179862810293\n",
      "train loss:0.15575297362653653\n",
      "train loss:0.250138273865424\n",
      "train loss:0.10412833230787469\n",
      "train loss:0.09162426876283605\n",
      "train loss:0.11803021914940226\n",
      "train loss:0.17527689031854654\n",
      "train loss:0.15359103829150678\n",
      "train loss:0.14072795800528068\n",
      "train loss:0.08566820817642373\n",
      "train loss:0.2686182116318033\n",
      "train loss:0.08792138249631956\n",
      "train loss:0.10741306823199043\n",
      "train loss:0.2579949083938372\n",
      "train loss:0.24885429581910973\n",
      "train loss:0.1326344428347779\n",
      "train loss:0.1057046973243686\n",
      "train loss:0.1906899135813673\n",
      "train loss:0.12409211012802407\n",
      "train loss:0.23110549413445627\n",
      "train loss:0.1327668094371152\n",
      "train loss:0.07162624854745737\n",
      "train loss:0.298802465370507\n",
      "train loss:0.058231916662775794\n",
      "train loss:0.2237028300501398\n",
      "train loss:0.18700094520465552\n",
      "train loss:0.099822554819922\n",
      "train loss:0.2127870145458158\n",
      "train loss:0.13120992164660725\n",
      "train loss:0.18607927470559146\n",
      "train loss:0.1041246685933373\n",
      "train loss:0.31753835013648485\n",
      "train loss:0.08308592376978043\n",
      "train loss:0.15288352867633237\n",
      "train loss:0.18484745311613587\n",
      "train loss:0.11525975410792111\n",
      "train loss:0.3226766341176112\n",
      "train loss:0.08965181322590002\n",
      "train loss:0.08990324530306154\n",
      "train loss:0.11466834044370998\n",
      "train loss:0.10231979656868752\n",
      "train loss:0.07444930027755102\n",
      "train loss:0.19093988860173827\n",
      "train loss:0.17936289091844185\n",
      "train loss:0.13907271060166948\n",
      "train loss:0.09867041888008535\n",
      "train loss:0.1264070085763847\n",
      "train loss:0.10281026120949716\n",
      "train loss:0.1743411491441577\n",
      "train loss:0.07897231742461665\n",
      "train loss:0.10056792207358187\n",
      "train loss:0.16033152687921468\n",
      "train loss:0.07468395185897975\n",
      "train loss:0.06138585694323839\n",
      "train loss:0.15475929636491403\n",
      "train loss:0.14785928574074572\n",
      "train loss:0.11853510257467116\n",
      "train loss:0.06628251972404021\n",
      "train loss:0.15069623889075381\n",
      "train loss:0.14828638231660043\n",
      "train loss:0.3895082765955781\n",
      "train loss:0.1453421272387363\n",
      "train loss:0.106731509303749\n",
      "train loss:0.15486006755632364\n",
      "train loss:0.15487130454166442\n",
      "train loss:0.10252602000606542\n",
      "train loss:0.1346566914901813\n",
      "train loss:0.15163019249920937\n",
      "train loss:0.11424670293107923\n",
      "train loss:0.16498715680029882\n",
      "train loss:0.05560937483320751\n",
      "train loss:0.2808325323039109\n",
      "train loss:0.09378474594967327\n",
      "train loss:0.18027742351900536\n",
      "train loss:0.11638339082217664\n",
      "train loss:0.17404786273928038\n",
      "train loss:0.05809469287639381\n",
      "train loss:0.07826109479257115\n",
      "train loss:0.18192240836196927\n",
      "train loss:0.1572979748353261\n",
      "train loss:0.10762826845651757\n",
      "train loss:0.20304139410091845\n",
      "train loss:0.149292422322037\n",
      "train loss:0.08954390144708352\n",
      "train loss:0.14067340633027878\n",
      "train loss:0.15307325353058773\n",
      "train loss:0.11688546698715845\n",
      "train loss:0.15018260546155626\n",
      "train loss:0.14487503272575264\n",
      "train loss:0.05733038285396499\n",
      "train loss:0.26293232080032103\n",
      "train loss:0.21758748750402443\n",
      "train loss:0.054404550565254034\n",
      "train loss:0.19297524558867235\n",
      "train loss:0.13192650193379196\n",
      "train loss:0.22601964892894377\n",
      "train loss:0.19448027488701464\n",
      "train loss:0.1014333183144795\n",
      "train loss:0.15188106235646442\n",
      "train loss:0.056059228971479726\n",
      "train loss:0.15988704964449263\n",
      "train loss:0.1508316642051607\n",
      "train loss:0.09906196347168755\n",
      "train loss:0.15294126356536714\n",
      "train loss:0.13317416001222449\n",
      "train loss:0.11481788530996928\n",
      "train loss:0.12645361759540466\n",
      "train loss:0.30342751964690645\n",
      "train loss:0.11090357437868124\n",
      "train loss:0.1282759408364644\n",
      "train loss:0.12549409179072155\n",
      "train loss:0.15515174907066245\n",
      "train loss:0.08348913294473763\n",
      "train loss:0.03936481067141909\n",
      "train loss:0.1629669209237829\n",
      "train loss:0.1727939503730925\n",
      "train loss:0.2728080699621649\n",
      "train loss:0.11150521750928637\n",
      "train loss:0.1331742721789449\n",
      "train loss:0.11863995338969163\n",
      "train loss:0.16426793900954473\n",
      "train loss:0.16483214039444832\n",
      "train loss:0.14840607331888442\n",
      "train loss:0.10626323012890843\n",
      "train loss:0.1257440900742226\n",
      "train loss:0.07317670644199571\n",
      "train loss:0.11023024869577328\n",
      "train loss:0.13994630412932121\n",
      "train loss:0.10068549629653999\n",
      "train loss:0.11061876712579537\n",
      "train loss:0.055301951028950865\n",
      "train loss:0.09071082110053048\n",
      "train loss:0.08149996670369579\n",
      "train loss:0.06524225845426213\n",
      "train loss:0.191716727320151\n",
      "train loss:0.21947962713019628\n",
      "train loss:0.11872756919111557\n",
      "train loss:0.10798921576352638\n",
      "train loss:0.12674303461816022\n",
      "train loss:0.07246645232027428\n",
      "train loss:0.08945741500685812\n",
      "train loss:0.10668522667450643\n",
      "train loss:0.11638379848899519\n",
      "train loss:0.17835126097919218\n",
      "train loss:0.14207203544086855\n",
      "train loss:0.12880159164018345\n",
      "train loss:0.1115363130394618\n",
      "train loss:0.11338648105574092\n",
      "train loss:0.23069789069167984\n",
      "train loss:0.17431893563663664\n",
      "train loss:0.06692605806946504\n",
      "train loss:0.20000952005093964\n",
      "train loss:0.05807468013293024\n",
      "train loss:0.07552259196313185\n",
      "train loss:0.14023541465524234\n",
      "train loss:0.08798473869789875\n",
      "train loss:0.14795889100262147\n",
      "train loss:0.0984899185161294\n",
      "train loss:0.12747801380467352\n",
      "train loss:0.1049888542658914\n",
      "train loss:0.12388251334392457\n",
      "train loss:0.10301010306754252\n",
      "train loss:0.14127934978440954\n",
      "train loss:0.1444389440351129\n",
      "train loss:0.11683772208160534\n",
      "train loss:0.11187316312717624\n",
      "train loss:0.13641288861329037\n",
      "train loss:0.08017529742193258\n",
      "train loss:0.08839131634173508\n",
      "train loss:0.07057964663170828\n",
      "train loss:0.08590991032400419\n",
      "train loss:0.1162054061547601\n",
      "train loss:0.14816251560050855\n",
      "train loss:0.05814372264691388\n",
      "train loss:0.046761542854743786\n",
      "train loss:0.06450582871395578\n",
      "train loss:0.15017000535529193\n",
      "train loss:0.03773273153573094\n",
      "train loss:0.2509159372638606\n",
      "train loss:0.07379082533216448\n",
      "train loss:0.21020587120657674\n",
      "train loss:0.17312095904456412\n",
      "train loss:0.25039461212187847\n",
      "train loss:0.10842155078532756\n",
      "train loss:0.09044642168954896\n",
      "train loss:0.09332556053785634\n",
      "train loss:0.10740889422111792\n",
      "train loss:0.1757304318785809\n",
      "train loss:0.16249130998217443\n",
      "train loss:0.2060408628109457\n",
      "train loss:0.07960286190372781\n",
      "train loss:0.0555698182163007\n",
      "train loss:0.20287368796636765\n",
      "train loss:0.09731915992292556\n",
      "train loss:0.10701406783148341\n",
      "train loss:0.11846653900187416\n",
      "train loss:0.09106629338937484\n",
      "train loss:0.05140068815419118\n",
      "=== epoch:2, train acc:0.97, test acc:0.967 ===\n",
      "train loss:0.1238399306498898\n",
      "train loss:0.08154351384736609\n",
      "train loss:0.13180927510080723\n",
      "train loss:0.09313664565730137\n",
      "train loss:0.09085081823504193\n",
      "train loss:0.11519008704696528\n",
      "train loss:0.0311438536282039\n",
      "train loss:0.12818179256827322\n",
      "train loss:0.15406922989287153\n",
      "train loss:0.09395103584564796\n",
      "train loss:0.07810687333237391\n",
      "train loss:0.2113444079709693\n",
      "train loss:0.06915335064082877\n",
      "train loss:0.11720561669295204\n",
      "train loss:0.07348649064655657\n",
      "train loss:0.05620786108109474\n",
      "train loss:0.11582843218335864\n",
      "train loss:0.060710919672549615\n",
      "train loss:0.05337275887859743\n",
      "train loss:0.16811181970889963\n",
      "train loss:0.1240551470632397\n",
      "train loss:0.14282008146638547\n",
      "train loss:0.15740468523609347\n",
      "train loss:0.07718693504693577\n",
      "train loss:0.2202024588824267\n",
      "train loss:0.16820841049658614\n",
      "train loss:0.08252754269535767\n",
      "train loss:0.06005122951994738\n",
      "train loss:0.11964702463198978\n",
      "train loss:0.09081619318718114\n",
      "train loss:0.13293449792606954\n",
      "train loss:0.08978569866691213\n",
      "train loss:0.19575456502183713\n",
      "train loss:0.06667583552836677\n",
      "train loss:0.08493794882081698\n",
      "train loss:0.1193839582730827\n",
      "train loss:0.0936124077367185\n",
      "train loss:0.07300733913449343\n",
      "train loss:0.07682557886307785\n",
      "train loss:0.07734637418992293\n",
      "train loss:0.11644923218714721\n",
      "train loss:0.11477611721512675\n",
      "train loss:0.07269257210145975\n",
      "train loss:0.12784391395834954\n",
      "train loss:0.08704998245160597\n",
      "train loss:0.07502920988213861\n",
      "train loss:0.0522374925882632\n",
      "train loss:0.19354653824150145\n",
      "train loss:0.0955998036855501\n",
      "train loss:0.09829870228246193\n",
      "train loss:0.0878558186740828\n",
      "train loss:0.12337762412743596\n",
      "train loss:0.12460044683387325\n",
      "train loss:0.2062558945745888\n",
      "train loss:0.031065797089028586\n",
      "train loss:0.14696608195140304\n",
      "train loss:0.21527639876850418\n",
      "train loss:0.04111576003422629\n",
      "train loss:0.09108396922461454\n",
      "train loss:0.12443746121491714\n",
      "train loss:0.07607020864032629\n",
      "train loss:0.13346767503113596\n",
      "train loss:0.08572186062816782\n",
      "train loss:0.04248132928753705\n",
      "train loss:0.11387952478618861\n",
      "train loss:0.11105432597006869\n",
      "train loss:0.22632645339738783\n",
      "train loss:0.06549906161217309\n",
      "train loss:0.09150337927156116\n",
      "train loss:0.10707489268803952\n",
      "train loss:0.12999351886792052\n",
      "train loss:0.07536770529009977\n",
      "train loss:0.08105951926482444\n",
      "train loss:0.0892906755648456\n",
      "train loss:0.11978894674459967\n",
      "train loss:0.12090968631540065\n",
      "train loss:0.06210567777918866\n",
      "train loss:0.1359770239064676\n",
      "train loss:0.09390978987099982\n",
      "train loss:0.1301651161080434\n",
      "train loss:0.09604147636637844\n",
      "train loss:0.10407154232267299\n",
      "train loss:0.10161004573375985\n",
      "train loss:0.16048851474940548\n",
      "train loss:0.06361617632155891\n",
      "train loss:0.07580125453483876\n",
      "train loss:0.12547408616943265\n",
      "train loss:0.12590181354651972\n",
      "train loss:0.09456839014201152\n",
      "train loss:0.04277939332147837\n",
      "train loss:0.09502600889495748\n",
      "train loss:0.1465041878481136\n",
      "train loss:0.023564880572320007\n",
      "train loss:0.10976691037871063\n",
      "train loss:0.09065432388705663\n",
      "train loss:0.07291675659564781\n",
      "train loss:0.1924697711644326\n",
      "train loss:0.07692015947982916\n",
      "train loss:0.1518931891101917\n",
      "train loss:0.06906286404479577\n",
      "train loss:0.06858465807042466\n",
      "train loss:0.14977084278314343\n",
      "train loss:0.09225396955713873\n",
      "train loss:0.09784553347752074\n",
      "train loss:0.08121869791129858\n",
      "train loss:0.09290553107259893\n",
      "train loss:0.13320773885149204\n",
      "train loss:0.08600160698604292\n",
      "train loss:0.13631098442470005\n",
      "train loss:0.10825119286050677\n",
      "train loss:0.14500108133467474\n",
      "train loss:0.06282456106406148\n",
      "train loss:0.05838710604126015\n",
      "train loss:0.12207351614889678\n",
      "train loss:0.17761948258962043\n",
      "train loss:0.08489535821225555\n",
      "train loss:0.06531809435953534\n",
      "train loss:0.1880108258647639\n",
      "train loss:0.07635533985918569\n",
      "train loss:0.1055118622168563\n",
      "train loss:0.08507051083921148\n",
      "train loss:0.030184937822868844\n",
      "train loss:0.05568454570536994\n",
      "train loss:0.09875460081339485\n",
      "train loss:0.09361577532995222\n",
      "train loss:0.09778036268200477\n",
      "train loss:0.10172938135331291\n",
      "train loss:0.1447686354865154\n",
      "train loss:0.08953793230836662\n",
      "train loss:0.08321445894811964\n",
      "train loss:0.11779912040822021\n",
      "train loss:0.0705220262002714\n",
      "train loss:0.0617682925447448\n",
      "train loss:0.07616867100171469\n",
      "train loss:0.10235858315672956\n",
      "train loss:0.039613799556345246\n",
      "train loss:0.06344803737017289\n",
      "train loss:0.09489327030773781\n",
      "train loss:0.14602271228914065\n",
      "train loss:0.07598423362365789\n",
      "train loss:0.15503977977048536\n",
      "train loss:0.11467957530767528\n",
      "train loss:0.07997889111652444\n",
      "train loss:0.04925962382003894\n",
      "train loss:0.0720547728292108\n",
      "train loss:0.03122685013423822\n",
      "train loss:0.19406207678626752\n",
      "train loss:0.06911055195512024\n",
      "train loss:0.1438517176362536\n",
      "train loss:0.07548375403917652\n",
      "train loss:0.10214816700728319\n",
      "train loss:0.02712505447354034\n",
      "train loss:0.08227232225453003\n",
      "train loss:0.18347937249505356\n",
      "train loss:0.06620141309049604\n",
      "train loss:0.08540589616162006\n",
      "train loss:0.10060223081607518\n",
      "train loss:0.045945866892382003\n",
      "train loss:0.0848573728496057\n",
      "train loss:0.11007362407965174\n",
      "train loss:0.0746756010043355\n",
      "train loss:0.11751577702424504\n",
      "train loss:0.030863523973774832\n",
      "train loss:0.060633681721252594\n",
      "train loss:0.12316283399212563\n",
      "train loss:0.0813644069980856\n",
      "train loss:0.07374177524345867\n",
      "train loss:0.07658149925305115\n",
      "train loss:0.06987765725261769\n",
      "train loss:0.1355080005668158\n",
      "train loss:0.11541360819460823\n",
      "train loss:0.033169841413050155\n",
      "train loss:0.048950162550068566\n",
      "train loss:0.06398671298756929\n",
      "train loss:0.06572463093939912\n",
      "train loss:0.08031946705249277\n",
      "train loss:0.06515110782042582\n",
      "train loss:0.08328726961736732\n",
      "train loss:0.09891584139444191\n",
      "train loss:0.04383072940136506\n",
      "train loss:0.05019743411209194\n",
      "train loss:0.06996831133320468\n",
      "train loss:0.13459447333768274\n",
      "train loss:0.09590747970564159\n",
      "train loss:0.09672108066158139\n",
      "train loss:0.05301014091062856\n",
      "train loss:0.06710302736205614\n",
      "train loss:0.09214789074247781\n",
      "train loss:0.08259696258070072\n",
      "train loss:0.09443086405278907\n",
      "train loss:0.11584898532438219\n",
      "train loss:0.09125265481374364\n",
      "train loss:0.2163063123966538\n",
      "train loss:0.15361387454887787\n",
      "train loss:0.12988907553543977\n",
      "train loss:0.04992727138087342\n",
      "train loss:0.20216243116202473\n",
      "train loss:0.0998446721101618\n",
      "train loss:0.13958307965023095\n",
      "train loss:0.0816522735447169\n",
      "train loss:0.07668907621677526\n",
      "train loss:0.09230407614007943\n",
      "train loss:0.12265734670827431\n",
      "train loss:0.10748505008137599\n",
      "train loss:0.07107946598884672\n",
      "train loss:0.10190850006757954\n",
      "train loss:0.028967557886219058\n",
      "train loss:0.12262077175189934\n",
      "train loss:0.034435036571335266\n",
      "train loss:0.02372738064164791\n",
      "train loss:0.06601177480699322\n",
      "train loss:0.04332758067724925\n",
      "train loss:0.10060075012751245\n",
      "train loss:0.05081929798258306\n",
      "train loss:0.03831746898257226\n",
      "train loss:0.13445892475496452\n",
      "train loss:0.03959328739911601\n",
      "train loss:0.051702623081512994\n",
      "train loss:0.032019640370531746\n",
      "train loss:0.04009145442227747\n",
      "train loss:0.07948525290820917\n",
      "train loss:0.0244103362489558\n",
      "train loss:0.049770223274408894\n",
      "train loss:0.13957957035145263\n",
      "train loss:0.09024325402097652\n",
      "train loss:0.03060590969444824\n",
      "train loss:0.1184815138376656\n",
      "train loss:0.04713035755013971\n",
      "train loss:0.19914106225569\n",
      "train loss:0.11623560522375326\n",
      "train loss:0.08185457105342263\n",
      "train loss:0.05142757982960098\n",
      "train loss:0.056093991803447596\n",
      "train loss:0.08467232287775678\n",
      "train loss:0.11229630594502758\n",
      "train loss:0.08584258047569725\n",
      "train loss:0.07508724047418816\n",
      "train loss:0.09491542351420053\n",
      "train loss:0.09715427698093303\n",
      "train loss:0.045890040965183336\n",
      "train loss:0.04639383399024258\n",
      "train loss:0.09571377236005907\n",
      "train loss:0.13193432117722714\n",
      "train loss:0.10284833370401485\n",
      "train loss:0.131908214025666\n",
      "train loss:0.028019114536354778\n",
      "train loss:0.12226205418485515\n",
      "train loss:0.09457458338766704\n",
      "train loss:0.0504545099175737\n",
      "train loss:0.0772157931701249\n",
      "train loss:0.16660311013136028\n",
      "train loss:0.0859532180672457\n",
      "train loss:0.06533264643991991\n",
      "train loss:0.03653261877064586\n",
      "train loss:0.11321533290149931\n",
      "train loss:0.04157066395286962\n",
      "train loss:0.07011641559108125\n",
      "train loss:0.12928080614974255\n",
      "train loss:0.10735444541388643\n",
      "train loss:0.04513140290555176\n",
      "train loss:0.10785757883155696\n",
      "train loss:0.04038799409681257\n",
      "train loss:0.06354134686234271\n",
      "train loss:0.03700352056545094\n",
      "train loss:0.052612090653309004\n",
      "train loss:0.11629960257972508\n",
      "train loss:0.12164288018165531\n",
      "train loss:0.07620627167136097\n",
      "train loss:0.058167103216550504\n",
      "train loss:0.1313858952695462\n",
      "train loss:0.07976093387271407\n",
      "train loss:0.1174402569325112\n",
      "train loss:0.05042841311199882\n",
      "train loss:0.08408842531837352\n",
      "train loss:0.052583044833982885\n",
      "train loss:0.09007037781955857\n",
      "train loss:0.23514188250758\n",
      "train loss:0.07994817870413812\n",
      "train loss:0.11278642739264295\n",
      "train loss:0.0983550198256517\n",
      "train loss:0.13451486017818973\n",
      "train loss:0.07041655145351668\n",
      "train loss:0.03924659833456803\n",
      "train loss:0.07843082071843321\n",
      "train loss:0.0645256751051997\n",
      "train loss:0.04859713091047337\n",
      "train loss:0.08524156183177464\n",
      "train loss:0.14049014395770726\n",
      "train loss:0.1883391022032075\n",
      "train loss:0.16377440910060986\n",
      "train loss:0.09001921344257818\n",
      "train loss:0.08396901913149232\n",
      "train loss:0.09850067896054718\n",
      "train loss:0.03492483201392458\n",
      "train loss:0.09792062887344422\n",
      "train loss:0.042892835878852294\n",
      "train loss:0.14097018140758424\n",
      "train loss:0.08084446964112554\n",
      "train loss:0.167494218758723\n",
      "train loss:0.05779313434067271\n",
      "train loss:0.1769831572395933\n",
      "train loss:0.09791090037940815\n",
      "train loss:0.10353450127223321\n",
      "train loss:0.03847621954639322\n",
      "train loss:0.13354054350267744\n",
      "train loss:0.09888078771252548\n",
      "train loss:0.07474064543411653\n",
      "train loss:0.10323166503723408\n",
      "train loss:0.06922511413846498\n",
      "train loss:0.09255598290724111\n",
      "train loss:0.10164391636706904\n",
      "train loss:0.08469813031111634\n",
      "train loss:0.03861417663808392\n",
      "train loss:0.04004266733481444\n",
      "train loss:0.10999179254852898\n",
      "train loss:0.09247598539266236\n",
      "train loss:0.09637631649169197\n",
      "train loss:0.08406418308168602\n",
      "train loss:0.1028859654490907\n",
      "train loss:0.046291767970967734\n",
      "train loss:0.06613020056466312\n",
      "train loss:0.050117008494543286\n",
      "train loss:0.13681369422765866\n",
      "train loss:0.07646797480138097\n",
      "train loss:0.09312780672934684\n",
      "train loss:0.06560796373777564\n",
      "train loss:0.10992655661061916\n",
      "train loss:0.03936902851224531\n",
      "train loss:0.03692187528880678\n",
      "train loss:0.07622656369409951\n",
      "train loss:0.06997786144667427\n",
      "train loss:0.11296809615057364\n",
      "train loss:0.04286883866429204\n",
      "train loss:0.15064068008240977\n",
      "train loss:0.06179746096818398\n",
      "train loss:0.06712534042952431\n",
      "train loss:0.057562911588928095\n",
      "train loss:0.037446452215042954\n",
      "train loss:0.05739193388071864\n",
      "train loss:0.08461228373857357\n",
      "train loss:0.07588701833570942\n",
      "train loss:0.06582102303076726\n",
      "train loss:0.06993669354924602\n",
      "train loss:0.12015561641231443\n",
      "train loss:0.051219499857890156\n",
      "train loss:0.040156992210106185\n",
      "train loss:0.0736898274438224\n",
      "train loss:0.1126029377976769\n",
      "train loss:0.0410093501570211\n",
      "train loss:0.06353040643243528\n",
      "train loss:0.11241777766612032\n",
      "train loss:0.10855199509582537\n",
      "train loss:0.0850311556842933\n",
      "train loss:0.12736648372940343\n",
      "train loss:0.034299049549292665\n",
      "train loss:0.05055504636530791\n",
      "train loss:0.12848788794901392\n",
      "train loss:0.07290360769045601\n",
      "train loss:0.04930607669286964\n",
      "train loss:0.07599578950956079\n",
      "train loss:0.09891426661316231\n",
      "train loss:0.0612649409574038\n",
      "train loss:0.10028681479668888\n",
      "train loss:0.0894421551338404\n",
      "train loss:0.08735509322738821\n",
      "train loss:0.0636871296607158\n",
      "train loss:0.04278789016265158\n",
      "train loss:0.06030694923468789\n",
      "train loss:0.05878125078682056\n",
      "train loss:0.05369126245362259\n",
      "train loss:0.030100417581579\n",
      "train loss:0.09565675266647743\n",
      "train loss:0.03691556349185794\n",
      "train loss:0.05411221137832202\n",
      "train loss:0.08652381668361497\n",
      "train loss:0.030896412506531395\n",
      "train loss:0.13727381171054248\n",
      "train loss:0.033403023830636\n",
      "train loss:0.16026287509504425\n",
      "train loss:0.040465949239563974\n",
      "train loss:0.04659630301944654\n",
      "train loss:0.11917682024139074\n",
      "train loss:0.03213307637244063\n",
      "train loss:0.026546725396371124\n",
      "train loss:0.040898275346464265\n",
      "train loss:0.10717059591628944\n",
      "train loss:0.03883281028265529\n",
      "train loss:0.03514142926837341\n",
      "train loss:0.030486478072812493\n",
      "train loss:0.1341756083239001\n",
      "train loss:0.058349452652958475\n",
      "train loss:0.09373360780481922\n",
      "train loss:0.03880602588182555\n",
      "train loss:0.06544079978359829\n",
      "train loss:0.06958731172278652\n",
      "train loss:0.06835912001696866\n",
      "train loss:0.11779946781016529\n",
      "train loss:0.03417139755238714\n",
      "train loss:0.042337622403776906\n",
      "train loss:0.08265039309842147\n",
      "train loss:0.06408709302005738\n",
      "train loss:0.0922560035835895\n",
      "train loss:0.09961272256991274\n",
      "train loss:0.03229745799222632\n",
      "train loss:0.110266368452145\n",
      "train loss:0.05564457338831966\n",
      "train loss:0.09085215380104827\n",
      "train loss:0.06321823917585116\n",
      "train loss:0.10571331368034598\n",
      "train loss:0.13560673170258583\n",
      "train loss:0.08448564431186244\n",
      "train loss:0.05256244038302078\n",
      "train loss:0.01838952242253219\n",
      "train loss:0.08595226211737238\n",
      "train loss:0.05694148818948532\n",
      "train loss:0.09263325828346339\n",
      "train loss:0.09155519060103331\n",
      "train loss:0.03847700174044641\n",
      "train loss:0.0777912854617298\n",
      "train loss:0.11174701290787778\n",
      "train loss:0.06768787576136408\n",
      "train loss:0.020224435570742987\n",
      "train loss:0.03951626243828339\n",
      "train loss:0.054992949353697786\n",
      "train loss:0.04995115378624096\n",
      "train loss:0.07669242883256222\n",
      "train loss:0.06961504828972967\n",
      "train loss:0.04141047414916479\n",
      "train loss:0.10976595608865038\n",
      "train loss:0.053353283875625566\n",
      "train loss:0.07216864476740928\n",
      "train loss:0.05322096458504939\n",
      "train loss:0.07002281947165077\n",
      "train loss:0.04469840140436143\n",
      "train loss:0.04749286724115968\n",
      "train loss:0.018585017828192097\n",
      "train loss:0.052502986287014775\n",
      "train loss:0.031624661384634614\n",
      "train loss:0.10900427450949389\n",
      "train loss:0.03388058125588249\n",
      "train loss:0.022660653576603688\n",
      "train loss:0.09220112153970791\n",
      "train loss:0.08423716995265863\n",
      "train loss:0.06278723941612392\n",
      "train loss:0.08108590713960463\n",
      "train loss:0.05827879515878145\n",
      "train loss:0.10332910455552263\n",
      "train loss:0.09127392953563909\n",
      "train loss:0.052254015549731754\n",
      "train loss:0.048203185905596656\n",
      "train loss:0.05915879043649885\n",
      "train loss:0.09001539341317887\n",
      "train loss:0.037289803768969405\n",
      "train loss:0.030098093446093342\n",
      "train loss:0.04040716322956896\n",
      "train loss:0.019731206615393838\n",
      "train loss:0.05106251035169628\n",
      "train loss:0.023101828322678694\n",
      "train loss:0.06954713749431361\n",
      "train loss:0.04326648767074763\n",
      "train loss:0.07456889676711591\n",
      "train loss:0.09365263409797576\n",
      "train loss:0.0655790109018284\n",
      "train loss:0.04149456004693087\n",
      "train loss:0.11067654077350357\n",
      "train loss:0.04143468265082695\n",
      "train loss:0.07742685708401169\n",
      "train loss:0.04468402180498403\n",
      "train loss:0.017674688270073048\n",
      "train loss:0.04001741893761582\n",
      "train loss:0.12912701906683893\n",
      "train loss:0.04274221341156891\n",
      "train loss:0.026825225998503777\n",
      "train loss:0.08484874169802908\n",
      "train loss:0.0786484543766524\n",
      "train loss:0.11169061849264159\n",
      "train loss:0.11389339986638777\n",
      "train loss:0.016721284397349167\n",
      "train loss:0.05843442082302672\n",
      "train loss:0.06150722879245331\n",
      "train loss:0.08297215858114328\n",
      "train loss:0.03290056190712456\n",
      "train loss:0.0832650364593221\n",
      "train loss:0.06679690376612117\n",
      "train loss:0.08967355200463062\n",
      "train loss:0.0655341483778532\n",
      "train loss:0.08795469031800053\n",
      "train loss:0.07379937498455282\n",
      "train loss:0.0750136927056146\n",
      "train loss:0.032148172662933065\n",
      "train loss:0.030006090013306164\n",
      "train loss:0.0462183431557364\n",
      "train loss:0.06302407922265034\n",
      "train loss:0.012479359019061713\n",
      "train loss:0.05606065889503543\n",
      "train loss:0.08098474489369664\n",
      "train loss:0.056883960014868326\n",
      "train loss:0.057151471418785106\n",
      "train loss:0.03500736872670184\n",
      "train loss:0.027287483105154782\n",
      "train loss:0.08472781151485403\n",
      "train loss:0.04006605226020856\n",
      "train loss:0.0949202136498359\n",
      "train loss:0.020763107869986516\n",
      "train loss:0.07030681209782602\n",
      "train loss:0.07278353597498212\n",
      "train loss:0.03311186065379081\n",
      "train loss:0.07837657388819724\n",
      "train loss:0.027253691238894154\n",
      "train loss:0.06734465623799472\n",
      "train loss:0.02193380188285373\n",
      "train loss:0.03612327990270401\n",
      "train loss:0.06103901465559293\n",
      "train loss:0.029225588415341393\n",
      "train loss:0.08095938903030138\n",
      "train loss:0.0652526780207468\n",
      "train loss:0.06717320780893095\n",
      "train loss:0.051357644251152926\n",
      "train loss:0.06901360929264443\n",
      "train loss:0.05293499296844835\n",
      "train loss:0.036343182358420364\n",
      "train loss:0.05025286325055696\n",
      "train loss:0.03438092361816225\n",
      "train loss:0.13029802595003218\n",
      "train loss:0.06560846427562139\n",
      "train loss:0.06700439227598624\n",
      "train loss:0.031318204977913794\n",
      "train loss:0.1339521580075719\n",
      "train loss:0.0660846578206998\n",
      "train loss:0.07072987610341558\n",
      "train loss:0.11341168219496307\n",
      "train loss:0.12508689098563772\n",
      "train loss:0.020961585148970906\n",
      "train loss:0.06124359743822039\n",
      "train loss:0.01689415442881406\n",
      "train loss:0.20911804044001248\n",
      "train loss:0.03499295031542485\n",
      "train loss:0.05975366046643781\n",
      "train loss:0.06345929746364208\n",
      "train loss:0.04237264274497217\n",
      "train loss:0.06856019503930072\n",
      "train loss:0.032773319282043044\n",
      "train loss:0.024641073903769507\n",
      "train loss:0.04410229880958519\n",
      "train loss:0.07244844949010704\n",
      "train loss:0.09443658531631234\n",
      "train loss:0.021898984436347337\n",
      "train loss:0.03669251382924421\n",
      "train loss:0.12392608794034154\n",
      "train loss:0.12200761641100465\n",
      "train loss:0.048996101921161654\n",
      "train loss:0.04788703087873219\n",
      "train loss:0.041780381184541346\n",
      "train loss:0.05950290769096504\n",
      "train loss:0.052638879365958596\n",
      "train loss:0.10489870905490462\n",
      "train loss:0.04991973392026081\n",
      "train loss:0.08921262866658788\n",
      "train loss:0.08155118535568599\n",
      "train loss:0.0346158026463999\n",
      "train loss:0.052234992779395605\n",
      "train loss:0.08701014987576057\n",
      "train loss:0.05146546324501788\n",
      "train loss:0.0256579849091652\n",
      "train loss:0.08823050584260583\n",
      "train loss:0.04777576960427994\n",
      "train loss:0.07323345288241405\n",
      "train loss:0.042024245324265984\n",
      "train loss:0.11216376345380523\n",
      "train loss:0.07210084328843654\n",
      "train loss:0.06323914390512751\n",
      "train loss:0.08276135264006962\n",
      "train loss:0.07844672006408636\n",
      "train loss:0.08003989305877152\n",
      "train loss:0.04074263878268335\n",
      "train loss:0.029120760014666982\n",
      "train loss:0.07653321584186322\n",
      "train loss:0.012823777474341483\n",
      "train loss:0.05928791084403927\n",
      "train loss:0.10215846113015348\n",
      "train loss:0.09506826195056411\n",
      "train loss:0.052361566923257295\n",
      "train loss:0.02574333960476016\n",
      "train loss:0.04632347860983239\n",
      "train loss:0.06398761820151852\n",
      "train loss:0.08887391390684826\n",
      "train loss:0.12976308782344345\n",
      "train loss:0.037730395221578596\n",
      "train loss:0.028069163173129765\n",
      "train loss:0.018819163189416288\n",
      "train loss:0.0830855783483713\n",
      "train loss:0.2134827982064234\n",
      "train loss:0.08325926203306352\n",
      "train loss:0.02289094150686568\n",
      "train loss:0.03211393723004938\n",
      "train loss:0.15161692334973342\n",
      "train loss:0.027161772733475028\n",
      "train loss:0.05996579672919888\n",
      "train loss:0.019710728270783767\n",
      "train loss:0.11329473671699603\n",
      "=== epoch:3, train acc:0.975, test acc:0.975 ===\n",
      "train loss:0.11024283895252525\n",
      "train loss:0.03944199678954832\n",
      "train loss:0.042142972164224364\n",
      "train loss:0.07787392518976728\n",
      "train loss:0.02413876202051495\n",
      "train loss:0.11980649761258301\n",
      "train loss:0.03689585092061725\n",
      "train loss:0.017455856033196316\n",
      "train loss:0.0791234116031013\n",
      "train loss:0.1753331826236257\n",
      "train loss:0.05984500013786465\n",
      "train loss:0.08927328968281541\n",
      "train loss:0.08677770314570334\n",
      "train loss:0.02483354428349613\n",
      "train loss:0.05511076871100151\n",
      "train loss:0.05167363924050019\n",
      "train loss:0.03254812834992195\n",
      "train loss:0.05141968615613383\n",
      "train loss:0.04315597851391243\n",
      "train loss:0.032496341341446286\n",
      "train loss:0.06653199358346353\n",
      "train loss:0.02751474792741893\n",
      "train loss:0.024690815998056875\n",
      "train loss:0.04190603211178938\n",
      "train loss:0.02148522567802212\n",
      "train loss:0.025385529203497948\n",
      "train loss:0.08957430987196008\n",
      "train loss:0.06623304807190382\n",
      "train loss:0.023343685228923244\n",
      "train loss:0.056647840657254596\n",
      "train loss:0.13010946793336114\n",
      "train loss:0.04591365550311608\n",
      "train loss:0.040076452507572656\n",
      "train loss:0.03810059290618521\n",
      "train loss:0.04320949303597223\n",
      "train loss:0.07143354527940109\n",
      "train loss:0.016411081993872086\n",
      "train loss:0.02792594055292246\n",
      "train loss:0.048229611070298634\n",
      "train loss:0.033025876585221624\n",
      "train loss:0.11037813797225518\n",
      "train loss:0.03355738619497979\n",
      "train loss:0.08161354318702171\n",
      "train loss:0.14830741482302529\n",
      "train loss:0.1285139984385365\n",
      "train loss:0.027175764169891724\n",
      "train loss:0.045188400284113424\n",
      "train loss:0.0263871429710792\n",
      "train loss:0.04931355495663572\n",
      "train loss:0.05528063067557917\n",
      "train loss:0.0950785078867166\n",
      "train loss:0.052853801326115606\n",
      "train loss:0.013592817636949668\n",
      "train loss:0.04821070290090064\n",
      "train loss:0.07676588275156532\n",
      "train loss:0.06458167389816455\n",
      "train loss:0.05715718554716172\n",
      "train loss:0.02420277349824897\n",
      "train loss:0.03523003626345609\n",
      "train loss:0.05940261244593215\n",
      "train loss:0.030286192595766436\n",
      "train loss:0.11111920183971241\n",
      "train loss:0.15071968488849688\n",
      "train loss:0.06997713551441448\n",
      "train loss:0.028999039847354063\n",
      "train loss:0.0731748030042612\n",
      "train loss:0.076800268787844\n",
      "train loss:0.027154117181383587\n",
      "train loss:0.029495490928885662\n",
      "train loss:0.04575705544391135\n",
      "train loss:0.045276771906503706\n",
      "train loss:0.045826286302540514\n",
      "train loss:0.0821921440892874\n",
      "train loss:0.06210052283449685\n",
      "train loss:0.052226640316513605\n",
      "train loss:0.05709059341202966\n",
      "train loss:0.020774104733079023\n",
      "train loss:0.023119031097690673\n",
      "train loss:0.031176297711237293\n",
      "train loss:0.0490756208265808\n",
      "train loss:0.06889897644391854\n",
      "train loss:0.028158861969985228\n",
      "train loss:0.08463880296778159\n",
      "train loss:0.04633550749150072\n",
      "train loss:0.04041422778223028\n",
      "train loss:0.06336462492842045\n",
      "train loss:0.06570840945419597\n",
      "train loss:0.02647124213066229\n",
      "train loss:0.06532501905938787\n",
      "train loss:0.04183640791332443\n",
      "train loss:0.023181519278348297\n",
      "train loss:0.05680592947112604\n",
      "train loss:0.02682436131560916\n",
      "train loss:0.09571868849087663\n",
      "train loss:0.12416039467581927\n",
      "train loss:0.02666206031594474\n",
      "train loss:0.073940495784288\n",
      "train loss:0.058403796626035336\n",
      "train loss:0.19423047427795595\n",
      "train loss:0.034380417272522265\n",
      "train loss:0.03780240164249034\n",
      "train loss:0.04941782226558778\n",
      "train loss:0.06866759742982104\n",
      "train loss:0.12974771338675417\n",
      "train loss:0.03815930402159853\n",
      "train loss:0.049782727945075315\n",
      "train loss:0.08968172240905009\n",
      "train loss:0.041532892027982084\n",
      "train loss:0.07912836184303786\n",
      "train loss:0.08147505709863773\n",
      "train loss:0.019958922641866748\n",
      "train loss:0.059325964956540966\n",
      "train loss:0.035519173479838136\n",
      "train loss:0.04853082992616795\n",
      "train loss:0.051012993496565476\n",
      "train loss:0.037384170821068166\n",
      "train loss:0.019541654541633414\n",
      "train loss:0.09624352662258943\n",
      "train loss:0.01761864688049197\n",
      "train loss:0.01987365888142982\n",
      "train loss:0.03356625743698579\n",
      "train loss:0.055238803767216814\n",
      "train loss:0.016287861048267446\n",
      "train loss:0.023284559223004306\n",
      "train loss:0.09807357918639514\n",
      "train loss:0.059767113754891375\n",
      "train loss:0.03154020043164704\n",
      "train loss:0.10622360083054697\n",
      "train loss:0.05342041371908639\n",
      "train loss:0.07932729190226886\n",
      "train loss:0.07678814501546277\n",
      "train loss:0.07711594676925425\n",
      "train loss:0.03341944496919276\n",
      "train loss:0.13930385480323162\n",
      "train loss:0.03207983954505801\n",
      "train loss:0.07336699760077915\n",
      "train loss:0.039470820397562154\n",
      "train loss:0.03975729095300717\n",
      "train loss:0.10189899741395271\n",
      "train loss:0.028309553420279167\n",
      "train loss:0.07196496386122346\n",
      "train loss:0.11361884484091224\n",
      "train loss:0.03096718583201666\n",
      "train loss:0.013721948645064722\n",
      "train loss:0.03048429521216132\n",
      "train loss:0.1590683849213808\n",
      "train loss:0.10913405316958577\n",
      "train loss:0.06349158619089257\n",
      "train loss:0.045379813652097256\n",
      "train loss:0.07424131236786619\n",
      "train loss:0.06548210958689275\n",
      "train loss:0.0752947107141171\n",
      "train loss:0.03938234165534619\n",
      "train loss:0.027349719741519344\n",
      "train loss:0.04385115267033522\n",
      "train loss:0.028264328240508122\n",
      "train loss:0.05853895837629509\n",
      "train loss:0.026605099598878433\n",
      "train loss:0.021943238889112884\n",
      "train loss:0.12204730436252553\n",
      "train loss:0.05335660845003962\n",
      "train loss:0.06321311532842207\n",
      "train loss:0.019773921392187362\n",
      "train loss:0.020826123935793587\n",
      "train loss:0.05363437537659078\n",
      "train loss:0.027247590717976777\n",
      "train loss:0.021463676900849448\n",
      "train loss:0.07814000049347923\n",
      "train loss:0.05963076940810036\n",
      "train loss:0.026342459016290662\n",
      "train loss:0.10256568011446633\n",
      "train loss:0.0171012829082668\n",
      "train loss:0.07816139888120055\n",
      "train loss:0.06144938439792842\n",
      "train loss:0.02899558506982768\n",
      "train loss:0.025558897439855576\n",
      "train loss:0.04304540637597621\n",
      "train loss:0.05843603704200368\n",
      "train loss:0.05101307526723575\n",
      "train loss:0.0535400847550494\n",
      "train loss:0.0902369216316745\n",
      "train loss:0.02068166845044093\n",
      "train loss:0.030805077933214582\n",
      "train loss:0.024588211259600277\n",
      "train loss:0.00962357287002405\n",
      "train loss:0.014468287214759921\n",
      "train loss:0.016077423543789444\n",
      "train loss:0.030712867283290847\n",
      "train loss:0.02604519498303673\n",
      "train loss:0.02668955928672496\n",
      "train loss:0.040110530325864646\n",
      "train loss:0.05857552543866509\n",
      "train loss:0.03786863176553977\n",
      "train loss:0.03164165515516349\n",
      "train loss:0.024398352368963653\n",
      "train loss:0.061320891918381616\n",
      "train loss:0.025602317336025898\n",
      "train loss:0.06574785615701115\n",
      "train loss:0.01414022405749838\n",
      "train loss:0.03023330174930204\n",
      "train loss:0.047746489250078426\n",
      "train loss:0.03524492607535879\n",
      "train loss:0.038525818629561226\n",
      "train loss:0.0555919027232502\n",
      "train loss:0.14492008135362203\n",
      "train loss:0.014032538215122216\n",
      "train loss:0.12017434541555407\n",
      "train loss:0.06291705831524202\n",
      "train loss:0.011279233989953216\n",
      "train loss:0.06727879346936148\n",
      "train loss:0.029477914859603844\n",
      "train loss:0.012993128166910674\n",
      "train loss:0.00915979242557085\n",
      "train loss:0.02434193291994723\n",
      "train loss:0.07370981285537448\n",
      "train loss:0.05667686256472728\n",
      "train loss:0.07472575828709728\n",
      "train loss:0.016945340837061677\n",
      "train loss:0.060454452933796594\n",
      "train loss:0.09265845711167021\n",
      "train loss:0.026824102680852523\n",
      "train loss:0.05751663453899115\n",
      "train loss:0.03453381615902095\n",
      "train loss:0.02769771701177735\n",
      "train loss:0.043424124319374674\n",
      "train loss:0.12130096992500655\n",
      "train loss:0.027278152219855\n",
      "train loss:0.10156989638636144\n",
      "train loss:0.15220231514538032\n",
      "train loss:0.0997126923559419\n",
      "train loss:0.043307328431946505\n",
      "train loss:0.026874342172686205\n",
      "train loss:0.019771225128826692\n",
      "train loss:0.06737539160035459\n",
      "train loss:0.037220280885613743\n",
      "train loss:0.042901473388728704\n",
      "train loss:0.09722491401230991\n",
      "train loss:0.017482604972607993\n",
      "train loss:0.031422268575071126\n",
      "train loss:0.01617158077058187\n",
      "train loss:0.048099017268483174\n",
      "train loss:0.05362648442905726\n",
      "train loss:0.03013417658925689\n",
      "train loss:0.03546136583620982\n",
      "train loss:0.08223214355560644\n",
      "train loss:0.04142226136133987\n",
      "train loss:0.04737165238804293\n",
      "train loss:0.011435927595469042\n",
      "train loss:0.04454909025643335\n",
      "train loss:0.036689798725112445\n",
      "train loss:0.05781926181741996\n",
      "train loss:0.06060091709294559\n",
      "train loss:0.05977603381622415\n",
      "train loss:0.21453026345849022\n",
      "train loss:0.04862232152971651\n",
      "train loss:0.024646223365839246\n",
      "train loss:0.03281640061304555\n",
      "train loss:0.01996726533361889\n",
      "train loss:0.11513100004346749\n",
      "train loss:0.04569617331422981\n",
      "train loss:0.022503392551432348\n",
      "train loss:0.08981670250597229\n",
      "train loss:0.0451600748702669\n",
      "train loss:0.04095079726087066\n",
      "train loss:0.014352931408940409\n",
      "train loss:0.03685727007258935\n",
      "train loss:0.09765206352157406\n",
      "train loss:0.03420051929260032\n",
      "train loss:0.03230600821210845\n",
      "train loss:0.07170768227117595\n",
      "train loss:0.02238894888990743\n",
      "train loss:0.01946257234734471\n",
      "train loss:0.04906161465197037\n",
      "train loss:0.12386864877559665\n",
      "train loss:0.021825542746750658\n",
      "train loss:0.0748510159068007\n",
      "train loss:0.06079754468752494\n",
      "train loss:0.03099735323243724\n",
      "train loss:0.014859824087795521\n",
      "train loss:0.011721707627071493\n",
      "train loss:0.03488018705055494\n",
      "train loss:0.03087367385808946\n",
      "train loss:0.04819172528403207\n",
      "train loss:0.027536908457421717\n",
      "train loss:0.030616142500009123\n",
      "train loss:0.03684422909962076\n",
      "train loss:0.06581414641151212\n",
      "train loss:0.020684616615100012\n",
      "train loss:0.060559294473188884\n",
      "train loss:0.03823767470147095\n",
      "train loss:0.03770647409354901\n",
      "train loss:0.08927874791505641\n",
      "train loss:0.1404017030923007\n",
      "train loss:0.07248048925775391\n",
      "train loss:0.026213795900422845\n",
      "train loss:0.06843883300865197\n",
      "train loss:0.06239962182023929\n",
      "train loss:0.12396202483341173\n",
      "train loss:0.11952182646174919\n",
      "train loss:0.06120723033769889\n",
      "train loss:0.02347668734578398\n",
      "train loss:0.021500675370256368\n",
      "train loss:0.02252008249163039\n",
      "train loss:0.08700362952370302\n",
      "train loss:0.021570125892395334\n",
      "train loss:0.24472302015085318\n",
      "train loss:0.048332255594154674\n",
      "train loss:0.02205451599454395\n",
      "train loss:0.04179256351204946\n",
      "train loss:0.03036539327875769\n",
      "train loss:0.037639129330774984\n",
      "train loss:0.030362766477900767\n",
      "train loss:0.03346325110848881\n",
      "train loss:0.056181986435401406\n",
      "train loss:0.0236582088116955\n",
      "train loss:0.021527503531074143\n",
      "train loss:0.06324771524126238\n",
      "train loss:0.0459258722001042\n",
      "train loss:0.03730094485763599\n",
      "train loss:0.056123321923311306\n",
      "train loss:0.13629846441299934\n",
      "train loss:0.011589966289480093\n",
      "train loss:0.022103788652331705\n",
      "train loss:0.009417895719591937\n",
      "train loss:0.07320215978397808\n",
      "train loss:0.024129509800323582\n",
      "train loss:0.011056516083256435\n",
      "train loss:0.051349115571869455\n",
      "train loss:0.06896741513370996\n",
      "train loss:0.012916832402555341\n",
      "train loss:0.05489858567836462\n",
      "train loss:0.018387110512411535\n",
      "train loss:0.009574052555672355\n",
      "train loss:0.04840560979501902\n",
      "train loss:0.060215182112198305\n",
      "train loss:0.033793646728841845\n",
      "train loss:0.05510951706516389\n",
      "train loss:0.11579949169946026\n",
      "train loss:0.02726117025327222\n",
      "train loss:0.05461932102903006\n",
      "train loss:0.059607013138407154\n",
      "train loss:0.02199231591245778\n",
      "train loss:0.0972711303539961\n",
      "train loss:0.033230472302534\n",
      "train loss:0.06926792198653148\n",
      "train loss:0.056696156570683186\n",
      "train loss:0.07563687049229806\n",
      "train loss:0.025646730501586893\n",
      "train loss:0.01768939703082069\n",
      "train loss:0.033703237581979746\n",
      "train loss:0.04197318377570507\n",
      "train loss:0.05404340386312126\n",
      "train loss:0.04607128464661756\n",
      "train loss:0.060610097725911534\n",
      "train loss:0.010542981554245448\n",
      "train loss:0.10927544670812135\n",
      "train loss:0.051018427321468864\n",
      "train loss:0.03493340408348042\n",
      "train loss:0.03724776377856609\n",
      "train loss:0.016442810545969227\n",
      "train loss:0.02503075278045206\n",
      "train loss:0.03593045446112266\n",
      "train loss:0.09356568875277319\n",
      "train loss:0.05567524911437403\n",
      "train loss:0.03761553276543986\n",
      "train loss:0.011210673052561054\n",
      "train loss:0.011319973390154036\n",
      "train loss:0.11585654468826302\n",
      "train loss:0.032829046217786664\n",
      "train loss:0.09067710893369473\n",
      "train loss:0.03455972337026368\n",
      "train loss:0.043874686332427026\n",
      "train loss:0.042761219700800594\n",
      "train loss:0.06045916964017524\n",
      "train loss:0.01125522341416177\n",
      "train loss:0.048381187079281934\n",
      "train loss:0.019114367907792352\n",
      "train loss:0.051527169325936546\n",
      "train loss:0.01184266686668543\n",
      "train loss:0.04014237302730448\n",
      "train loss:0.08450800834810705\n",
      "train loss:0.05113604968833361\n",
      "train loss:0.007906212478564646\n",
      "train loss:0.08442388868923058\n",
      "train loss:0.010054044810571608\n",
      "train loss:0.04116259188407548\n",
      "train loss:0.05341070015103245\n",
      "train loss:0.032333083391973154\n",
      "train loss:0.047767689126506314\n",
      "train loss:0.03633840869097626\n",
      "train loss:0.005142762027357755\n",
      "train loss:0.07119756928602215\n",
      "train loss:0.07966040189747273\n",
      "train loss:0.08136472501510911\n",
      "train loss:0.12855472977645918\n",
      "train loss:0.0206118427098928\n",
      "train loss:0.0772084850346301\n",
      "train loss:0.06673867286062021\n",
      "train loss:0.021694675270937144\n",
      "train loss:0.027617355714270854\n",
      "train loss:0.11230714978421583\n",
      "train loss:0.09780782427916054\n",
      "train loss:0.03972824681971282\n",
      "train loss:0.030147668800780535\n",
      "train loss:0.01311547236759586\n",
      "train loss:0.015162519946689285\n",
      "train loss:0.056282068303854176\n",
      "train loss:0.03760224734082021\n",
      "train loss:0.038902641844685025\n",
      "train loss:0.07741549853219426\n",
      "train loss:0.01179593591422581\n",
      "train loss:0.057984136576616246\n",
      "train loss:0.0617349573832744\n",
      "train loss:0.042029846180248034\n",
      "train loss:0.035832683801360884\n",
      "train loss:0.01843691659709064\n",
      "train loss:0.017130977786768215\n",
      "train loss:0.07046762769632423\n",
      "train loss:0.004667528275008698\n",
      "train loss:0.06760678110377798\n",
      "train loss:0.07388278643641369\n",
      "train loss:0.10175875835365022\n",
      "train loss:0.02108190006542912\n",
      "train loss:0.01392984606274016\n",
      "train loss:0.017787425572954593\n",
      "train loss:0.06939327434692573\n",
      "train loss:0.05042595600448751\n",
      "train loss:0.019270779150504534\n",
      "train loss:0.03458916677706202\n",
      "train loss:0.07451002451969779\n",
      "train loss:0.01885875941272796\n",
      "train loss:0.16373248243400698\n",
      "train loss:0.06411928482312595\n",
      "train loss:0.049093258164460966\n",
      "train loss:0.060606554521586704\n",
      "train loss:0.028844644676140386\n",
      "train loss:0.007950955306294302\n",
      "train loss:0.07247144464938299\n",
      "train loss:0.02868850732479092\n",
      "train loss:0.01379078460069297\n",
      "train loss:0.023405633117820877\n",
      "train loss:0.018401476475459456\n",
      "train loss:0.022642157669756618\n",
      "train loss:0.03550889360242734\n",
      "train loss:0.09917585814798761\n",
      "train loss:0.014489659127784181\n",
      "train loss:0.0034820552300534645\n",
      "train loss:0.05276930314275001\n",
      "train loss:0.03888048684626188\n",
      "train loss:0.05621300569602802\n",
      "train loss:0.08864256157059205\n",
      "train loss:0.01877962934213662\n",
      "train loss:0.009926548033465108\n",
      "train loss:0.022765357320762382\n",
      "train loss:0.04852572199522855\n",
      "train loss:0.07021321026852013\n",
      "train loss:0.054665837637443364\n",
      "train loss:0.059671613372010324\n",
      "train loss:0.051696302934293524\n",
      "train loss:0.0211083445951992\n",
      "train loss:0.029871747773178706\n",
      "train loss:0.07421170817288884\n",
      "train loss:0.015310920776314336\n",
      "train loss:0.07218676134867158\n",
      "train loss:0.018314440838132943\n",
      "train loss:0.01370097934712783\n",
      "train loss:0.04042207283352652\n",
      "train loss:0.1390871306281462\n",
      "train loss:0.028577767203929495\n",
      "train loss:0.05102139516137918\n",
      "train loss:0.020830915717913682\n",
      "train loss:0.045446151014502965\n",
      "train loss:0.04507676843436816\n",
      "train loss:0.018556863428189824\n",
      "train loss:0.021956579820301657\n",
      "train loss:0.08016337890413842\n",
      "train loss:0.022014523799891043\n",
      "train loss:0.029775816190471267\n",
      "train loss:0.022882273418017837\n",
      "train loss:0.018944797093069392\n",
      "train loss:0.01791161216839096\n",
      "train loss:0.02923850587188753\n",
      "train loss:0.03919972770939603\n",
      "train loss:0.07133198546160671\n",
      "train loss:0.04565953925507707\n",
      "train loss:0.048100654735583\n",
      "train loss:0.03671169746195806\n",
      "train loss:0.012224126956750625\n",
      "train loss:0.026902725352648857\n",
      "train loss:0.04479051959686966\n",
      "train loss:0.037013673810551755\n",
      "train loss:0.01834934614348357\n",
      "train loss:0.10393286769393367\n",
      "train loss:0.005073236012439574\n",
      "train loss:0.019162261675312188\n",
      "train loss:0.07604278757484481\n",
      "train loss:0.026094098460729184\n",
      "train loss:0.012230318976902322\n",
      "train loss:0.023217125464666828\n",
      "train loss:0.0723233411876453\n",
      "train loss:0.022847997864221602\n",
      "train loss:0.014015147995305224\n",
      "train loss:0.024837489798157905\n",
      "train loss:0.04454025344259675\n",
      "train loss:0.05422230618243813\n",
      "train loss:0.030884982839466245\n",
      "train loss:0.05263698787940803\n",
      "train loss:0.03441838936716698\n",
      "train loss:0.08702252208337988\n",
      "train loss:0.030236157617481774\n",
      "train loss:0.02796298065235354\n",
      "train loss:0.06024258021896241\n",
      "train loss:0.06492071243889233\n",
      "train loss:0.017496606780911944\n",
      "train loss:0.052023915841859204\n",
      "train loss:0.021973725691413707\n",
      "train loss:0.059183638197570894\n",
      "train loss:0.08004283064851471\n",
      "train loss:0.02897210937137662\n",
      "train loss:0.03451285247445049\n",
      "train loss:0.01875579903459799\n",
      "train loss:0.015619691609030317\n",
      "train loss:0.043606461436462386\n",
      "train loss:0.041318986044716566\n",
      "train loss:0.040602737327882533\n",
      "train loss:0.04076743618647535\n",
      "train loss:0.0940389533994455\n",
      "train loss:0.07228304755204844\n",
      "train loss:0.12225536760004486\n",
      "train loss:0.04149232556706456\n",
      "train loss:0.1573139550764159\n",
      "train loss:0.01931621285856231\n",
      "train loss:0.00937780936685756\n",
      "train loss:0.09713579689357024\n",
      "train loss:0.04752694255797727\n",
      "train loss:0.009453025520846804\n",
      "train loss:0.021518471907086726\n",
      "train loss:0.07606641416462741\n",
      "train loss:0.032757179253419894\n",
      "train loss:0.025514757789632306\n",
      "train loss:0.05426358336209229\n",
      "train loss:0.027008760684457032\n",
      "train loss:0.10173117622914933\n",
      "train loss:0.017085485781005886\n",
      "train loss:0.010897410819702675\n",
      "train loss:0.026520830324718716\n",
      "train loss:0.014385398702924412\n",
      "train loss:0.009634529668911696\n",
      "train loss:0.009824169992317021\n",
      "train loss:0.06852445377339224\n",
      "train loss:0.012583070348598693\n",
      "train loss:0.13124389987528992\n",
      "train loss:0.04425274797823644\n",
      "train loss:0.10903181032302144\n",
      "train loss:0.05868888316134531\n",
      "train loss:0.0301651236251668\n",
      "train loss:0.009488480674144715\n",
      "train loss:0.007090638468613681\n",
      "train loss:0.021876899472346584\n",
      "train loss:0.07262735918158288\n",
      "train loss:0.03793596976184748\n",
      "train loss:0.057195528478573\n",
      "train loss:0.03151968941755502\n",
      "train loss:0.07101208632571428\n",
      "train loss:0.039901850303312864\n",
      "train loss:0.00818732439191762\n",
      "train loss:0.017827646981151378\n",
      "train loss:0.04736627355127764\n",
      "train loss:0.047316803546560766\n",
      "train loss:0.051935863333072076\n",
      "train loss:0.02178635947555136\n",
      "train loss:0.026254249723481372\n",
      "train loss:0.07094097099248196\n",
      "train loss:0.031727331309942514\n",
      "train loss:0.04232070030147104\n",
      "train loss:0.04829334793839994\n",
      "train loss:0.036234732085863766\n",
      "train loss:0.03364546957493386\n",
      "train loss:0.047116654199284146\n",
      "train loss:0.04405533303263288\n",
      "train loss:0.07116358953976688\n",
      "train loss:0.03701087383429638\n",
      "train loss:0.041167540088064204\n",
      "train loss:0.008599821577669885\n",
      "train loss:0.019711650479115987\n",
      "train loss:0.05167147855181962\n",
      "train loss:0.05240025842261939\n",
      "train loss:0.011622586295049444\n",
      "train loss:0.11991539547147903\n",
      "train loss:0.07476908463641492\n",
      "train loss:0.03244535474230159\n",
      "train loss:0.1048935089661744\n",
      "train loss:0.026714166515863377\n",
      "train loss:0.030274006824960124\n",
      "train loss:0.07989800985380337\n",
      "train loss:0.04661628121707394\n",
      "train loss:0.1203467139793066\n",
      "train loss:0.060114550616934645\n",
      "train loss:0.017853035327735255\n",
      "train loss:0.03037469833315269\n",
      "=== epoch:4, train acc:0.985, test acc:0.983 ===\n",
      "train loss:0.043240093802525595\n",
      "train loss:0.03032009310740824\n",
      "train loss:0.022478111926252386\n",
      "train loss:0.05386982630397119\n",
      "train loss:0.016384398010182363\n",
      "train loss:0.01808550933888229\n",
      "train loss:0.018858239429881084\n",
      "train loss:0.03943126063906034\n",
      "train loss:0.021398353485028256\n",
      "train loss:0.020695311063345698\n",
      "train loss:0.021705522703339853\n",
      "train loss:0.0070221756459338085\n",
      "train loss:0.1100403893545603\n",
      "train loss:0.05771622519328463\n",
      "train loss:0.018122555641677578\n",
      "train loss:0.03282873273674773\n",
      "train loss:0.01088875059769182\n",
      "train loss:0.053807130829239894\n",
      "train loss:0.08018003593792965\n",
      "train loss:0.07387418481128828\n",
      "train loss:0.025104212357853147\n",
      "train loss:0.008651504252578235\n",
      "train loss:0.01728207330355749\n",
      "train loss:0.01412411895648249\n",
      "train loss:0.08426592438729559\n",
      "train loss:0.04153049995447586\n",
      "train loss:0.049426734833192265\n",
      "train loss:0.02884345247127262\n",
      "train loss:0.1316164507786267\n",
      "train loss:0.10371102337648795\n",
      "train loss:0.13980929597734953\n",
      "train loss:0.03274491373343427\n",
      "train loss:0.05223906379652366\n",
      "train loss:0.07935485379200642\n",
      "train loss:0.07073900647419822\n",
      "train loss:0.03829701506192011\n",
      "train loss:0.037763953371579534\n",
      "train loss:0.019351046280675833\n",
      "train loss:0.021570539176449955\n",
      "train loss:0.07586866564169821\n",
      "train loss:0.02192138288731393\n",
      "train loss:0.03551998746657066\n",
      "train loss:0.021503485106159088\n",
      "train loss:0.0289221098766196\n",
      "train loss:0.0377935286090296\n",
      "train loss:0.0419909526211612\n",
      "train loss:0.09721618724886834\n",
      "train loss:0.05934819446881025\n",
      "train loss:0.029596951638829413\n",
      "train loss:0.04667194673135304\n",
      "train loss:0.03888991048314894\n",
      "train loss:0.032068121120709045\n",
      "train loss:0.033412007107128584\n",
      "train loss:0.013689807474781763\n",
      "train loss:0.029722782144317565\n",
      "train loss:0.007896376420367925\n",
      "train loss:0.03874806254806032\n",
      "train loss:0.0352984912041195\n",
      "train loss:0.02024308666033957\n",
      "train loss:0.021723076664599324\n",
      "train loss:0.02902154920582316\n",
      "train loss:0.05888480099467525\n",
      "train loss:0.018455994973407455\n",
      "train loss:0.022313541234391384\n",
      "train loss:0.016860083905859632\n",
      "train loss:0.039266118991746125\n",
      "train loss:0.014022946911875878\n",
      "train loss:0.019335603411637675\n",
      "train loss:0.06535017381774845\n",
      "train loss:0.04673117357820645\n",
      "train loss:0.045192912721282584\n",
      "train loss:0.03924600028109851\n",
      "train loss:0.10718399861990312\n",
      "train loss:0.09157212259611416\n",
      "train loss:0.01014768321239208\n",
      "train loss:0.022408811183907123\n",
      "train loss:0.07913782745292197\n",
      "train loss:0.03157368659052155\n",
      "train loss:0.028679260873087985\n",
      "train loss:0.02569135565725185\n",
      "train loss:0.03606511728353128\n",
      "train loss:0.012490555746037775\n",
      "train loss:0.012405876363427652\n",
      "train loss:0.06687664559159688\n",
      "train loss:0.03306776758847819\n",
      "train loss:0.05170364157974383\n",
      "train loss:0.018450562004311912\n",
      "train loss:0.07825295750487297\n",
      "train loss:0.03025196435642915\n",
      "train loss:0.02622558079820257\n",
      "train loss:0.044337757296245436\n",
      "train loss:0.027311760067692467\n",
      "train loss:0.019741118322940395\n",
      "train loss:0.019970849748325022\n",
      "train loss:0.07010402976982368\n",
      "train loss:0.013759555649240258\n",
      "train loss:0.02913169120374435\n",
      "train loss:0.044962133519812646\n",
      "train loss:0.013128741354688585\n",
      "train loss:0.02597558833604593\n",
      "train loss:0.05490441549424733\n",
      "train loss:0.01352787759260428\n",
      "train loss:0.01686058886704261\n",
      "train loss:0.04224530839921898\n",
      "train loss:0.04605424139344178\n",
      "train loss:0.04393624961600147\n",
      "train loss:0.032049520559814096\n",
      "train loss:0.004747973904710643\n",
      "train loss:0.06333895753958659\n",
      "train loss:0.025835861245625962\n",
      "train loss:0.03861497344201739\n",
      "train loss:0.038504953182817436\n",
      "train loss:0.027618120753188733\n",
      "train loss:0.02081101077671891\n",
      "train loss:0.04829027538277494\n",
      "train loss:0.11587899147886524\n",
      "train loss:0.028072998199411586\n",
      "train loss:0.01455026488005365\n",
      "train loss:0.017620012668066117\n",
      "train loss:0.025508116397314956\n",
      "train loss:0.03877849206445094\n",
      "train loss:0.016839707111586395\n",
      "train loss:0.02983962259676018\n",
      "train loss:0.1018334549869964\n",
      "train loss:0.02633746222262984\n",
      "train loss:0.06154054700191569\n",
      "train loss:0.03101197981671985\n",
      "train loss:0.11110253573859694\n",
      "train loss:0.06494506849924894\n",
      "train loss:0.016106053722573215\n",
      "train loss:0.037345707655620404\n",
      "train loss:0.030881195791197063\n",
      "train loss:0.02079968051569747\n",
      "train loss:0.025338114525015533\n",
      "train loss:0.02497709432751819\n",
      "train loss:0.03498914089816931\n",
      "train loss:0.07256871379232432\n",
      "train loss:0.058528752384929744\n",
      "train loss:0.046054613221781865\n",
      "train loss:0.025854353078229097\n",
      "train loss:0.05940549193259073\n",
      "train loss:0.010315020158429293\n",
      "train loss:0.025791270233629135\n",
      "train loss:0.06894635936097221\n",
      "train loss:0.013905793869414715\n",
      "train loss:0.049484674514723936\n",
      "train loss:0.04436726735132132\n",
      "train loss:0.009670289787942571\n",
      "train loss:0.15588885616267\n",
      "train loss:0.018929705223525935\n",
      "train loss:0.005901284866598999\n",
      "train loss:0.04239957863825506\n",
      "train loss:0.022193573657101034\n",
      "train loss:0.03510279670323523\n",
      "train loss:0.02755000104092049\n",
      "train loss:0.006392313234167883\n",
      "train loss:0.042161319952320205\n",
      "train loss:0.016339330416095716\n",
      "train loss:0.030108082007469696\n",
      "train loss:0.01105647689033179\n",
      "train loss:0.02360769361526494\n",
      "train loss:0.044714370056224924\n",
      "train loss:0.03170201747857466\n",
      "train loss:0.009663332791896002\n",
      "train loss:0.06759247473192367\n",
      "train loss:0.048175067442023226\n",
      "train loss:0.010427077671344891\n",
      "train loss:0.026247032768730652\n",
      "train loss:0.0018688423968962835\n",
      "train loss:0.021783012079741267\n",
      "train loss:0.015728379658689076\n",
      "train loss:0.054811694254967236\n",
      "train loss:0.02420238329721645\n",
      "train loss:0.0964868959590503\n",
      "train loss:0.08178371866981825\n",
      "train loss:0.023577373227280433\n",
      "train loss:0.016697851408290255\n",
      "train loss:0.012237819677029783\n",
      "train loss:0.020511512380704443\n",
      "train loss:0.010663626086761666\n",
      "train loss:0.04750984529944387\n",
      "train loss:0.02573713470456245\n",
      "train loss:0.015614626676529731\n",
      "train loss:0.03858630280430447\n",
      "train loss:0.035542193832512475\n",
      "train loss:0.02220428855574111\n",
      "train loss:0.01416988922840635\n",
      "train loss:0.015621386626204324\n",
      "train loss:0.09441580673529831\n",
      "train loss:0.01633040696074862\n",
      "train loss:0.02419392460369931\n",
      "train loss:0.04019177211203365\n",
      "train loss:0.01165269841394156\n",
      "train loss:0.06395245474699462\n",
      "train loss:0.009145706590161734\n",
      "train loss:0.01081439765803407\n",
      "train loss:0.013419243811021497\n",
      "train loss:0.059481765151973495\n",
      "train loss:0.018542497347271203\n",
      "train loss:0.040496682350058835\n",
      "train loss:0.006582313257711304\n",
      "train loss:0.009359067189451911\n",
      "train loss:0.023453033427970196\n",
      "train loss:0.004462926785775912\n",
      "train loss:0.013031502877023792\n",
      "train loss:0.048591505108737436\n",
      "train loss:0.025474492723709937\n",
      "train loss:0.051357578943700376\n",
      "train loss:0.01642275748841914\n",
      "train loss:0.012245176245453005\n",
      "train loss:0.039157538590754756\n",
      "train loss:0.005950288104040618\n",
      "train loss:0.007065557450057668\n",
      "train loss:0.003947201404339333\n",
      "train loss:0.053951472276386074\n",
      "train loss:0.05006239312746691\n",
      "train loss:0.011812324465008803\n",
      "train loss:0.008150406916433012\n",
      "train loss:0.025615244036095736\n",
      "train loss:0.018371741131097782\n",
      "train loss:0.010668958462375018\n",
      "train loss:0.05498164451892074\n",
      "train loss:0.015250392715038218\n",
      "train loss:0.0427924994487497\n",
      "train loss:0.02561870858837001\n",
      "train loss:0.03942506711988914\n",
      "train loss:0.00997480363115922\n",
      "train loss:0.029283169545101014\n",
      "train loss:0.0068513590493895545\n",
      "train loss:0.020790024022164656\n",
      "train loss:0.018090001193073434\n",
      "train loss:0.008088272444689001\n",
      "train loss:0.012119975590362575\n",
      "train loss:0.04373574991162164\n",
      "train loss:0.04243300242434761\n",
      "train loss:0.011791667533983816\n",
      "train loss:0.013608573421970402\n",
      "train loss:0.0765385617844303\n",
      "train loss:0.06071905290335326\n",
      "train loss:0.0065937703777441984\n",
      "train loss:0.031091175239891054\n",
      "train loss:0.026405366767672144\n",
      "train loss:0.015997791410909193\n",
      "train loss:0.015543271092725155\n",
      "train loss:0.08656323132611587\n",
      "train loss:0.0883847881268641\n",
      "train loss:0.06798268824752526\n",
      "train loss:0.0166104507579449\n",
      "train loss:0.08825445049239362\n",
      "train loss:0.05006861464968693\n",
      "train loss:0.006469620639497191\n",
      "train loss:0.018322271718839617\n",
      "train loss:0.014447206925157299\n",
      "train loss:0.044606674783395486\n",
      "train loss:0.010803185264038607\n",
      "train loss:0.02243992957092809\n",
      "train loss:0.03640667985916535\n",
      "train loss:0.016278571327885346\n",
      "train loss:0.025703926114746062\n",
      "train loss:0.01975698075313515\n",
      "train loss:0.027929464901115874\n",
      "train loss:0.027408797830041465\n",
      "train loss:0.005941601939840101\n",
      "train loss:0.04028595778478847\n",
      "train loss:0.01513055776566587\n",
      "train loss:0.005936035574411\n",
      "train loss:0.020166578255166993\n",
      "train loss:0.005677297328891013\n",
      "train loss:0.033046322953385075\n",
      "train loss:0.024175113534551977\n",
      "train loss:0.024754301797447798\n",
      "train loss:0.03768485782560607\n",
      "train loss:0.017536348225272728\n",
      "train loss:0.010038688912307484\n",
      "train loss:0.030844613833721866\n",
      "train loss:0.04868728713757549\n",
      "train loss:0.005538070534315047\n",
      "train loss:0.05344988832672728\n",
      "train loss:0.009944867807692041\n",
      "train loss:0.03472123946554525\n",
      "train loss:0.016563895252324778\n",
      "train loss:0.009516684527076286\n",
      "train loss:0.06554919478185788\n",
      "train loss:0.08760854699144652\n",
      "train loss:0.023109593768375812\n",
      "train loss:0.06609879086234936\n",
      "train loss:0.05077687002539791\n",
      "train loss:0.0193224441303315\n",
      "train loss:0.06117565736789985\n",
      "train loss:0.06210751015190242\n",
      "train loss:0.0216786788488107\n",
      "train loss:0.048206980044261494\n",
      "train loss:0.055622930756253304\n",
      "train loss:0.01982843548219287\n",
      "train loss:0.03521626352000566\n",
      "train loss:0.03575999675597487\n",
      "train loss:0.016224709991956947\n",
      "train loss:0.025169163318715226\n",
      "train loss:0.07553993554861628\n",
      "train loss:0.0244909585685278\n",
      "train loss:0.01555517696645645\n",
      "train loss:0.015120646146827989\n",
      "train loss:0.044347249519363724\n",
      "train loss:0.03380663757122958\n",
      "train loss:0.030527913162286233\n",
      "train loss:0.1016667340256713\n",
      "train loss:0.03441050990042262\n",
      "train loss:0.08703004775153661\n",
      "train loss:0.00931631890988589\n",
      "train loss:0.041657967728996985\n",
      "train loss:0.021111837142269132\n",
      "train loss:0.02264199592917523\n",
      "train loss:0.0440145312779308\n",
      "train loss:0.09034884497772674\n",
      "train loss:0.037094167646659064\n",
      "train loss:0.07587836091089105\n",
      "train loss:0.0827685787444425\n",
      "train loss:0.017448988292614213\n",
      "train loss:0.026132519275848875\n",
      "train loss:0.0472330949233149\n",
      "train loss:0.009289243447751318\n",
      "train loss:0.016777198303023072\n",
      "train loss:0.019462999779275885\n",
      "train loss:0.0302931129389107\n",
      "train loss:0.024222968802703863\n",
      "train loss:0.06995716200675842\n",
      "train loss:0.051933000301375626\n",
      "train loss:0.03548707202378963\n",
      "train loss:0.026171030611029343\n",
      "train loss:0.012568963308481739\n",
      "train loss:0.02893248960214031\n",
      "train loss:0.015550246463937982\n",
      "train loss:0.03946221110908966\n",
      "train loss:0.004751254406423807\n",
      "train loss:0.033456713845842805\n",
      "train loss:0.0327468677856407\n",
      "train loss:0.02576581912810052\n",
      "train loss:0.03120840563175147\n",
      "train loss:0.04219198289307822\n",
      "train loss:0.013242566869658657\n",
      "train loss:0.01176037103594654\n",
      "train loss:0.06988363413369073\n",
      "train loss:0.08568830019760888\n",
      "train loss:0.08744783102230656\n",
      "train loss:0.11825420188832891\n",
      "train loss:0.014146521216780094\n",
      "train loss:0.01235376341788473\n",
      "train loss:0.012125893874290338\n",
      "train loss:0.043166909289269625\n",
      "train loss:0.021571832135219484\n",
      "train loss:0.01205445987309745\n",
      "train loss:0.008446587451427743\n",
      "train loss:0.017274017477847247\n",
      "train loss:0.010753157162454549\n",
      "train loss:0.05167592936012968\n",
      "train loss:0.010400972674935682\n",
      "train loss:0.02270776173449458\n",
      "train loss:0.013759827911449356\n",
      "train loss:0.013236599046280028\n",
      "train loss:0.03491920147966661\n",
      "train loss:0.02077778716361724\n",
      "train loss:0.004728086466680757\n",
      "train loss:0.05953077612232589\n",
      "train loss:0.028645666071316925\n",
      "train loss:0.05188892196799158\n",
      "train loss:0.00887081938510647\n",
      "train loss:0.00785888626389249\n",
      "train loss:0.03003189405406239\n",
      "train loss:0.014095318851764111\n",
      "train loss:0.040693555347947566\n",
      "train loss:0.04035738816298263\n",
      "train loss:0.008441738544648893\n",
      "train loss:0.033123615249872534\n",
      "train loss:0.05973350132804364\n",
      "train loss:0.020462226693455206\n",
      "train loss:0.03749280208106082\n",
      "train loss:0.06626694962266401\n",
      "train loss:0.010892301088638489\n",
      "train loss:0.056659098160106065\n",
      "train loss:0.021641175746906046\n",
      "train loss:0.020239037327051915\n",
      "train loss:0.03808726625005277\n",
      "train loss:0.046651127644139125\n",
      "train loss:0.013155928472705955\n",
      "train loss:0.030953906149397196\n",
      "train loss:0.004589995334850683\n",
      "train loss:0.015499593720803477\n",
      "train loss:0.047184283515344004\n",
      "train loss:0.011341623245587129\n",
      "train loss:0.028121017911428025\n",
      "train loss:0.01929418133011066\n",
      "train loss:0.023113425089676634\n",
      "train loss:0.008438690714849779\n",
      "train loss:0.025843194326120957\n",
      "train loss:0.02458217740140913\n",
      "train loss:0.00974750563354326\n",
      "train loss:0.025473361936202876\n",
      "train loss:0.026090964826643175\n",
      "train loss:0.01232425285277306\n",
      "train loss:0.0226775252913493\n",
      "train loss:0.00957374273108303\n",
      "train loss:0.0254314375823482\n",
      "train loss:0.03330299371024654\n",
      "train loss:0.004222770062189838\n",
      "train loss:0.05156321190151052\n",
      "train loss:0.02071490424313191\n",
      "train loss:0.005737385335318549\n",
      "train loss:0.03387409130301692\n",
      "train loss:0.012827684065567484\n",
      "train loss:0.023372799395779985\n",
      "train loss:0.019556186066682746\n",
      "train loss:0.011209514361732408\n",
      "train loss:0.03145994897351792\n",
      "train loss:0.011364194245287429\n",
      "train loss:0.0773616123333855\n",
      "train loss:0.04071411866433288\n",
      "train loss:0.014442993214249831\n",
      "train loss:0.03278027534081056\n",
      "train loss:0.016311849630856783\n",
      "train loss:0.05478249035077569\n",
      "train loss:0.02481190291863423\n",
      "train loss:0.004497419133297113\n",
      "train loss:0.01521954696488807\n",
      "train loss:0.04022262146078401\n",
      "train loss:0.021264041511774703\n",
      "train loss:0.022154145950113165\n",
      "train loss:0.02649369309369396\n",
      "train loss:0.02571579952483969\n",
      "train loss:0.018122157106107234\n",
      "train loss:0.030284751472393276\n",
      "train loss:0.010888702880701817\n",
      "train loss:0.022498519559202533\n",
      "train loss:0.02736458140356731\n",
      "train loss:0.04152225587882069\n",
      "train loss:0.07873515833731531\n",
      "train loss:0.005257218537568265\n",
      "train loss:0.029390738837306717\n",
      "train loss:0.03152768078366839\n",
      "train loss:0.0020458479043431346\n",
      "train loss:0.01713264173431297\n",
      "train loss:0.012446849335050757\n",
      "train loss:0.024569218544989058\n",
      "train loss:0.047075265050859426\n",
      "train loss:0.014549978818571776\n",
      "train loss:0.08877033891836003\n",
      "train loss:0.053078069238600235\n",
      "train loss:0.00808412068987225\n",
      "train loss:0.022710951612209384\n",
      "train loss:0.02101727354584299\n",
      "train loss:0.03404780414906071\n",
      "train loss:0.027315951205412727\n",
      "train loss:0.07962286373476762\n",
      "train loss:0.028843336341979917\n",
      "train loss:0.007286962983473465\n",
      "train loss:0.03228125862376833\n",
      "train loss:0.03236085621039941\n",
      "train loss:0.019342152807723306\n",
      "train loss:0.006694063521181597\n",
      "train loss:0.0091310570625938\n",
      "train loss:0.04327001482551991\n",
      "train loss:0.03772837782137627\n",
      "train loss:0.016974374547259762\n",
      "train loss:0.04127923393857879\n",
      "train loss:0.019523996252214487\n",
      "train loss:0.0819470528354142\n",
      "train loss:0.06931505928367827\n",
      "train loss:0.014336165465216993\n",
      "train loss:0.0363618530981713\n",
      "train loss:0.07676623659491223\n",
      "train loss:0.03798372584345432\n",
      "train loss:0.05508535236444478\n",
      "train loss:0.002397067056915363\n",
      "train loss:0.03045345658627041\n",
      "train loss:0.010995587122911264\n",
      "train loss:0.00968526699575905\n",
      "train loss:0.034837945435180505\n",
      "train loss:0.031713168137825695\n",
      "train loss:0.03688865609017862\n",
      "train loss:0.02464779512323521\n",
      "train loss:0.032552658684792506\n",
      "train loss:0.010122929957103559\n",
      "train loss:0.04861942770362018\n",
      "train loss:0.04907586830732855\n",
      "train loss:0.01449074177676254\n",
      "train loss:0.011188780755389578\n",
      "train loss:0.004674850558417983\n",
      "train loss:0.02887901156602791\n",
      "train loss:0.01712954799345427\n",
      "train loss:0.011725377855062325\n",
      "train loss:0.03198979648810651\n",
      "train loss:0.053683854154980185\n",
      "train loss:0.0068861323596616005\n",
      "train loss:0.06588343004680693\n",
      "train loss:0.046369058212569494\n",
      "train loss:0.006831283689847566\n",
      "train loss:0.01354365891214373\n",
      "train loss:0.020668766544328077\n",
      "train loss:0.07589866298557521\n",
      "train loss:0.013443144608832771\n",
      "train loss:0.015961062459197407\n",
      "train loss:0.00944397999522136\n",
      "train loss:0.029105839090254385\n",
      "train loss:0.06197133247796824\n",
      "train loss:0.01730504403921619\n",
      "train loss:0.016694840928523212\n",
      "train loss:0.014799885969041178\n",
      "train loss:0.04365068856683346\n",
      "train loss:0.00825673152900285\n",
      "train loss:0.09850020850603286\n",
      "train loss:0.015275764536279603\n",
      "train loss:0.01765443818626316\n",
      "train loss:0.027850131218006963\n",
      "train loss:0.02369865223862411\n",
      "train loss:0.012263402088165292\n",
      "train loss:0.05951437073488972\n",
      "train loss:0.02508338421111469\n",
      "train loss:0.042295743578841895\n",
      "train loss:0.032014779045162665\n",
      "train loss:0.01303713637515395\n",
      "train loss:0.02516934938385562\n",
      "train loss:0.01253786064122192\n",
      "train loss:0.008178353144773259\n",
      "train loss:0.01607669621690263\n",
      "train loss:0.02431490167092164\n",
      "train loss:0.03702586877784508\n",
      "train loss:0.013305510553447086\n",
      "train loss:0.011463662727675175\n",
      "train loss:0.012974072716242895\n",
      "train loss:0.03742928406518984\n",
      "train loss:0.02857416165046816\n",
      "train loss:0.03308009139053061\n",
      "train loss:0.02974827687734811\n",
      "train loss:0.03457922357153468\n",
      "train loss:0.03251500923369036\n",
      "train loss:0.03567137500610679\n",
      "train loss:0.011264825760695209\n",
      "train loss:0.010761427182121459\n",
      "train loss:0.007471951472331576\n",
      "train loss:0.008088398518613303\n",
      "train loss:0.018250161327158624\n",
      "train loss:0.01581562367736539\n",
      "train loss:0.06038241122614371\n",
      "train loss:0.011823085108374551\n",
      "train loss:0.012829359263107395\n",
      "train loss:0.013752138265485183\n",
      "train loss:0.0038381122870631583\n",
      "train loss:0.03960175767747669\n",
      "train loss:0.015618464164185887\n",
      "train loss:0.025581955241230712\n",
      "train loss:0.04179455916514348\n",
      "train loss:0.03489258572336136\n",
      "train loss:0.10639913768851163\n",
      "train loss:0.021853269552334905\n",
      "train loss:0.05145719938066506\n",
      "train loss:0.024864790926487433\n",
      "train loss:0.01075008896017774\n",
      "train loss:0.018567293220041345\n",
      "train loss:0.0069853403604628606\n",
      "train loss:0.02583636036596017\n",
      "train loss:0.058246497226514\n",
      "train loss:0.028874449817681284\n",
      "train loss:0.05665140431088579\n",
      "train loss:0.041773759315345854\n",
      "train loss:0.12323376422605666\n",
      "train loss:0.0037296288655095503\n",
      "train loss:0.013845386413099438\n",
      "train loss:0.011306843529815185\n",
      "train loss:0.021159284635188468\n",
      "train loss:0.013559624188120347\n",
      "train loss:0.025553636775390767\n",
      "train loss:0.01807987457010183\n",
      "train loss:0.024722013489371322\n",
      "train loss:0.03274574820913936\n",
      "train loss:0.06029496590394147\n",
      "train loss:0.021420046514589633\n",
      "train loss:0.03340929785898388\n",
      "train loss:0.06371906906566552\n",
      "train loss:0.019643531899812366\n",
      "train loss:0.01811578835535266\n",
      "train loss:0.040686934262932455\n",
      "train loss:0.008754494460456196\n",
      "train loss:0.03937254268989246\n",
      "train loss:0.041504670769567716\n",
      "train loss:0.005324860570450939\n",
      "train loss:0.0342249623458821\n",
      "train loss:0.02864910387134456\n",
      "train loss:0.027760332478441736\n",
      "train loss:0.13159388694098995\n",
      "train loss:0.006110958730260353\n",
      "train loss:0.04387268572719261\n",
      "train loss:0.01253831799667521\n",
      "train loss:0.0044311175754402085\n",
      "train loss:0.024820823496825555\n",
      "train loss:0.08448648828511121\n",
      "train loss:0.003941106541091858\n",
      "train loss:0.017983190018341186\n",
      "train loss:0.019294211462240984\n",
      "train loss:0.037116848656242846\n",
      "train loss:0.04887229673972549\n",
      "train loss:0.03382666074616527\n",
      "=== epoch:5, train acc:0.983, test acc:0.979 ===\n",
      "train loss:0.03416181109164824\n",
      "train loss:0.08220395558676895\n",
      "train loss:0.02963400781848202\n",
      "train loss:0.007961605583141006\n",
      "train loss:0.010363336493072282\n",
      "train loss:0.05869832775644091\n",
      "train loss:0.023143832624323596\n",
      "train loss:0.06274044951347775\n",
      "train loss:0.022801985514515802\n",
      "train loss:0.04285440382473766\n",
      "train loss:0.09281447011806138\n",
      "train loss:0.029800149316196588\n",
      "train loss:0.017112238863375377\n",
      "train loss:0.013893648933521512\n",
      "train loss:0.0826753373706687\n",
      "train loss:0.0359338386802422\n",
      "train loss:0.015061116503151924\n",
      "train loss:0.019143262607250855\n",
      "train loss:0.06498873828735768\n",
      "train loss:0.04380362776908563\n",
      "train loss:0.02820684546308079\n",
      "train loss:0.040491885001310374\n",
      "train loss:0.007867805730197183\n",
      "train loss:0.025277664822452736\n",
      "train loss:0.027652686756713393\n",
      "train loss:0.00871841640460189\n",
      "train loss:0.012078731505259517\n",
      "train loss:0.05258124050326301\n",
      "train loss:0.013041545987709428\n",
      "train loss:0.046056696146685276\n",
      "train loss:0.050322051236638264\n",
      "train loss:0.02985819921430618\n",
      "train loss:0.09815761328598051\n",
      "train loss:0.01293944687670525\n",
      "train loss:0.04687630748745562\n",
      "train loss:0.012720333840689251\n",
      "train loss:0.027482384750328256\n",
      "train loss:0.012646192227288619\n",
      "train loss:0.0068671427272085405\n",
      "train loss:0.05318079106871747\n",
      "train loss:0.04879180974819779\n",
      "train loss:0.006930125722281141\n",
      "train loss:0.028605548705420182\n",
      "train loss:0.0131118450894381\n",
      "train loss:0.012088436360671634\n",
      "train loss:0.004490031212336987\n",
      "train loss:0.035217317861499516\n",
      "train loss:0.027222392101507332\n",
      "train loss:0.009538490560942869\n",
      "train loss:0.04547562949708312\n",
      "train loss:0.039030658017043904\n",
      "train loss:0.029136632297945338\n",
      "train loss:0.004946573182209977\n",
      "train loss:0.023683991606768758\n",
      "train loss:0.010199021389704974\n",
      "train loss:0.052910815044169765\n",
      "train loss:0.05455165391728728\n",
      "train loss:0.015214466429963814\n",
      "train loss:0.011422339965249675\n",
      "train loss:0.08282902010249803\n",
      "train loss:0.013749906041019756\n",
      "train loss:0.010081521173026589\n",
      "train loss:0.009161651507690266\n",
      "train loss:0.039037777423601365\n",
      "train loss:0.016106567682443434\n",
      "train loss:0.04455176427191423\n",
      "train loss:0.09194957077033283\n",
      "train loss:0.014828747388338963\n",
      "train loss:0.018372315146870287\n",
      "train loss:0.027411502393647928\n",
      "train loss:0.01795354596642825\n",
      "train loss:0.010638989747274697\n",
      "train loss:0.04256449292741256\n",
      "train loss:0.014038186288597994\n",
      "train loss:0.03466314125018452\n",
      "train loss:0.03554635775926739\n",
      "train loss:0.0463027061698294\n",
      "train loss:0.005866216782485551\n",
      "train loss:0.04515578859150026\n",
      "train loss:0.0363287220404758\n",
      "train loss:0.03798266761104494\n",
      "train loss:0.08759865594365097\n",
      "train loss:0.014470949570056797\n",
      "train loss:0.055629027347720586\n",
      "train loss:0.02098104520782064\n",
      "train loss:0.008979840001146372\n",
      "train loss:0.03988036461693134\n",
      "train loss:0.02608119603126767\n",
      "train loss:0.009873519236649182\n",
      "train loss:0.032176813302075266\n",
      "train loss:0.0161311359361775\n",
      "train loss:0.021079553933143775\n",
      "train loss:0.014269964335560645\n",
      "train loss:0.016716865470278466\n",
      "train loss:0.027172578738765517\n",
      "train loss:0.008796790185986513\n",
      "train loss:0.04187322394601201\n",
      "train loss:0.014478593513861304\n",
      "train loss:0.012266603241724567\n",
      "train loss:0.021406852731854557\n",
      "train loss:0.03344430414587903\n",
      "train loss:0.00827317510096447\n",
      "train loss:0.015626859095780928\n",
      "train loss:0.04720435996759312\n",
      "train loss:0.07065726688439079\n",
      "train loss:0.08852341648062569\n",
      "train loss:0.008300931010743773\n",
      "train loss:0.028307264100736095\n",
      "train loss:0.01853802471974677\n",
      "train loss:0.015261685375855698\n",
      "train loss:0.00545388344608841\n",
      "train loss:0.026577470279488022\n",
      "train loss:0.005415267908636313\n",
      "train loss:0.014647832340991549\n",
      "train loss:0.019614219028505415\n",
      "train loss:0.01968334220891136\n",
      "train loss:0.03179883291392724\n",
      "train loss:0.029967835164164413\n",
      "train loss:0.022319512933440856\n",
      "train loss:0.015623350894580891\n",
      "train loss:0.008184750502923078\n",
      "train loss:0.005202265398707687\n",
      "train loss:0.02977693106101638\n",
      "train loss:0.04241305897493796\n",
      "train loss:0.03795295691764271\n",
      "train loss:0.008923622747200854\n",
      "train loss:0.008385492553897162\n",
      "train loss:0.026004528894531354\n",
      "train loss:0.016962257434474555\n",
      "train loss:0.09028562036073834\n",
      "train loss:0.014812963608424896\n",
      "train loss:0.023781167175096162\n",
      "train loss:0.04973968778047696\n",
      "train loss:0.020560643378449033\n",
      "train loss:0.013540793509283137\n",
      "train loss:0.05571963579942585\n",
      "train loss:0.05353683375712315\n",
      "train loss:0.03317602393383425\n",
      "train loss:0.028564717298125528\n",
      "train loss:0.052973460655719985\n",
      "train loss:0.02287364316182368\n",
      "train loss:0.043438860115851234\n",
      "train loss:0.00979807488930631\n",
      "train loss:0.019786221877630763\n",
      "train loss:0.005678123695528603\n",
      "train loss:0.010269991445565972\n",
      "train loss:0.0510098999859134\n",
      "train loss:0.026767879193896622\n",
      "train loss:0.07602028273506244\n",
      "train loss:0.024712062652517793\n",
      "train loss:0.025781072783873937\n",
      "train loss:0.009460532944366506\n",
      "train loss:0.022577748528150046\n",
      "train loss:0.008889591710881732\n",
      "train loss:0.05596290883261381\n",
      "train loss:0.016764296210293975\n",
      "train loss:0.012031756233186973\n",
      "train loss:0.004945339118806582\n",
      "train loss:0.036453976852388784\n",
      "train loss:0.061855488609592106\n",
      "train loss:0.020354441460293825\n",
      "train loss:0.009329565902645229\n",
      "train loss:0.009119744100053211\n",
      "train loss:0.0050391432554268535\n",
      "train loss:0.03250189141515989\n",
      "train loss:0.048363689672880586\n",
      "train loss:0.018874331827489706\n",
      "train loss:0.03495365793316517\n",
      "train loss:0.026202579956916775\n",
      "train loss:0.015361467689292783\n",
      "train loss:0.07503108349751168\n",
      "train loss:0.015867841854798635\n",
      "train loss:0.018741253197147537\n",
      "train loss:0.013307280130060167\n",
      "train loss:0.022496767404354627\n",
      "train loss:0.028121809409640312\n",
      "train loss:0.019160354195218932\n",
      "train loss:0.0057289630752951045\n",
      "train loss:0.005617018083234631\n",
      "train loss:0.06708449554308304\n",
      "train loss:0.01314734028517665\n",
      "train loss:0.03377379620093807\n",
      "train loss:0.016714069240090845\n",
      "train loss:0.024011784885666118\n",
      "train loss:0.00582713962686587\n",
      "train loss:0.07923491894320078\n",
      "train loss:0.04058709599507735\n",
      "train loss:0.011371044630986734\n",
      "train loss:0.012521675638346543\n",
      "train loss:0.04117772039130041\n",
      "train loss:0.011162720366509997\n",
      "train loss:0.0970385734367116\n",
      "train loss:0.0189739780164019\n",
      "train loss:0.02203921376425614\n",
      "train loss:0.030565945923343593\n",
      "train loss:0.02794138765601436\n",
      "train loss:0.007436770477704261\n",
      "train loss:0.022185086026309628\n",
      "train loss:0.09788176543928624\n",
      "train loss:0.017144921531247075\n",
      "train loss:0.026924475343470228\n",
      "train loss:0.008581147123106979\n",
      "train loss:0.0635615473114936\n",
      "train loss:0.005905296028572977\n",
      "train loss:0.03718588540240892\n",
      "train loss:0.007062590549621525\n",
      "train loss:0.01183545103449541\n",
      "train loss:0.05701612494771088\n",
      "train loss:0.003552845976977771\n",
      "train loss:0.009689207924010339\n",
      "train loss:0.02698142618340173\n",
      "train loss:0.007903719197686015\n",
      "train loss:0.005322010721209125\n",
      "train loss:0.012539829850669471\n",
      "train loss:0.019366072113162193\n",
      "train loss:0.04085461208424929\n",
      "train loss:0.023842441049477334\n",
      "train loss:0.029444491990197122\n",
      "train loss:0.006864437178202113\n",
      "train loss:0.012834913303028146\n",
      "train loss:0.02984013552835294\n",
      "train loss:0.02716353829039516\n",
      "train loss:0.03479687174355885\n",
      "train loss:0.049237744772068\n",
      "train loss:0.005419537987090324\n",
      "train loss:0.08216377746572669\n",
      "train loss:0.014044422829129892\n",
      "train loss:0.01920899422688084\n",
      "train loss:0.025888311299108034\n",
      "train loss:0.006400661914751103\n",
      "train loss:0.004266537890719048\n",
      "train loss:0.056807149075425833\n",
      "train loss:0.007835411398823284\n",
      "train loss:0.005575088934151483\n",
      "train loss:0.007453521345771248\n",
      "train loss:0.028072684972960834\n",
      "train loss:0.11072777707817914\n",
      "train loss:0.01269845756967663\n",
      "train loss:0.016004058817271224\n",
      "train loss:0.015330959451301912\n",
      "train loss:0.00473650415162466\n",
      "train loss:0.0065921464955255825\n",
      "train loss:0.020952871912960597\n",
      "train loss:0.007941113272842908\n",
      "train loss:0.039681825643799316\n",
      "train loss:0.015821620482888335\n",
      "train loss:0.03981594522933825\n",
      "train loss:0.03056069893435516\n",
      "train loss:0.05224145173235012\n",
      "train loss:0.03330703844058663\n",
      "train loss:0.02856988908992986\n",
      "train loss:0.02204108128885952\n",
      "train loss:0.01811958338220602\n",
      "train loss:0.08042409826203095\n",
      "train loss:0.01733173796165161\n",
      "train loss:0.055773772648191236\n",
      "train loss:0.009100293730962312\n",
      "train loss:0.04792013302557683\n",
      "train loss:0.009933426901435614\n",
      "train loss:0.03425233982617504\n",
      "train loss:0.018534232742907723\n",
      "train loss:0.048984008047154824\n",
      "train loss:0.013541782440710722\n",
      "train loss:0.028915514117619757\n",
      "train loss:0.050552017324562964\n",
      "train loss:0.018837945769791597\n",
      "train loss:0.010575294501111556\n",
      "train loss:0.08567993586034568\n",
      "train loss:0.037522839195605145\n",
      "train loss:0.03396122954469122\n",
      "train loss:0.07187232820335974\n",
      "train loss:0.011425910851375797\n",
      "train loss:0.0035156019873281087\n",
      "train loss:0.026077718130952342\n",
      "train loss:0.08985271461998233\n",
      "train loss:0.03895895991205701\n",
      "train loss:0.018955036300413432\n",
      "train loss:0.0201121011457957\n",
      "train loss:0.012672260672394608\n",
      "train loss:0.060855721682134316\n",
      "train loss:0.02321564058084843\n",
      "train loss:0.07746250716107496\n",
      "train loss:0.050925461428152075\n",
      "train loss:0.019726363399916274\n",
      "train loss:0.00879163131859\n",
      "train loss:0.021898936940182495\n",
      "train loss:0.019226198250033813\n",
      "train loss:0.032519383026238476\n",
      "train loss:0.007166333043972502\n",
      "train loss:0.07098243233966717\n",
      "train loss:0.027850701673488017\n",
      "train loss:0.014846698788321542\n",
      "train loss:0.06252717429688406\n",
      "train loss:0.021253272522138978\n",
      "train loss:0.021410642774036325\n",
      "train loss:0.025884132744733446\n",
      "train loss:0.02327818782284298\n",
      "train loss:0.01697590434906622\n",
      "train loss:0.06932600947853934\n",
      "train loss:0.03987180450770614\n",
      "train loss:0.007147609733756274\n",
      "train loss:0.00985613838681063\n",
      "train loss:0.02653174883375596\n",
      "train loss:0.11327074264843696\n",
      "train loss:0.0284956550087239\n",
      "train loss:0.018229602624464082\n",
      "train loss:0.023982731364369368\n",
      "train loss:0.00925943132803854\n",
      "train loss:0.03398753453687672\n",
      "train loss:0.012152032471545289\n",
      "train loss:0.030734206351359355\n",
      "train loss:0.022434785111393515\n",
      "train loss:0.006092176269586584\n",
      "train loss:0.0275202117460009\n",
      "train loss:0.035796901900925916\n",
      "train loss:0.03371372334063851\n",
      "train loss:0.06579793416445809\n",
      "train loss:0.04149112134883591\n",
      "train loss:0.007833914486977952\n",
      "train loss:0.02363483553097211\n",
      "train loss:0.042580945971897564\n",
      "train loss:0.007543914895553799\n",
      "train loss:0.05698821295099744\n",
      "train loss:0.04988414430950451\n",
      "train loss:0.03700055322031263\n",
      "train loss:0.006990649798294787\n",
      "train loss:0.018434344894941366\n",
      "train loss:0.01887204804781674\n",
      "train loss:0.009325344636809113\n",
      "train loss:0.011047857310739775\n",
      "train loss:0.01903684817698008\n",
      "train loss:0.013142809592609446\n",
      "train loss:0.03854776470069779\n",
      "train loss:0.030621448937407834\n",
      "train loss:0.03026436219557858\n",
      "train loss:0.023818729579470288\n",
      "train loss:0.02780259218556539\n",
      "train loss:0.01159370435757561\n",
      "train loss:0.022491375307457404\n",
      "train loss:0.04412353053808513\n",
      "train loss:0.014583929855066567\n",
      "train loss:0.02604335946850862\n",
      "train loss:0.008737386511674931\n",
      "train loss:0.02229960291089028\n",
      "train loss:0.011392380100028006\n",
      "train loss:0.03806822377300712\n",
      "train loss:0.014606715457877869\n",
      "train loss:0.04483814816457676\n",
      "train loss:0.004767972504578797\n",
      "train loss:0.04350959098732192\n",
      "train loss:0.004964442828641941\n",
      "train loss:0.027709794667325278\n",
      "train loss:0.021816486823428315\n",
      "train loss:0.04193067186404481\n",
      "train loss:0.007886689550315115\n",
      "train loss:0.012304257663988521\n",
      "train loss:0.027971299910193977\n",
      "train loss:0.03642742662961755\n",
      "train loss:0.024286953034490334\n",
      "train loss:0.01429339054519264\n",
      "train loss:0.017855202163654386\n",
      "train loss:0.003979854297679187\n",
      "train loss:0.056418021535251654\n",
      "train loss:0.018816928257005025\n",
      "train loss:0.011068951867670086\n",
      "train loss:0.02511418533588022\n",
      "train loss:0.07018557103464867\n",
      "train loss:0.038715711249342546\n",
      "train loss:0.00870967350201523\n",
      "train loss:0.014050151014698094\n",
      "train loss:0.012221511040626173\n",
      "train loss:0.006827885735609882\n",
      "train loss:0.00410977099403909\n",
      "train loss:0.020405074317785793\n",
      "train loss:0.05676822159975494\n",
      "train loss:0.008684310018590236\n",
      "train loss:0.01566884342060228\n",
      "train loss:0.011242494006686659\n",
      "train loss:0.014168901599383894\n",
      "train loss:0.006041763025371114\n",
      "train loss:0.017510565357712424\n",
      "train loss:0.008554228815584257\n",
      "train loss:0.019308690825454906\n",
      "train loss:0.02860009278582065\n",
      "train loss:0.017944969805946634\n",
      "train loss:0.0017441337962230674\n",
      "train loss:0.047191344604706115\n",
      "train loss:0.02669515416500087\n",
      "train loss:0.011668553912606194\n",
      "train loss:0.009839852809060839\n",
      "train loss:0.011590355584229495\n",
      "train loss:0.015429741070914516\n",
      "train loss:0.0038981933345860575\n",
      "train loss:0.0273704180918953\n",
      "train loss:0.02355295244863838\n",
      "train loss:0.02472758833728371\n",
      "train loss:0.023152306221834527\n",
      "train loss:0.06708037635606255\n",
      "train loss:0.04548548226126634\n",
      "train loss:0.005639138275392399\n",
      "train loss:0.004044672118529845\n",
      "train loss:0.040402658561026386\n",
      "train loss:0.02826560171924061\n",
      "train loss:0.018681302744434573\n",
      "train loss:0.023381033456581525\n",
      "train loss:0.040943110190456974\n",
      "train loss:0.01714204106316936\n",
      "train loss:0.05301306361165356\n",
      "train loss:0.0038311802565673507\n",
      "train loss:0.050917713597053896\n",
      "train loss:0.011814597696176581\n",
      "train loss:0.009600528987342268\n",
      "train loss:0.004575670931979523\n",
      "train loss:0.008194774163355482\n",
      "train loss:0.05375633229076802\n",
      "train loss:0.013021316300814195\n",
      "train loss:0.010590428678575448\n",
      "train loss:0.01172612232331742\n",
      "train loss:0.05940828916564756\n",
      "train loss:0.043866652780862075\n",
      "train loss:0.007698843191170706\n",
      "train loss:0.0359803226729442\n",
      "train loss:0.010860933194015616\n",
      "train loss:0.02229701134223394\n",
      "train loss:0.04884490397845277\n",
      "train loss:0.00798641133476476\n",
      "train loss:0.022287808873040237\n",
      "train loss:0.05725719182557802\n",
      "train loss:0.004875946519410369\n",
      "train loss:0.020403081002116536\n",
      "train loss:0.036871906871451095\n",
      "train loss:0.010202446188159821\n",
      "train loss:0.004669352829648341\n",
      "train loss:0.04643855984959644\n",
      "train loss:0.0346872871212788\n",
      "train loss:0.00959128923663688\n",
      "train loss:0.0334735889217401\n",
      "train loss:0.012155281960051334\n",
      "train loss:0.028345484862393536\n",
      "train loss:0.011873822197191774\n",
      "train loss:0.010626368393166417\n",
      "train loss:0.08892339188869088\n",
      "train loss:0.023219067640243337\n",
      "train loss:0.01933601581233739\n",
      "train loss:0.020924308476793493\n",
      "train loss:0.015703942869649035\n",
      "train loss:0.0012995513167346256\n",
      "train loss:0.003774587290531823\n",
      "train loss:0.06462199750458758\n",
      "train loss:0.05490177814083312\n",
      "train loss:0.04881288163031451\n",
      "train loss:0.0032610908442975016\n",
      "train loss:0.0031563514601183557\n",
      "train loss:0.0958844114093607\n",
      "train loss:0.0034456463520885884\n",
      "train loss:0.023679510341669038\n",
      "train loss:0.0164639605484041\n",
      "train loss:0.004842617720809452\n",
      "train loss:0.016770638741460606\n",
      "train loss:0.039218051283237985\n",
      "train loss:0.008975574513280644\n",
      "train loss:0.011860258259784035\n",
      "train loss:0.011679684783806545\n",
      "train loss:0.0175031281819301\n",
      "train loss:0.013484386045343412\n",
      "train loss:0.003129240473190609\n",
      "train loss:0.012444760943593914\n",
      "train loss:0.016611218339173724\n",
      "train loss:0.014039005566784633\n",
      "train loss:0.011545332748934094\n",
      "train loss:0.01474454062865701\n",
      "train loss:0.04250000065422792\n",
      "train loss:0.00891755030574996\n",
      "train loss:0.03304870473864813\n",
      "train loss:0.03505815038490971\n",
      "train loss:0.007413509389009944\n",
      "train loss:0.005721348851467873\n",
      "train loss:0.0047212527606069555\n",
      "train loss:0.00704289772732016\n",
      "train loss:0.012360048756801252\n",
      "train loss:0.0431565729156607\n",
      "train loss:0.035915272641467814\n",
      "train loss:0.00822938616041607\n",
      "train loss:0.08961078944728769\n",
      "train loss:0.010996004148523671\n",
      "train loss:0.024101840797081067\n",
      "train loss:0.00827440811934031\n",
      "train loss:0.011511102129427847\n",
      "train loss:0.012815341371764629\n",
      "train loss:0.028956614637588564\n",
      "train loss:0.01998528336463807\n",
      "train loss:0.014692523809599288\n",
      "train loss:0.03635023077878879\n",
      "train loss:0.03223689482573559\n",
      "train loss:0.027092108424399387\n",
      "train loss:0.018713999017912143\n",
      "train loss:0.03465890832621197\n",
      "train loss:0.06527536116820133\n",
      "train loss:0.06233472782582143\n",
      "train loss:0.06410859394662809\n",
      "train loss:0.024632793661528662\n",
      "train loss:0.01859920060788868\n",
      "train loss:0.026758199578434366\n",
      "train loss:0.01678313603363207\n",
      "train loss:0.0039524189351402275\n",
      "train loss:0.010497388487539158\n",
      "train loss:0.00866105096206917\n",
      "train loss:0.026964919335071585\n",
      "train loss:0.027865336547915075\n",
      "train loss:0.008878723237471568\n",
      "train loss:0.04296953596227469\n",
      "train loss:0.012681421138669284\n",
      "train loss:0.02562758623020174\n",
      "train loss:0.022492776807346283\n",
      "train loss:0.017285080814248545\n",
      "train loss:0.023050142640636224\n",
      "train loss:0.06677645586972131\n",
      "train loss:0.008058273583342025\n",
      "train loss:0.03340505122686616\n",
      "train loss:0.00599979086966133\n",
      "train loss:0.05705018825679801\n",
      "train loss:0.005707096363760684\n",
      "train loss:0.005977328225277947\n",
      "train loss:0.030794459568969555\n",
      "train loss:0.010989244743278705\n",
      "train loss:0.0064324357468068406\n",
      "train loss:0.024382372662865488\n",
      "train loss:0.020326044916030517\n",
      "train loss:0.01143100994136221\n",
      "train loss:0.055639173082232096\n",
      "train loss:0.037090370623866574\n",
      "train loss:0.027060873472305676\n",
      "train loss:0.028420174707644148\n",
      "train loss:0.02172728711600528\n",
      "train loss:0.01452817268156344\n",
      "train loss:0.013235022434456128\n",
      "train loss:0.022820109311804518\n",
      "train loss:0.04812370112432501\n",
      "train loss:0.04041939011740109\n",
      "train loss:0.04097744917642708\n",
      "train loss:0.05781332647360281\n",
      "train loss:0.012960417034048847\n",
      "train loss:0.010357478225907619\n",
      "train loss:0.009888015742348094\n",
      "train loss:0.0082682274631733\n",
      "train loss:0.019290387351738186\n",
      "train loss:0.013789042805677951\n",
      "train loss:0.0391885431288597\n",
      "train loss:0.01933266717990083\n",
      "train loss:0.00866586935975601\n",
      "train loss:0.00965100013323238\n",
      "train loss:0.013162084254455999\n",
      "train loss:0.003822436514590434\n",
      "train loss:0.031106209067384885\n",
      "train loss:0.025994831180880983\n",
      "train loss:0.00662150860636452\n",
      "train loss:0.015694183372206825\n",
      "train loss:0.06321024680664031\n",
      "train loss:0.030215175414128517\n",
      "train loss:0.04434761314470734\n",
      "train loss:0.01147892941066916\n",
      "train loss:0.0062302369899793295\n",
      "train loss:0.004861329832625322\n",
      "train loss:0.014576688929286295\n",
      "train loss:0.020170529480442897\n",
      "train loss:0.01302236582865546\n",
      "train loss:0.021510496222434564\n",
      "train loss:0.012430881758927807\n",
      "train loss:0.05823784347695244\n",
      "train loss:0.012837630302116508\n",
      "train loss:0.027388018833505893\n",
      "train loss:0.002566869850742123\n",
      "train loss:0.003501549523045925\n",
      "train loss:0.005993380233495656\n",
      "train loss:0.005381430395554779\n",
      "train loss:0.03316239512955041\n",
      "train loss:0.0023546056550995094\n",
      "train loss:0.008122371972848425\n",
      "train loss:0.011058114022474815\n",
      "train loss:0.026357734594894048\n",
      "train loss:0.06866074294161845\n",
      "train loss:0.009780753417763592\n",
      "train loss:0.07534087168508384\n",
      "train loss:0.07316967812973663\n",
      "train loss:0.004233370442117556\n",
      "train loss:0.02758250703851969\n",
      "train loss:0.019118529824915367\n",
      "train loss:0.013745794331651417\n",
      "train loss:0.013275033401042784\n",
      "train loss:0.022053979890888368\n",
      "train loss:0.0065277210303080245\n",
      "train loss:0.010629649722323982\n",
      "train loss:0.058186605109166754\n",
      "train loss:0.029692264120212752\n",
      "train loss:0.05930999532820001\n",
      "train loss:0.08016482481585895\n",
      "train loss:0.003367612972231834\n",
      "train loss:0.009616397684784485\n",
      "train loss:0.04993516976373633\n",
      "train loss:0.03904765895511714\n",
      "=== epoch:6, train acc:0.988, test acc:0.987 ===\n",
      "train loss:0.01579615542112082\n",
      "train loss:0.024437062345312347\n",
      "train loss:0.02870761156490304\n",
      "train loss:0.0241243288351106\n",
      "train loss:0.002895108298623203\n",
      "train loss:0.013418810626297071\n",
      "train loss:0.06577195884707221\n",
      "train loss:0.006210958280523189\n",
      "train loss:0.010760059685048845\n",
      "train loss:0.01162440368792883\n",
      "train loss:0.008571136082637484\n",
      "train loss:0.010196400661905778\n",
      "train loss:0.01949421239912953\n",
      "train loss:0.08077219664345886\n",
      "train loss:0.011101115110401256\n",
      "train loss:0.029836006545548903\n",
      "train loss:0.02319276447584476\n",
      "train loss:0.018204252881054272\n",
      "train loss:0.00820215599299412\n",
      "train loss:0.040066885146767914\n",
      "train loss:0.02936416343123644\n",
      "train loss:0.053217968001211396\n",
      "train loss:0.022687227508003196\n",
      "train loss:0.018437697604014973\n",
      "train loss:0.009194063698171114\n",
      "train loss:0.00663262106956185\n",
      "train loss:0.01002389857326749\n",
      "train loss:0.01748364176329587\n",
      "train loss:0.036960980002062625\n",
      "train loss:0.009728751983135369\n",
      "train loss:0.007117048882488545\n",
      "train loss:0.006886209091650604\n",
      "train loss:0.0052245608891255835\n",
      "train loss:0.004457943343941396\n",
      "train loss:0.07431310833172433\n",
      "train loss:0.011557853524325085\n",
      "train loss:0.03200470826797926\n",
      "train loss:0.004955068015068681\n",
      "train loss:0.010617094400196689\n",
      "train loss:0.02113643145630081\n",
      "train loss:0.030666284869119112\n",
      "train loss:0.013265560244409082\n",
      "train loss:0.01553777490111365\n",
      "train loss:0.010772768422493724\n",
      "train loss:0.010830441328638998\n",
      "train loss:0.03243704241885161\n",
      "train loss:0.024071554961720202\n",
      "train loss:0.004113179423684241\n",
      "train loss:0.004501028126002035\n",
      "train loss:0.032016922654016816\n",
      "train loss:0.08387386172199891\n",
      "train loss:0.011021984740436332\n",
      "train loss:0.016170296120754536\n",
      "train loss:0.007611753698031195\n",
      "train loss:0.0049012876000275405\n",
      "train loss:0.006661757840736887\n",
      "train loss:0.011036853468777334\n",
      "train loss:0.0042699544538340704\n",
      "train loss:0.055971771583103784\n",
      "train loss:0.006396441375181385\n",
      "train loss:0.020629842737356668\n",
      "train loss:0.07710511154134635\n",
      "train loss:0.04891983812264741\n",
      "train loss:0.005242070613413582\n",
      "train loss:0.018812385991692764\n",
      "train loss:0.025486075286426234\n",
      "train loss:0.015117307682601094\n",
      "train loss:0.01050143593536455\n",
      "train loss:0.01918251367125\n",
      "train loss:0.09673859291401994\n",
      "train loss:0.005882614038160571\n",
      "train loss:0.017305708269075737\n",
      "train loss:0.010644206014701752\n",
      "train loss:0.007120836117734678\n",
      "train loss:0.00964894985514623\n",
      "train loss:0.03160857946379911\n",
      "train loss:0.012267981205986047\n",
      "train loss:0.008312967220463277\n",
      "train loss:0.0048454312113858046\n",
      "train loss:0.01951742147268902\n",
      "train loss:0.007340919372094059\n",
      "train loss:0.05318445699604931\n",
      "train loss:0.004680586999952137\n",
      "train loss:0.035992114791158436\n",
      "train loss:0.02061032860463646\n",
      "train loss:0.004594472129127768\n",
      "train loss:0.011557370714299275\n",
      "train loss:0.01503381636250732\n",
      "train loss:0.01699367935443325\n",
      "train loss:0.07587236062493974\n",
      "train loss:0.013475824457119446\n",
      "train loss:0.05245737354964988\n",
      "train loss:0.005562508915688328\n",
      "train loss:0.02876076747117378\n",
      "train loss:0.006297871679963516\n",
      "train loss:0.011702824407397991\n",
      "train loss:0.025008984514083635\n",
      "train loss:0.004936433348148155\n",
      "train loss:0.015095625219782485\n",
      "train loss:0.061394631653765755\n",
      "train loss:0.010045343114942658\n",
      "train loss:0.01597698858789438\n",
      "train loss:0.026033485420001733\n",
      "train loss:0.017466806332887767\n",
      "train loss:0.007034124227345955\n",
      "train loss:0.019278082895775567\n",
      "train loss:0.03093937716820488\n",
      "train loss:0.030958142403461238\n",
      "train loss:0.10141125664976247\n",
      "train loss:0.005601244434865062\n",
      "train loss:0.023419205178602128\n",
      "train loss:0.030164376955004644\n",
      "train loss:0.01624359681438503\n",
      "train loss:0.0090814127651953\n",
      "train loss:0.08283410899306222\n",
      "train loss:0.006531791642896546\n",
      "train loss:0.01586227863330445\n",
      "train loss:0.0008277697721326935\n",
      "train loss:0.003657265341030564\n",
      "train loss:0.010760509171037207\n",
      "train loss:0.011722190946625017\n",
      "train loss:0.014780293754581801\n",
      "train loss:0.046839253794548286\n",
      "train loss:0.0048212849567761005\n",
      "train loss:0.013763500051916455\n",
      "train loss:0.031028086253089192\n",
      "train loss:0.004182884430331987\n",
      "train loss:0.012552691755935907\n",
      "train loss:0.00935591667568568\n",
      "train loss:0.00642697924660087\n",
      "train loss:0.06142418720766289\n",
      "train loss:0.003814792501104794\n",
      "train loss:0.022904194012126955\n",
      "train loss:0.03562031378568196\n",
      "train loss:0.008279382948186571\n",
      "train loss:0.008277079838237824\n",
      "train loss:0.009178907953352411\n",
      "train loss:0.013163055612982805\n",
      "train loss:0.03366111629888764\n",
      "train loss:0.013390954859757888\n",
      "train loss:0.02075592866795883\n",
      "train loss:0.015213447431631215\n",
      "train loss:0.0023457934357303924\n",
      "train loss:0.04156575973948445\n",
      "train loss:0.06465924923619504\n",
      "train loss:0.005576878185681233\n",
      "train loss:0.009313576669245337\n",
      "train loss:0.007913523093462008\n",
      "train loss:0.005479109537582758\n",
      "train loss:0.012184662319993772\n",
      "train loss:0.010963585902410726\n",
      "train loss:0.015218026254035198\n",
      "train loss:0.050281125351082895\n",
      "train loss:0.031186689178649495\n",
      "train loss:0.014473673281274082\n",
      "train loss:0.01478013933917408\n",
      "train loss:0.023584756011669365\n",
      "train loss:0.006231734901263227\n",
      "train loss:0.01749344512962803\n",
      "train loss:0.03192766368018969\n",
      "train loss:0.023155947726002\n",
      "train loss:0.00286389153818163\n",
      "train loss:0.04086470170697962\n",
      "train loss:0.016234751731257067\n",
      "train loss:0.011748617189851936\n",
      "train loss:0.007171727601432178\n",
      "train loss:0.005375856825518006\n",
      "train loss:0.006220151195937207\n",
      "train loss:0.03560309214312071\n",
      "train loss:0.02215916112679005\n",
      "train loss:0.03325144894788525\n",
      "train loss:0.018884826193871492\n",
      "train loss:0.002166560944704896\n",
      "train loss:0.04819498085067166\n",
      "train loss:0.042472743478869146\n",
      "train loss:0.023166621391028736\n",
      "train loss:0.023163261622711255\n",
      "train loss:0.034431300483387106\n",
      "train loss:0.01770652710780779\n",
      "train loss:0.019143457286126833\n",
      "train loss:0.022568301245166206\n",
      "train loss:0.04938505989933361\n",
      "train loss:0.008868753694344282\n",
      "train loss:0.03965477270637001\n",
      "train loss:0.0031213213927008747\n",
      "train loss:0.04590444695408571\n",
      "train loss:0.018054028360886033\n",
      "train loss:0.10155613071548414\n",
      "train loss:0.015822452360248332\n",
      "train loss:0.008082493178998978\n",
      "train loss:0.003806236991579534\n",
      "train loss:0.007740903700841082\n",
      "train loss:0.00985277321559218\n",
      "train loss:0.04068209575882025\n",
      "train loss:0.044681580018861765\n",
      "train loss:0.01358367862182623\n",
      "train loss:0.010676575528789948\n",
      "train loss:0.005925803991221544\n",
      "train loss:0.02606717031072512\n",
      "train loss:0.03306835225123393\n",
      "train loss:0.01923286930973441\n",
      "train loss:0.07897130983034979\n",
      "train loss:0.006112291063721792\n",
      "train loss:0.014425866299075028\n",
      "train loss:0.0050243401642912685\n",
      "train loss:0.030963366459464462\n",
      "train loss:0.024580537142572422\n",
      "train loss:0.02357666196925975\n",
      "train loss:0.05042705797646205\n",
      "train loss:0.013867037709201113\n",
      "train loss:0.009840501089838066\n",
      "train loss:0.07303624811497743\n",
      "train loss:0.0033056480451161678\n",
      "train loss:0.013563970595557955\n",
      "train loss:0.010417041259090858\n",
      "train loss:0.02322388205775833\n",
      "train loss:0.008728286796557787\n",
      "train loss:0.03467598040551365\n",
      "train loss:0.018208106472854303\n",
      "train loss:0.01722093385846694\n",
      "train loss:0.10028614910416993\n",
      "train loss:0.03645282465738042\n",
      "train loss:0.01709614398927592\n",
      "train loss:0.05590646891846163\n",
      "train loss:0.009334085212868708\n",
      "train loss:0.020059539102838177\n",
      "train loss:0.021637319630418362\n",
      "train loss:0.02673713987116531\n",
      "train loss:0.006579163455663626\n",
      "train loss:0.025871967253070012\n",
      "train loss:0.05690277677549076\n",
      "train loss:0.034930655721439934\n",
      "train loss:0.0073091774837604675\n",
      "train loss:0.038056380205101194\n",
      "train loss:0.016406688701742306\n",
      "train loss:0.004638869230182839\n",
      "train loss:0.024015517350864653\n",
      "train loss:0.004184639996411229\n",
      "train loss:0.032168810659553244\n",
      "train loss:0.01976048534244339\n",
      "train loss:0.008768793107671908\n",
      "train loss:0.03441722234522117\n",
      "train loss:0.003929380440677238\n",
      "train loss:0.03533347633926529\n",
      "train loss:0.04030465417061643\n",
      "train loss:0.014774547189175768\n",
      "train loss:0.008081784593729961\n",
      "train loss:0.008754545386788848\n",
      "train loss:0.010726063539559533\n",
      "train loss:0.021273395536105157\n",
      "train loss:0.013035633038280998\n",
      "train loss:0.047763502824197696\n",
      "train loss:0.016112587031263725\n",
      "train loss:0.017939142484932558\n",
      "train loss:0.00869104802928345\n",
      "train loss:0.017192050553028734\n",
      "train loss:0.04530776345466153\n",
      "train loss:0.026455258154490492\n",
      "train loss:0.00422758197774599\n",
      "train loss:0.0038654823743082306\n",
      "train loss:0.009982666005068188\n",
      "train loss:0.014307647227533155\n",
      "train loss:0.031185134627999288\n",
      "train loss:0.05676640985099948\n",
      "train loss:0.009198395315953869\n",
      "train loss:0.029193763303734622\n",
      "train loss:0.026101961164394484\n",
      "train loss:0.011797436885022005\n",
      "train loss:0.0022889039820154903\n",
      "train loss:0.005687068018697328\n",
      "train loss:0.00792695101544832\n",
      "train loss:0.0155588680127735\n",
      "train loss:0.021914132097244146\n",
      "train loss:0.01047063639569329\n",
      "train loss:0.006576238173730681\n",
      "train loss:0.012207773652377147\n",
      "train loss:0.017475322753798473\n",
      "train loss:0.005346548052927588\n",
      "train loss:0.03801230244444871\n",
      "train loss:0.008660455243853275\n",
      "train loss:0.05452012366202722\n",
      "train loss:0.029532023614755397\n",
      "train loss:0.01056739659261783\n",
      "train loss:0.04737949186995182\n",
      "train loss:0.004124517900374127\n",
      "train loss:0.012659605341425349\n",
      "train loss:0.0036859855583349023\n",
      "train loss:0.032845694800771784\n",
      "train loss:0.009058579938675123\n",
      "train loss:0.027597095024913636\n",
      "train loss:0.04094516816601514\n",
      "train loss:0.016635391628940094\n",
      "train loss:0.027204572156865118\n",
      "train loss:0.02315521577076199\n",
      "train loss:0.013716976793881867\n",
      "train loss:0.05455105613698679\n",
      "train loss:0.028606169626490267\n",
      "train loss:0.02395068640547341\n",
      "train loss:0.030843199013650496\n",
      "train loss:0.0010027498231225548\n",
      "train loss:0.028004810171235198\n",
      "train loss:0.007709155198438522\n",
      "train loss:0.038555026072346184\n",
      "train loss:0.007737041826448002\n",
      "train loss:0.005501523123128021\n",
      "train loss:0.006061552616361674\n",
      "train loss:0.008479659435930854\n",
      "train loss:0.03209006709602532\n",
      "train loss:0.02362065453954366\n",
      "train loss:0.02501486588999381\n",
      "train loss:0.01688259346896427\n",
      "train loss:0.04834981861486422\n",
      "train loss:0.003734692559294843\n",
      "train loss:0.014932386331946722\n",
      "train loss:0.0071770386799254815\n",
      "train loss:0.010973745734763571\n",
      "train loss:0.00505499090186979\n",
      "train loss:0.006267634830064026\n",
      "train loss:0.030497928580091264\n",
      "train loss:0.01220268976261289\n",
      "train loss:0.046728777719673265\n",
      "train loss:0.01248719468222843\n",
      "train loss:0.013018412278124319\n",
      "train loss:0.020777490021084696\n",
      "train loss:0.03792290549350307\n",
      "train loss:0.016936814956231658\n",
      "train loss:0.010638463072358238\n",
      "train loss:0.005388695910595086\n",
      "train loss:0.03225302811627858\n",
      "train loss:0.02844471957456174\n",
      "train loss:0.010654657990223845\n",
      "train loss:0.013284040764519946\n",
      "train loss:0.007836523912621081\n",
      "train loss:0.01336542127014503\n",
      "train loss:0.020223062659136568\n",
      "train loss:0.018779598405074625\n",
      "train loss:0.01125482482016082\n",
      "train loss:0.05739331049584333\n",
      "train loss:0.012590948243745625\n",
      "train loss:0.067382194454663\n",
      "train loss:0.006334027819731581\n",
      "train loss:0.028853452699979493\n",
      "train loss:0.04544579905336677\n",
      "train loss:0.01445786532009542\n",
      "train loss:0.02629765463281676\n",
      "train loss:0.01028872976124446\n",
      "train loss:0.008982902648697716\n",
      "train loss:0.016239285222585997\n",
      "train loss:0.007679141087050804\n",
      "train loss:0.019661684291660892\n",
      "train loss:0.012825443565169925\n",
      "train loss:0.04988832926932186\n",
      "train loss:0.011775896463121908\n",
      "train loss:0.01416330115600659\n",
      "train loss:0.010445349843226266\n",
      "train loss:0.010881595423633983\n",
      "train loss:0.004430043448814681\n",
      "train loss:0.0032611896435873955\n",
      "train loss:0.004083632271939152\n",
      "train loss:0.01825495395878285\n",
      "train loss:0.01296638247620038\n",
      "train loss:0.014424607595374813\n",
      "train loss:0.012060826528291706\n",
      "train loss:0.014768977310743486\n",
      "train loss:0.07296219066187923\n",
      "train loss:0.01296333640916127\n",
      "train loss:0.004033297616036577\n",
      "train loss:0.0014738900518980358\n",
      "train loss:0.01951406267939534\n",
      "train loss:0.0054170606685876675\n",
      "train loss:0.01471508111055994\n",
      "train loss:0.005097971486451903\n",
      "train loss:0.017461832276219683\n",
      "train loss:0.02820026426162264\n",
      "train loss:0.005150205405454007\n",
      "train loss:0.07346974196341755\n",
      "train loss:0.0649194249929872\n",
      "train loss:0.10585137565867538\n",
      "train loss:0.01577090684098284\n",
      "train loss:0.004609089855891143\n",
      "train loss:0.04775317622640089\n",
      "train loss:0.003163618426335819\n",
      "train loss:0.1388323712935102\n",
      "train loss:0.03551651600785573\n",
      "train loss:0.014084211430180356\n",
      "train loss:0.006603984908794108\n",
      "train loss:0.003575382656815043\n",
      "train loss:0.009584622986823072\n",
      "train loss:0.013038286558571713\n",
      "train loss:0.003250544136440503\n",
      "train loss:0.009818183608664223\n",
      "train loss:0.02308697663291154\n",
      "train loss:0.0294023019145356\n",
      "train loss:0.005870592632195161\n",
      "train loss:0.03955944758192398\n",
      "train loss:0.01202697516087474\n",
      "train loss:0.019931741854112322\n",
      "train loss:0.010383115714990381\n",
      "train loss:0.02057630393972733\n",
      "train loss:0.01035716502799702\n",
      "train loss:0.004597038446636063\n",
      "train loss:0.02634172600562861\n",
      "train loss:0.011892590103098104\n",
      "train loss:0.023829761276872915\n",
      "train loss:0.006459060254471907\n",
      "train loss:0.04309170386406562\n",
      "train loss:0.0065619140749579585\n",
      "train loss:0.021611411240998076\n",
      "train loss:0.004277628950179186\n",
      "train loss:0.009249583723537241\n",
      "train loss:0.015511715953441514\n",
      "train loss:0.025196670959302808\n",
      "train loss:0.006178365418692457\n",
      "train loss:0.007126924961162722\n",
      "train loss:0.009763441089598097\n",
      "train loss:0.08705832508414649\n",
      "train loss:0.010604349357455698\n",
      "train loss:0.006804321594506013\n",
      "train loss:0.03579320312544235\n",
      "train loss:0.017591255642004363\n",
      "train loss:0.01562885409847821\n",
      "train loss:0.009623892806433975\n",
      "train loss:0.004073095234049203\n",
      "train loss:0.007268759787661993\n",
      "train loss:0.041022252170098854\n",
      "train loss:0.03472537306888661\n",
      "train loss:0.00703864020112001\n",
      "train loss:0.016896717779968384\n",
      "train loss:0.02540724735606343\n",
      "train loss:0.002013266135649712\n",
      "train loss:0.01786483281535385\n",
      "train loss:0.035572218147322435\n",
      "train loss:0.018748061435239497\n",
      "train loss:0.015365715127524411\n",
      "train loss:0.005573037983731586\n",
      "train loss:0.01725375773539673\n",
      "train loss:0.01186802106813631\n",
      "train loss:0.018754140914659545\n",
      "train loss:0.0032732090290614347\n",
      "train loss:0.03670350054282793\n",
      "train loss:0.01743666082720334\n",
      "train loss:0.0036858503496805628\n",
      "train loss:0.024442841462232787\n",
      "train loss:0.0053039459257390575\n",
      "train loss:0.002543114651881278\n",
      "train loss:0.024297055334117343\n",
      "train loss:0.002787627868658879\n",
      "train loss:0.04482284609368818\n",
      "train loss:0.041720474756405286\n",
      "train loss:0.0031970549777374157\n",
      "train loss:0.023951618004344498\n",
      "train loss:0.01558224600794506\n",
      "train loss:0.004858557968457325\n",
      "train loss:0.026977982841351097\n",
      "train loss:0.03174360918595146\n",
      "train loss:0.00789868420005403\n",
      "train loss:0.029624127555394657\n",
      "train loss:0.012623578074062936\n",
      "train loss:0.025651974535633903\n",
      "train loss:0.005667391783849989\n",
      "train loss:0.013770750214573503\n",
      "train loss:0.0019420234937065376\n",
      "train loss:0.04129424353377963\n",
      "train loss:0.048820394776627224\n",
      "train loss:0.020134118234922047\n",
      "train loss:0.0014990427414167218\n",
      "train loss:0.019950577542905825\n",
      "train loss:0.006938779515751978\n",
      "train loss:0.00492875618866273\n",
      "train loss:0.006003360456802225\n",
      "train loss:0.0053032338056022525\n",
      "train loss:0.003839856766074457\n",
      "train loss:0.016953315714584347\n",
      "train loss:0.020809394157685374\n",
      "train loss:0.024133865015454035\n",
      "train loss:0.006799313240438539\n",
      "train loss:0.021243299927409174\n",
      "train loss:0.058207827892639834\n",
      "train loss:0.02343885297326601\n",
      "train loss:0.009819082146407693\n",
      "train loss:0.012871454242099973\n",
      "train loss:0.007421465451034257\n",
      "train loss:0.013389541299093801\n",
      "train loss:0.004613817519662702\n",
      "train loss:0.016456497280740875\n",
      "train loss:0.042433366068955765\n",
      "train loss:0.01809560355783282\n",
      "train loss:0.0021550458458354294\n",
      "train loss:0.006040971319050764\n",
      "train loss:0.009299436382356766\n",
      "train loss:0.01552673565645434\n",
      "train loss:0.03125356429353092\n",
      "train loss:0.015944770665008864\n",
      "train loss:0.015469318319553844\n",
      "train loss:0.009703610298077066\n",
      "train loss:0.050180415994587536\n",
      "train loss:0.0326439480548583\n",
      "train loss:0.008344349955533993\n",
      "train loss:0.008513525835122387\n",
      "train loss:0.005797820264265747\n",
      "train loss:0.01640006496600339\n",
      "train loss:0.012569912557819777\n",
      "train loss:0.020656655181648182\n",
      "train loss:0.04134261188388269\n",
      "train loss:0.006719400343043969\n",
      "train loss:0.01550039986443701\n",
      "train loss:0.016948489650964862\n",
      "train loss:0.02235683094002574\n",
      "train loss:0.014050242037282794\n",
      "train loss:0.007294400675876709\n",
      "train loss:0.012250594751302808\n",
      "train loss:0.010625561683392081\n",
      "train loss:0.0037971348409190577\n",
      "train loss:0.027565841861488107\n",
      "train loss:0.014582466623170247\n",
      "train loss:0.0036223673835539274\n",
      "train loss:0.06722060498525381\n",
      "train loss:0.00931255801313696\n",
      "train loss:0.017546975250471136\n",
      "train loss:0.005896451090671678\n",
      "train loss:0.007471854459269378\n",
      "train loss:0.007699421633604943\n",
      "train loss:0.01751189514884949\n",
      "train loss:0.010496058825971403\n",
      "train loss:0.01880108146595818\n",
      "train loss:0.0048385791444481045\n",
      "train loss:0.008671033644911475\n",
      "train loss:0.0017604224940657455\n",
      "train loss:0.0016382110846568509\n",
      "train loss:0.003998020877009147\n",
      "train loss:0.004135895021324414\n",
      "train loss:0.0495977351723979\n",
      "train loss:0.019450565488988592\n",
      "train loss:0.002786142195217716\n",
      "train loss:0.007840446985327288\n",
      "train loss:0.028452693591505908\n",
      "train loss:0.008525507235043478\n",
      "train loss:0.003196415400296044\n",
      "train loss:0.0027905967673057504\n",
      "train loss:0.013909575555492135\n",
      "train loss:0.005101479869675059\n",
      "train loss:0.01428517547095151\n",
      "train loss:0.019581625645969533\n",
      "train loss:0.007987210908081854\n",
      "train loss:0.07903668087170858\n",
      "train loss:0.004200086657707988\n",
      "train loss:0.005952492974645625\n",
      "train loss:0.004635255195020212\n",
      "train loss:0.006836802084859522\n",
      "train loss:0.0011742936258044613\n",
      "train loss:0.04042168267543495\n",
      "train loss:0.011741180002241915\n",
      "train loss:0.014238147738247752\n",
      "train loss:0.012876525089547935\n",
      "train loss:0.0034918765735010688\n",
      "train loss:0.009159613121029038\n",
      "train loss:0.011679028073983055\n",
      "train loss:0.026530373267278074\n",
      "train loss:0.011522496350027183\n",
      "train loss:0.03810169550133361\n",
      "train loss:0.006990311063564701\n",
      "train loss:0.015620105183084989\n",
      "train loss:0.004345747943338547\n",
      "train loss:0.01822302431440397\n",
      "train loss:0.03626548823160148\n",
      "train loss:0.014356138968753334\n",
      "train loss:0.007211757851311524\n",
      "train loss:0.009188882314537768\n",
      "train loss:0.03021153228838999\n",
      "train loss:0.018059299490234704\n",
      "train loss:0.00952181280860997\n",
      "train loss:0.019844330064288044\n",
      "train loss:0.0027963296838157915\n",
      "train loss:0.016153431145561597\n",
      "train loss:0.01448344553409971\n",
      "train loss:0.031059134839812243\n",
      "train loss:0.010376817515581636\n",
      "train loss:0.016819482215831544\n",
      "train loss:0.010949775701035338\n",
      "train loss:0.019775353692121748\n",
      "train loss:0.028085772493599243\n",
      "train loss:0.002404068968100325\n",
      "train loss:0.0023175199681899506\n",
      "train loss:0.006592612036457792\n",
      "train loss:0.01245531052321267\n",
      "train loss:0.00603040630931715\n",
      "train loss:0.0036809395253925713\n",
      "train loss:0.007582721527267352\n",
      "train loss:0.007969516826631237\n",
      "train loss:0.024339298530888665\n",
      "train loss:0.013281546880821393\n",
      "train loss:0.004117739017525117\n",
      "train loss:0.04644163277669884\n",
      "train loss:0.014517508931895049\n",
      "train loss:0.021786659122812083\n",
      "train loss:0.003614930933591541\n",
      "train loss:0.006225210568085297\n",
      "train loss:0.023273897520457033\n",
      "train loss:0.00614357997336372\n",
      "train loss:0.004376330216108146\n",
      "=== epoch:7, train acc:0.991, test acc:0.983 ===\n",
      "train loss:0.0019453613673448007\n",
      "train loss:0.014218909306392516\n",
      "train loss:0.018610624230164723\n",
      "train loss:0.005826877270529108\n",
      "train loss:0.021119464814856763\n",
      "train loss:0.04940511932710583\n",
      "train loss:0.005815797703871671\n",
      "train loss:0.016622557111556347\n",
      "train loss:0.01645893644907334\n",
      "train loss:0.003629799778414716\n",
      "train loss:0.02052555157266351\n",
      "train loss:0.004394145250406299\n",
      "train loss:0.0009806170168165452\n",
      "train loss:0.04975531781004999\n",
      "train loss:0.004628689464510401\n",
      "train loss:0.15071241770272603\n",
      "train loss:0.003936740519956272\n",
      "train loss:0.023480141805930294\n",
      "train loss:0.0038874326997264726\n",
      "train loss:0.013803862109652227\n",
      "train loss:0.01122409728952724\n",
      "train loss:0.05545780526163838\n",
      "train loss:0.003323405247989755\n",
      "train loss:0.009472907028225858\n",
      "train loss:0.006219710438561852\n",
      "train loss:0.024261383394418478\n",
      "train loss:0.021724146623982764\n",
      "train loss:0.012857744662821776\n",
      "train loss:0.02341410750392676\n",
      "train loss:0.007643909597632459\n",
      "train loss:0.007544037858867412\n",
      "train loss:0.015046429475331406\n",
      "train loss:0.009819116363891139\n",
      "train loss:0.006155633716513782\n",
      "train loss:0.0023381715241438724\n",
      "train loss:0.006197554284693391\n",
      "train loss:0.0238493480000263\n",
      "train loss:0.010370158203832969\n",
      "train loss:0.0269097805589583\n",
      "train loss:0.01818192248148297\n",
      "train loss:0.0022484662044512747\n",
      "train loss:0.011032618700645916\n",
      "train loss:0.003491773664942222\n",
      "train loss:0.03935478256553728\n",
      "train loss:0.007837140632299571\n",
      "train loss:0.008550274475394758\n",
      "train loss:0.008409867289095645\n",
      "train loss:0.007292902152265488\n",
      "train loss:0.0008891158050242327\n",
      "train loss:0.005811174647989043\n",
      "train loss:0.006771737847212048\n",
      "train loss:0.01260538147533722\n",
      "train loss:0.00719727394030204\n",
      "train loss:0.013153877168213366\n",
      "train loss:0.00722334515453322\n",
      "train loss:0.013374978561776628\n",
      "train loss:0.017713601991246372\n",
      "train loss:0.0021291837520650723\n",
      "train loss:0.014828821794454519\n",
      "train loss:0.006532706769776181\n",
      "train loss:0.014594698936659751\n",
      "train loss:0.004880518973209707\n",
      "train loss:0.08932538982764339\n",
      "train loss:0.007978583000264964\n",
      "train loss:0.004976293039959213\n",
      "train loss:0.00748754024177791\n",
      "train loss:0.007471554766466258\n",
      "train loss:0.016969339758691592\n",
      "train loss:0.00550511591448481\n",
      "train loss:0.023101875475076503\n",
      "train loss:0.003121583290539695\n",
      "train loss:0.0019950544306069407\n",
      "train loss:0.014404590552684253\n",
      "train loss:0.007202024636678362\n",
      "train loss:0.011378706298929056\n",
      "train loss:0.019538726067466535\n",
      "train loss:0.040235241044164025\n",
      "train loss:0.013038054998487574\n",
      "train loss:0.0323849154977853\n",
      "train loss:0.027345194064297827\n",
      "train loss:0.013058782945255866\n",
      "train loss:0.007415299941527355\n",
      "train loss:0.02659799578576016\n",
      "train loss:0.012968930361685513\n",
      "train loss:0.005877396176003442\n",
      "train loss:0.0021596247655450824\n",
      "train loss:0.016274990960496304\n",
      "train loss:0.045208992717931204\n",
      "train loss:0.09813638163690439\n",
      "train loss:0.041841357565231656\n",
      "train loss:0.019009765147271103\n",
      "train loss:0.013719179959554698\n",
      "train loss:0.013508313028099052\n",
      "train loss:0.016067222583685956\n",
      "train loss:0.018627278754120916\n",
      "train loss:0.004256609529147669\n",
      "train loss:0.041309685844576005\n",
      "train loss:0.004914500712090933\n",
      "train loss:0.006666358183619364\n",
      "train loss:0.009076002534112497\n",
      "train loss:0.06382983034599203\n",
      "train loss:0.010869222748901593\n",
      "train loss:0.013226583497821402\n",
      "train loss:0.0033170282873138295\n",
      "train loss:0.004654544116346914\n",
      "train loss:0.020561285374599603\n",
      "train loss:0.026899909721728385\n",
      "train loss:0.018667351786318342\n",
      "train loss:0.009297449317508843\n",
      "train loss:0.00926855135115671\n",
      "train loss:0.007214855402294234\n",
      "train loss:0.05281425164396981\n",
      "train loss:0.01878049398986633\n",
      "train loss:0.0065611846984746924\n",
      "train loss:0.026018591730758055\n",
      "train loss:0.015565915698080069\n",
      "train loss:0.00536563320711021\n",
      "train loss:0.006301527879707268\n",
      "train loss:0.0021757837327804953\n",
      "train loss:0.0037849213432446134\n",
      "train loss:0.0033859727373747026\n",
      "train loss:0.006472758925668739\n",
      "train loss:0.004115316251599005\n",
      "train loss:0.009882639416671111\n",
      "train loss:0.06649830133962795\n",
      "train loss:0.023736101417540664\n",
      "train loss:0.006377098462918423\n",
      "train loss:0.03508590058898862\n",
      "train loss:0.02095759064779715\n",
      "train loss:0.021807986468173016\n",
      "train loss:0.0178480851374177\n",
      "train loss:0.009974199680012544\n",
      "train loss:0.007664334364418147\n",
      "train loss:0.010612969804673212\n",
      "train loss:0.02843649781190671\n",
      "train loss:0.014877625367665945\n",
      "train loss:0.016551652603940176\n",
      "train loss:0.0424681806061253\n",
      "train loss:0.007957588458589765\n",
      "train loss:0.00903347940411042\n",
      "train loss:0.03561943347755745\n",
      "train loss:0.003467870014180463\n",
      "train loss:0.011858697727593704\n",
      "train loss:0.00625923187528776\n",
      "train loss:0.025470758359056767\n",
      "train loss:0.006686203991243232\n",
      "train loss:0.018024206569301874\n",
      "train loss:0.07132674446168046\n",
      "train loss:0.0508316529722165\n",
      "train loss:0.009559504688052178\n",
      "train loss:0.011343588448912282\n",
      "train loss:0.03437244597682445\n",
      "train loss:0.019160898153198727\n",
      "train loss:0.042867825898799596\n",
      "train loss:0.0012036103253953627\n",
      "train loss:0.012166573445259184\n",
      "train loss:0.00979996012227366\n",
      "train loss:0.02858562247094742\n",
      "train loss:0.023989573903572183\n",
      "train loss:0.004449409864384272\n",
      "train loss:0.006744241781942452\n",
      "train loss:0.016716479391403805\n",
      "train loss:0.0017199683077947683\n",
      "train loss:0.006285702837005724\n",
      "train loss:0.006346073101064324\n",
      "train loss:0.038447467901973724\n",
      "train loss:0.04531673373229885\n",
      "train loss:0.018468944851128984\n",
      "train loss:0.01708417448757588\n",
      "train loss:0.005111247746237996\n",
      "train loss:0.020961853704817384\n",
      "train loss:0.012174658362444725\n",
      "train loss:0.023690099132292523\n",
      "train loss:0.06753984499452534\n",
      "train loss:0.008460383202994763\n",
      "train loss:0.019760139344096216\n",
      "train loss:0.021314332448963663\n",
      "train loss:0.08743739905020843\n",
      "train loss:0.0076357675255057525\n",
      "train loss:0.012326861202106038\n",
      "train loss:0.01859751556655488\n",
      "train loss:0.02056375722526698\n",
      "train loss:0.004106043346333805\n",
      "train loss:0.00831149276771088\n",
      "train loss:0.006690455882735088\n",
      "train loss:0.0039000207415832797\n",
      "train loss:0.021025967505826085\n",
      "train loss:0.010226309558329684\n",
      "train loss:0.024707071898865136\n",
      "train loss:0.006263877689063949\n",
      "train loss:0.01444878097470022\n",
      "train loss:0.023190306816340236\n",
      "train loss:0.02718544114344363\n",
      "train loss:0.009555464828135402\n",
      "train loss:0.09613815759976248\n",
      "train loss:0.01891556444497019\n",
      "train loss:0.0049734566042894625\n",
      "train loss:0.011226487786462618\n",
      "train loss:0.006139053873576045\n",
      "train loss:0.020398785244541827\n",
      "train loss:0.013271913364926502\n",
      "train loss:0.0005715273507566817\n",
      "train loss:0.036504301478612874\n",
      "train loss:0.011142430507542163\n",
      "train loss:0.03341864428933821\n",
      "train loss:0.0333691753666142\n",
      "train loss:0.013687115999882218\n",
      "train loss:0.03347703788508719\n",
      "train loss:0.013170806556139357\n",
      "train loss:0.03353944249547416\n",
      "train loss:0.00909141484949303\n",
      "train loss:0.007353526429084802\n",
      "train loss:0.009447756340167878\n",
      "train loss:0.024588320572622543\n",
      "train loss:0.014889345428668816\n",
      "train loss:0.01291036411245931\n",
      "train loss:0.008499233542597876\n",
      "train loss:0.0020480213467728315\n",
      "train loss:0.024126116757323315\n",
      "train loss:0.002598956947865652\n",
      "train loss:0.014986722291397993\n",
      "train loss:0.01163304092892417\n",
      "train loss:0.003808809153581628\n",
      "train loss:0.019553806644199406\n",
      "train loss:0.0016816214626328072\n",
      "train loss:0.003810703971075321\n",
      "train loss:0.012228470753128272\n",
      "train loss:0.014009910165883535\n",
      "train loss:0.010118655710640145\n",
      "train loss:0.006403575624373632\n",
      "train loss:0.019241166736436256\n",
      "train loss:0.01058261549940122\n",
      "train loss:0.007409328746962765\n",
      "train loss:0.020763481479023585\n",
      "train loss:0.004741432133004051\n",
      "train loss:0.026048102198865393\n",
      "train loss:0.016025013071167046\n",
      "train loss:0.008738851813028583\n",
      "train loss:0.009586265692515026\n",
      "train loss:0.022567666003996054\n",
      "train loss:0.00829920342062472\n",
      "train loss:0.016537937699163004\n",
      "train loss:0.010885571386627277\n",
      "train loss:0.006860345575483096\n",
      "train loss:0.00969654697051214\n",
      "train loss:0.002770801684791149\n",
      "train loss:0.017004853369922426\n",
      "train loss:0.030763783130586683\n",
      "train loss:0.006030750193449943\n",
      "train loss:0.010103160082712644\n",
      "train loss:0.012091613483829923\n",
      "train loss:0.0676344313952626\n",
      "train loss:0.005315913759222826\n",
      "train loss:0.016018411400921518\n",
      "train loss:0.006193129642429935\n",
      "train loss:0.004347375982262634\n",
      "train loss:0.010024364977565916\n",
      "train loss:0.021975538561728354\n",
      "train loss:0.014079962129039043\n",
      "train loss:0.013148385758821726\n",
      "train loss:0.0021421245579533333\n",
      "train loss:0.013576795843710686\n",
      "train loss:0.0098229173290035\n",
      "train loss:0.004195516926106136\n",
      "train loss:0.007474714780766831\n",
      "train loss:0.00847695653488835\n",
      "train loss:0.006730627479456139\n",
      "train loss:0.014007903692115591\n",
      "train loss:0.015668407857306714\n",
      "train loss:0.008649439556471242\n",
      "train loss:0.008567817897503587\n",
      "train loss:0.013482263813806652\n",
      "train loss:0.010153637313667523\n",
      "train loss:0.05084634090508873\n",
      "train loss:0.006231618066928173\n",
      "train loss:0.009255319464034436\n",
      "train loss:0.008898827521378\n",
      "train loss:0.0185665178224387\n",
      "train loss:0.05622712813232281\n",
      "train loss:0.015619301251251749\n",
      "train loss:0.013764987197572734\n",
      "train loss:0.02813573795952343\n",
      "train loss:0.008717128761415586\n",
      "train loss:0.006618777111613272\n",
      "train loss:0.0036020502863012786\n",
      "train loss:0.027818100559131177\n",
      "train loss:0.029977798284108514\n",
      "train loss:0.0958321977409585\n",
      "train loss:0.014374697658633986\n",
      "train loss:0.004521858438885199\n",
      "train loss:0.011992322656819742\n",
      "train loss:0.005055255971408751\n",
      "train loss:0.014500630724708703\n",
      "train loss:0.003920342346536616\n",
      "train loss:0.002700131106003893\n",
      "train loss:0.002792674442009819\n",
      "train loss:0.007019376813065125\n",
      "train loss:0.00946466904532137\n",
      "train loss:0.010112236408675056\n",
      "train loss:0.029912093947007504\n",
      "train loss:0.021883422038836295\n",
      "train loss:0.008280777800840267\n",
      "train loss:0.014663328948498812\n",
      "train loss:0.03594256881854153\n",
      "train loss:0.018539977546051037\n",
      "train loss:0.026109494846154133\n",
      "train loss:0.02778273992589275\n",
      "train loss:0.07486800534641076\n",
      "train loss:0.04389034970527229\n",
      "train loss:0.008544786134030473\n",
      "train loss:0.002945887609832641\n",
      "train loss:0.008680148376534894\n",
      "train loss:0.01460883922328726\n",
      "train loss:0.02319108696423671\n",
      "train loss:0.04221319696969657\n",
      "train loss:0.0032677983151644823\n",
      "train loss:0.017395330644692054\n",
      "train loss:0.004568867983774354\n",
      "train loss:0.006722366278639472\n",
      "train loss:0.007936825274340167\n",
      "train loss:0.006104910608262619\n",
      "train loss:0.020522195700490048\n",
      "train loss:0.014151957357894667\n",
      "train loss:0.016204742232128776\n",
      "train loss:0.04346871847668937\n",
      "train loss:0.00511602473646967\n",
      "train loss:0.07398071363843793\n",
      "train loss:0.020676790536823555\n",
      "train loss:0.013870954987971756\n",
      "train loss:0.026818799529926385\n",
      "train loss:0.02841351549848549\n",
      "train loss:0.007448289164225742\n",
      "train loss:0.011425013569094217\n",
      "train loss:0.0032180542755408294\n",
      "train loss:0.013951719905200643\n",
      "train loss:0.010125439594477459\n",
      "train loss:0.007777307297878375\n",
      "train loss:0.0072657880202672285\n",
      "train loss:0.01732821109616435\n",
      "train loss:0.02994762219029609\n",
      "train loss:0.00266895902972835\n",
      "train loss:0.04446197173420474\n",
      "train loss:0.012229110544341488\n",
      "train loss:0.04671806752616536\n",
      "train loss:0.019986969405696133\n",
      "train loss:0.008903862257699305\n",
      "train loss:0.004713856622844634\n",
      "train loss:0.009489010805226962\n",
      "train loss:0.0022673943671572766\n",
      "train loss:0.0036619302602218058\n",
      "train loss:0.02282991189371706\n",
      "train loss:0.015141113457263412\n",
      "train loss:0.0031505986677993953\n",
      "train loss:0.02683111491592021\n",
      "train loss:0.016970124328782376\n",
      "train loss:0.005406199260453555\n",
      "train loss:0.009895215675788209\n",
      "train loss:0.018466980134926404\n",
      "train loss:0.001558962374841042\n",
      "train loss:0.008673943345595791\n",
      "train loss:0.011573415033265913\n",
      "train loss:0.007249246056345768\n",
      "train loss:0.007124003583485749\n",
      "train loss:0.002701371811874923\n",
      "train loss:0.011076207477854294\n",
      "train loss:0.003742858744992835\n",
      "train loss:0.015070803239302079\n",
      "train loss:0.0028236516613690445\n",
      "train loss:0.022802817513832226\n",
      "train loss:0.011670009441892422\n",
      "train loss:0.0030820799492248634\n",
      "train loss:0.011875643580168743\n",
      "train loss:0.009941533296169253\n",
      "train loss:0.009391867530882284\n",
      "train loss:0.007107598205456198\n",
      "train loss:0.0012851002965656936\n",
      "train loss:0.027694615014967075\n",
      "train loss:0.0456956742101378\n",
      "train loss:0.02395863135947757\n",
      "train loss:0.010536045310991234\n",
      "train loss:0.04057883927649784\n",
      "train loss:0.0035390166829222676\n",
      "train loss:0.033119337599603234\n",
      "train loss:0.055966015446547966\n",
      "train loss:0.02863427971712197\n",
      "train loss:0.006843856757927076\n",
      "train loss:0.012601661675243507\n",
      "train loss:0.0458733178882703\n",
      "train loss:0.00209103655667409\n",
      "train loss:0.0032803540118070923\n",
      "train loss:0.006931876684183595\n",
      "train loss:0.01314756070835468\n",
      "train loss:0.015349154237918905\n",
      "train loss:0.013942076922855042\n",
      "train loss:0.006041167085358024\n",
      "train loss:0.007316565878679576\n",
      "train loss:0.012706138843521113\n",
      "train loss:0.01804042080943981\n",
      "train loss:0.004883699304175812\n",
      "train loss:0.07469997985484818\n",
      "train loss:0.009596352810837854\n",
      "train loss:0.01873227978172183\n",
      "train loss:0.016868977143316484\n",
      "train loss:0.016711395152237102\n",
      "train loss:0.014482721927803028\n",
      "train loss:0.002011337022533409\n",
      "train loss:0.00871653681387765\n",
      "train loss:0.009146559531505852\n",
      "train loss:0.013523919108932178\n",
      "train loss:0.005945434637627724\n",
      "train loss:0.08218773048175514\n",
      "train loss:0.011542443162814631\n",
      "train loss:0.0441563257538507\n",
      "train loss:0.026825281839679548\n",
      "train loss:0.009383607492883477\n",
      "train loss:0.001968987155439921\n",
      "train loss:0.01594006880787874\n",
      "train loss:0.010835440733501078\n",
      "train loss:0.0064990427866993715\n",
      "train loss:0.007692048353331093\n",
      "train loss:0.012932424904411911\n",
      "train loss:0.008113925547566044\n",
      "train loss:0.029566625650024735\n",
      "train loss:0.005528329402717668\n",
      "train loss:0.01337633440345757\n",
      "train loss:0.004603243051413548\n",
      "train loss:0.01093053533845902\n",
      "train loss:0.002449970604817149\n",
      "train loss:0.03777391129423864\n",
      "train loss:0.0019101690283876078\n",
      "train loss:0.029243560825989282\n",
      "train loss:0.012199095876655179\n",
      "train loss:0.003984681600184752\n",
      "train loss:0.007900441883110372\n",
      "train loss:0.003984608466770649\n",
      "train loss:0.0036233029296550267\n",
      "train loss:0.0021595424863883055\n",
      "train loss:0.051904071153846744\n",
      "train loss:0.002637876070480375\n",
      "train loss:0.004975688457630047\n",
      "train loss:0.01376239496857849\n",
      "train loss:0.0066010813618655574\n",
      "train loss:0.002404007170162378\n",
      "train loss:0.040995105174587695\n",
      "train loss:0.019553105221637983\n",
      "train loss:0.008793506197403458\n",
      "train loss:0.004754335598399399\n",
      "train loss:0.015098905385221826\n",
      "train loss:0.002743403785670957\n",
      "train loss:0.03666197744338745\n",
      "train loss:0.018340809742252207\n",
      "train loss:0.010583211595633336\n",
      "train loss:0.0035539753241085183\n",
      "train loss:0.006053988479936146\n",
      "train loss:0.03687340784818202\n",
      "train loss:0.056651523247630814\n",
      "train loss:0.003345497139218261\n",
      "train loss:0.007593174815514139\n",
      "train loss:0.004544654770619953\n",
      "train loss:0.012875828238022262\n",
      "train loss:0.05843280087408378\n",
      "train loss:0.021987514451274177\n",
      "train loss:0.03664622737504514\n",
      "train loss:0.007286098028769361\n",
      "train loss:0.014407258530033046\n",
      "train loss:0.013229041140651538\n",
      "train loss:0.00804289304597402\n",
      "train loss:0.01748201851761959\n",
      "train loss:0.0036941894795430937\n",
      "train loss:0.020556410457650377\n",
      "train loss:0.01296517625122436\n",
      "train loss:0.013108621468331368\n",
      "train loss:0.04865061759813385\n",
      "train loss:0.02575451954162646\n",
      "train loss:0.009739875295450354\n",
      "train loss:0.006259546129144072\n",
      "train loss:0.0014957160521617227\n",
      "train loss:0.005510810498621184\n",
      "train loss:0.0012957954529981038\n",
      "train loss:0.005688729285219627\n",
      "train loss:0.01128904360326677\n",
      "train loss:0.005695335574228724\n",
      "train loss:0.0038732434890402703\n",
      "train loss:0.006045561465783285\n",
      "train loss:0.04424418243615782\n",
      "train loss:0.005326761051979395\n",
      "train loss:0.006757638944573954\n",
      "train loss:0.002486732129223932\n",
      "train loss:0.016211544070186663\n",
      "train loss:0.004814230107426122\n",
      "train loss:0.004223154348501713\n",
      "train loss:0.0051990628701087584\n",
      "train loss:0.0366996904851816\n",
      "train loss:0.02678633479443464\n",
      "train loss:0.05240354149842803\n",
      "train loss:0.019511176080619815\n",
      "train loss:0.005967727276716232\n",
      "train loss:0.01559683643310089\n",
      "train loss:0.013751022590297675\n",
      "train loss:0.004056642347023779\n",
      "train loss:0.025944757581968795\n",
      "train loss:0.006083019858750939\n",
      "train loss:0.0013409611352143823\n",
      "train loss:0.01183781035922754\n",
      "train loss:0.09239412012544024\n",
      "train loss:0.009849308839183961\n",
      "train loss:0.019539816318896414\n",
      "train loss:0.014286084972846788\n",
      "train loss:0.0033604282272093927\n",
      "train loss:0.0023501397105691285\n",
      "train loss:0.003960683340756853\n",
      "train loss:0.014044055615479738\n",
      "train loss:0.03593897444346433\n",
      "train loss:0.00790397505136512\n",
      "train loss:0.010528600209178072\n",
      "train loss:0.028292805691489106\n",
      "train loss:0.02329121290423711\n",
      "train loss:0.002056797181019467\n",
      "train loss:0.05082608770482455\n",
      "train loss:0.0038308482341922584\n",
      "train loss:0.08963644505624956\n",
      "train loss:0.007008743759104469\n",
      "train loss:0.01757935359123257\n",
      "train loss:0.047661519413940534\n",
      "train loss:0.004276766509766972\n",
      "train loss:0.02732427145295373\n",
      "train loss:0.009360666363171314\n",
      "train loss:0.00841420353086008\n",
      "train loss:0.005722872583159904\n",
      "train loss:0.015393209230674785\n",
      "train loss:0.003973455383595078\n",
      "train loss:0.056344078599676724\n",
      "train loss:0.008398268214113151\n",
      "train loss:0.0013824754416958197\n",
      "train loss:0.01312775592116052\n",
      "train loss:0.0025756867170319666\n",
      "train loss:0.0062789343644495624\n",
      "train loss:0.03434160238111426\n",
      "train loss:0.012686080009724907\n",
      "train loss:0.012505481699209875\n",
      "train loss:0.00914982250472546\n",
      "train loss:0.009994045389242893\n",
      "train loss:0.008771033355948346\n",
      "train loss:0.023731101653765463\n",
      "train loss:0.03617274401279819\n",
      "train loss:0.013050967858475258\n",
      "train loss:0.015028423124302923\n",
      "train loss:0.06315045402178686\n",
      "train loss:0.0034233677577006094\n",
      "train loss:0.07429009438477858\n",
      "train loss:0.008547024017552737\n",
      "train loss:0.012152031913684453\n",
      "train loss:0.013220298879268877\n",
      "train loss:0.025071612796541534\n",
      "train loss:0.009188622307899524\n",
      "train loss:0.0066738474781452995\n",
      "train loss:0.01130703559268103\n",
      "train loss:0.00798795978221313\n",
      "train loss:0.019097779343685523\n",
      "train loss:0.023297102603342758\n",
      "train loss:0.0018825206000731855\n",
      "train loss:0.006349783132298404\n",
      "train loss:0.01697753338540247\n",
      "train loss:0.008427304294402539\n",
      "train loss:0.0037723660364304373\n",
      "train loss:0.018295162349375175\n",
      "train loss:0.017776295264998555\n",
      "train loss:0.009991720000687744\n",
      "train loss:0.023129099763582017\n",
      "train loss:0.03717785489744873\n",
      "train loss:0.009182354087270719\n",
      "train loss:0.011224212943913257\n",
      "train loss:0.007557466042804819\n",
      "train loss:0.00827581521393042\n",
      "train loss:0.022937025454891097\n",
      "train loss:0.006903851530928829\n",
      "train loss:0.007750322622189052\n",
      "train loss:0.021930815425014766\n",
      "train loss:0.00406261828186615\n",
      "train loss:0.0062605971271005745\n",
      "train loss:0.004906484274115012\n",
      "train loss:0.0036359257688929635\n",
      "train loss:0.006729701713255293\n",
      "train loss:0.0024381151243809767\n",
      "train loss:0.010508567990905167\n",
      "train loss:0.017624538329414585\n",
      "train loss:0.01252810160069801\n",
      "train loss:0.03164114071537396\n",
      "train loss:0.0037023617409520713\n",
      "train loss:0.012974286610286831\n",
      "train loss:0.01401711145358009\n",
      "train loss:0.008509234709107241\n",
      "train loss:0.006953988285651133\n",
      "train loss:0.002996261797670287\n",
      "train loss:0.0011800991762636198\n",
      "train loss:0.006248279431263905\n",
      "train loss:0.00819014587096488\n",
      "train loss:0.02029029152730874\n",
      "train loss:0.016850322434126666\n",
      "train loss:0.025775720741292342\n",
      "=== epoch:8, train acc:0.989, test acc:0.989 ===\n",
      "train loss:0.0568177257680593\n",
      "train loss:0.013971151171270004\n",
      "train loss:0.005397037363464862\n",
      "train loss:0.0027431236552210993\n",
      "train loss:0.01678346285368779\n",
      "train loss:0.019790406292534483\n",
      "train loss:0.004473452936179258\n",
      "train loss:0.010410236818014755\n",
      "train loss:0.005664076367233107\n",
      "train loss:0.0033810508888759737\n",
      "train loss:0.037646269529334836\n",
      "train loss:0.009628871633391205\n",
      "train loss:0.028791342313311142\n",
      "train loss:0.010977654677615111\n",
      "train loss:0.009187077972732948\n",
      "train loss:0.010338555411974412\n",
      "train loss:0.009933839925319175\n",
      "train loss:0.0038374205590934562\n",
      "train loss:0.0040032567480663885\n",
      "train loss:0.028915380364819044\n",
      "train loss:0.0023866278565918783\n",
      "train loss:0.0011894762608479763\n",
      "train loss:0.013357364746001334\n",
      "train loss:0.03758242007445338\n",
      "train loss:0.0031804118783510006\n",
      "train loss:0.009533486994199078\n",
      "train loss:0.010856847081486607\n",
      "train loss:0.0273580959924559\n",
      "train loss:0.019979982787179516\n",
      "train loss:0.004014467021535774\n",
      "train loss:0.02884388975170852\n",
      "train loss:0.012705333867678885\n",
      "train loss:0.008661831583449776\n",
      "train loss:0.00391833094445509\n",
      "train loss:0.0019509282940401696\n",
      "train loss:0.009787331882348567\n",
      "train loss:0.011129607435740943\n",
      "train loss:0.010988728961279101\n",
      "train loss:0.05133302130940743\n",
      "train loss:0.007628851702376559\n",
      "train loss:0.006381710822424501\n",
      "train loss:0.003611793785874592\n",
      "train loss:0.00590723007567444\n",
      "train loss:0.07100771421464741\n",
      "train loss:0.010939926931977307\n",
      "train loss:0.02081511672969919\n",
      "train loss:0.06250947461111239\n",
      "train loss:0.00824343032809721\n",
      "train loss:0.07158829048040252\n",
      "train loss:0.0032186090600453475\n",
      "train loss:0.015880023845775387\n",
      "train loss:0.004342831656317854\n",
      "train loss:0.003878284563732047\n",
      "train loss:0.010269542724556668\n",
      "train loss:0.01601729091794532\n",
      "train loss:0.004133292197319286\n",
      "train loss:0.0030011874740737666\n",
      "train loss:0.0021936403337772126\n",
      "train loss:0.028360779422216046\n",
      "train loss:0.007809125398547196\n",
      "train loss:0.0026847675448070114\n",
      "train loss:0.01859034080284454\n",
      "train loss:0.00304276363248762\n",
      "train loss:0.014615874713815083\n",
      "train loss:0.0070904797470088795\n",
      "train loss:0.019147161481870706\n",
      "train loss:0.0017445555526330412\n",
      "train loss:0.008889465848613925\n",
      "train loss:0.005796244507249228\n",
      "train loss:0.007267797609238906\n",
      "train loss:0.013709151613865869\n",
      "train loss:0.010646864288858178\n",
      "train loss:0.007500326247571414\n",
      "train loss:0.00565393914161101\n",
      "train loss:0.005349873191876313\n",
      "train loss:0.041331601436268704\n",
      "train loss:0.010775465807107042\n",
      "train loss:0.023687038654595493\n",
      "train loss:0.003982928319836308\n",
      "train loss:0.025631766508310127\n",
      "train loss:0.0444733578594696\n",
      "train loss:0.018845294807869978\n",
      "train loss:0.01126339524671403\n",
      "train loss:0.010887788721363544\n",
      "train loss:0.00926737249297874\n",
      "train loss:0.0073666507317591045\n",
      "train loss:0.0028756037636300698\n",
      "train loss:0.0049435434860914396\n",
      "train loss:0.002658738973333062\n",
      "train loss:0.012562638958095207\n",
      "train loss:0.0024263487491797044\n",
      "train loss:0.006272680897782078\n",
      "train loss:0.002117011818159752\n",
      "train loss:0.01295776266767571\n",
      "train loss:0.002691073439146289\n",
      "train loss:0.013611170945043868\n",
      "train loss:0.002328756304247839\n",
      "train loss:0.00873118402014313\n",
      "train loss:0.0005937721373982468\n",
      "train loss:0.006868275790844832\n",
      "train loss:0.003961368687655062\n",
      "train loss:0.007776613206002643\n",
      "train loss:0.06142437675757629\n",
      "train loss:0.009008052254271115\n",
      "train loss:0.008635390182558484\n",
      "train loss:0.04864432671278135\n",
      "train loss:0.0038845402973709535\n",
      "train loss:0.02230261569703978\n",
      "train loss:0.003211290031507173\n",
      "train loss:0.01875981287606933\n",
      "train loss:0.013856751998478753\n",
      "train loss:0.010035009593177269\n",
      "train loss:0.011547447582229944\n",
      "train loss:0.006904976107021145\n",
      "train loss:0.006637514343776943\n",
      "train loss:0.05022693910096944\n",
      "train loss:0.0035045293954233813\n",
      "train loss:0.0016111246851625515\n",
      "train loss:0.013346302911919851\n",
      "train loss:0.015312581640870258\n",
      "train loss:0.007609392814744711\n",
      "train loss:0.030564082495829162\n",
      "train loss:0.0019592387908139498\n",
      "train loss:0.05289330131354081\n",
      "train loss:0.017218230708835987\n",
      "train loss:0.004802950119306031\n",
      "train loss:0.0030089019386953103\n",
      "train loss:0.029639524684326915\n",
      "train loss:0.0021686201243890156\n",
      "train loss:0.017208066294860772\n",
      "train loss:0.004501491754979948\n",
      "train loss:0.012875997681569273\n",
      "train loss:0.0020543220448743264\n",
      "train loss:0.03219024170793969\n",
      "train loss:0.013891364504068011\n",
      "train loss:0.008730976640581412\n",
      "train loss:0.06786525975797283\n",
      "train loss:0.015565818427835134\n",
      "train loss:0.003895880118472211\n",
      "train loss:0.01607024783587406\n",
      "train loss:0.013837672527578471\n",
      "train loss:0.00935048226422745\n",
      "train loss:0.004866996985801836\n",
      "train loss:0.00726785965202996\n",
      "train loss:0.0439687877136361\n",
      "train loss:0.019213968796814675\n",
      "train loss:0.008847755130929417\n",
      "train loss:0.001724604916919789\n",
      "train loss:0.005010821831893423\n",
      "train loss:0.029046131934150413\n",
      "train loss:0.003092052630614196\n",
      "train loss:0.009862940582746707\n",
      "train loss:0.054954330150393596\n",
      "train loss:0.017492809369485128\n",
      "train loss:0.021107001442000096\n",
      "train loss:0.009475566693044212\n",
      "train loss:0.006669581701025987\n",
      "train loss:0.0067259286196472\n",
      "train loss:0.0014801979042626554\n",
      "train loss:0.004693035506178592\n",
      "train loss:0.00601073042535602\n",
      "train loss:0.0020371156040202947\n",
      "train loss:0.007105183310629499\n",
      "train loss:0.011332162932022223\n",
      "train loss:0.028771536276341034\n",
      "train loss:0.006416205967762454\n",
      "train loss:0.002867743230954164\n",
      "train loss:0.0007797591531202874\n",
      "train loss:0.003100097914037393\n",
      "train loss:0.013339008617328608\n",
      "train loss:0.0025438287293515064\n",
      "train loss:0.015380597212217772\n",
      "train loss:0.02172854225880481\n",
      "train loss:0.011278301430715508\n",
      "train loss:0.013962228283580418\n",
      "train loss:0.01932600276838116\n",
      "train loss:0.029351913682432836\n",
      "train loss:0.0047651305975862715\n",
      "train loss:0.01712084257106607\n",
      "train loss:0.013916867447806697\n",
      "train loss:0.007387484261079634\n",
      "train loss:0.008838107172476965\n",
      "train loss:0.0021533661879535666\n",
      "train loss:0.0033474452408664133\n",
      "train loss:0.006879599291199158\n",
      "train loss:0.010878026666239007\n",
      "train loss:0.017875921533357247\n",
      "train loss:0.00236653041583584\n",
      "train loss:0.0447526464730888\n",
      "train loss:0.022809399308721594\n",
      "train loss:0.005338330900975291\n",
      "train loss:0.00932121794276269\n",
      "train loss:0.01682500315557213\n",
      "train loss:0.001965954888738178\n",
      "train loss:0.008582737790333382\n",
      "train loss:0.0054419307723255015\n",
      "train loss:0.006379593849621981\n",
      "train loss:0.015543518915056903\n",
      "train loss:0.012298585427454368\n",
      "train loss:0.04552225417595583\n",
      "train loss:0.01625791883962263\n",
      "train loss:0.001990032047437369\n",
      "train loss:0.008820507225211868\n",
      "train loss:0.001995155939224282\n",
      "train loss:0.002895401318494453\n",
      "train loss:0.03595049973847439\n",
      "train loss:0.0017470802049458936\n",
      "train loss:0.0018588797039947869\n",
      "train loss:0.017868141570375716\n",
      "train loss:0.0014694596837206545\n",
      "train loss:0.011915560570197414\n",
      "train loss:0.009845611928573684\n",
      "train loss:0.03119496784670878\n",
      "train loss:0.007933021846436308\n",
      "train loss:0.027603819952576485\n",
      "train loss:0.01719874589458078\n",
      "train loss:0.008356318267783842\n",
      "train loss:0.0014499380500179759\n",
      "train loss:0.008171136055059007\n",
      "train loss:0.021959309421342672\n",
      "train loss:0.012482753638743665\n",
      "train loss:0.0015436881864900579\n",
      "train loss:0.0050334206331306185\n",
      "train loss:0.014874807474756328\n",
      "train loss:0.004843715826276979\n",
      "train loss:0.007863642418803298\n",
      "train loss:0.010854159747144274\n",
      "train loss:0.007958335105575033\n",
      "train loss:0.01676423513743206\n",
      "train loss:0.004129569734770434\n",
      "train loss:0.007631181126987256\n",
      "train loss:0.004321956990986923\n",
      "train loss:0.007022915119698016\n",
      "train loss:0.004898390162787109\n",
      "train loss:0.00760756958653572\n",
      "train loss:0.014118035242897242\n",
      "train loss:0.042087096824605014\n",
      "train loss:0.008559932632478931\n",
      "train loss:0.01007299355091882\n",
      "train loss:0.006278424301933525\n",
      "train loss:0.03023059890567352\n",
      "train loss:0.004113175242647962\n",
      "train loss:0.004256943277327383\n",
      "train loss:0.007140873296110042\n",
      "train loss:0.01763372797324747\n",
      "train loss:0.008915601448256438\n",
      "train loss:0.0049950064864989095\n",
      "train loss:0.004759454802189661\n",
      "train loss:0.012385370106585409\n",
      "train loss:0.006593854937392528\n",
      "train loss:0.034197362778326384\n",
      "train loss:0.005740827561215659\n",
      "train loss:0.001755017277700932\n",
      "train loss:0.004318170499793087\n",
      "train loss:0.006009877094664038\n",
      "train loss:0.00463697666798698\n",
      "train loss:0.01466935233916545\n",
      "train loss:0.023686372787738504\n",
      "train loss:0.002646025735549037\n",
      "train loss:0.0035883790654714577\n",
      "train loss:0.0074690920465410844\n",
      "train loss:0.027089898960842355\n",
      "train loss:0.0036416289935387075\n",
      "train loss:0.00536082787471069\n",
      "train loss:0.006947010905858858\n",
      "train loss:0.0018117849017634882\n",
      "train loss:0.0008450337325635307\n",
      "train loss:0.005821644856861279\n",
      "train loss:0.006392111848955419\n",
      "train loss:0.0035568154782980614\n",
      "train loss:0.0017963104610056182\n",
      "train loss:0.014036990201768034\n",
      "train loss:0.0054441392503973805\n",
      "train loss:0.027396520310649167\n",
      "train loss:0.013976193182646703\n",
      "train loss:0.0018895896392125508\n",
      "train loss:0.014753993696524295\n",
      "train loss:0.0030206913184162854\n",
      "train loss:0.018168753277902475\n",
      "train loss:0.0023489566892653845\n",
      "train loss:0.004086170559379655\n",
      "train loss:0.006731016709737122\n",
      "train loss:0.0078046830852928776\n",
      "train loss:0.005202504891477305\n",
      "train loss:0.06371513303226445\n",
      "train loss:0.0029467484280348115\n",
      "train loss:0.016774026905023325\n",
      "train loss:0.019882077079634432\n",
      "train loss:0.008120056399832971\n",
      "train loss:0.0035147179278243856\n",
      "train loss:0.003883283795320588\n",
      "train loss:0.010844018529997777\n",
      "train loss:0.003377458506706579\n",
      "train loss:0.010618479922496133\n",
      "train loss:0.0023129760819068285\n",
      "train loss:0.004878030975417444\n",
      "train loss:0.009977887609731893\n",
      "train loss:0.0016148893668972417\n",
      "train loss:0.011570579161797361\n",
      "train loss:0.0009140925741758705\n",
      "train loss:0.054186826266990264\n",
      "train loss:0.00511812792335781\n",
      "train loss:0.0005812354149374085\n",
      "train loss:0.00527496220090913\n",
      "train loss:0.0032200913693036216\n",
      "train loss:0.015140774104833858\n",
      "train loss:0.006200290058099077\n",
      "train loss:0.045732712616446014\n",
      "train loss:0.031966843159515666\n",
      "train loss:0.015630752543975456\n",
      "train loss:0.009339253092940716\n",
      "train loss:0.003614266065110003\n",
      "train loss:0.0019145450122556552\n",
      "train loss:0.018630317414397062\n",
      "train loss:0.010118320617569891\n",
      "train loss:0.0072785385477617745\n",
      "train loss:0.005856199098016241\n",
      "train loss:0.0038639348914045212\n",
      "train loss:0.01592822286781564\n",
      "train loss:0.04180821787228792\n",
      "train loss:0.003812303654559855\n",
      "train loss:0.022248786888889368\n",
      "train loss:0.004552932367640807\n",
      "train loss:0.01518562786821074\n",
      "train loss:0.08067190503709465\n",
      "train loss:0.003350469347498207\n",
      "train loss:0.007526768256949011\n",
      "train loss:0.006509094597917625\n",
      "train loss:0.009010380636294103\n",
      "train loss:0.006458228764507524\n",
      "train loss:0.0016240533486152453\n",
      "train loss:0.010492549093975465\n",
      "train loss:0.01189562661129123\n",
      "train loss:0.003358836609417529\n",
      "train loss:0.01253470665629973\n",
      "train loss:0.011355699505333603\n",
      "train loss:0.00069721571013306\n",
      "train loss:0.004207803060561567\n",
      "train loss:0.0017051086956251526\n",
      "train loss:0.003961250815299495\n",
      "train loss:0.012889177563367176\n",
      "train loss:0.010345582726532045\n",
      "train loss:0.00434804697588812\n",
      "train loss:0.0019455774356473448\n",
      "train loss:0.01130334423823256\n",
      "train loss:0.01746333596342908\n",
      "train loss:0.003097609560541071\n",
      "train loss:0.0047516916982178895\n",
      "train loss:0.00963848542165104\n",
      "train loss:0.04529171667952356\n",
      "train loss:0.006172338646664498\n",
      "train loss:0.0059335889297259595\n",
      "train loss:0.02135943412248482\n",
      "train loss:0.007713779552118729\n",
      "train loss:0.00202465724690997\n",
      "train loss:0.005822621918306285\n",
      "train loss:0.018363518415130183\n",
      "train loss:0.011204757243972154\n",
      "train loss:0.011319046755302901\n",
      "train loss:0.00998607512707447\n",
      "train loss:0.0112811951293876\n",
      "train loss:0.11270357410334242\n",
      "train loss:0.003914468143549706\n",
      "train loss:0.011144624480906234\n",
      "train loss:0.001997830096216494\n",
      "train loss:0.008120799276290815\n",
      "train loss:0.005422239642436515\n",
      "train loss:0.03576598061096854\n",
      "train loss:0.006879576331533616\n",
      "train loss:0.012022867169916171\n",
      "train loss:0.0015195245137926794\n",
      "train loss:0.006297604186439684\n",
      "train loss:0.010652999171149691\n",
      "train loss:0.00537297489185048\n",
      "train loss:0.006040012837596429\n",
      "train loss:0.009927429871042573\n",
      "train loss:0.02235383664771559\n",
      "train loss:0.011002800698479682\n",
      "train loss:0.012361241584192495\n",
      "train loss:0.019912421949920402\n",
      "train loss:0.0019328513406099048\n",
      "train loss:0.004388349538948426\n",
      "train loss:0.00864797734937426\n",
      "train loss:0.062367795014885985\n",
      "train loss:0.0021308704744561896\n",
      "train loss:0.004229000069927389\n",
      "train loss:0.1207448926376224\n",
      "train loss:0.007560183407440243\n",
      "train loss:0.008938463976452243\n",
      "train loss:0.08493941414222773\n",
      "train loss:0.008680925106075418\n",
      "train loss:0.032977705442598024\n",
      "train loss:0.004147875686009149\n",
      "train loss:0.008860500912657944\n",
      "train loss:0.0021903744002908813\n",
      "train loss:0.16615382511332236\n",
      "train loss:0.010751262774109507\n",
      "train loss:0.009998448085368785\n",
      "train loss:0.017505290965972656\n",
      "train loss:0.006159386622423198\n",
      "train loss:0.0033161104506591217\n",
      "train loss:0.006722743509867401\n",
      "train loss:0.005893093083606704\n",
      "train loss:0.0069068461674485345\n",
      "train loss:0.022565230322075247\n",
      "train loss:0.004621792302482464\n",
      "train loss:0.003304087283612095\n",
      "train loss:0.005772111766279236\n",
      "train loss:0.029941715455963892\n",
      "train loss:0.016545658730481444\n",
      "train loss:0.02291927002294443\n",
      "train loss:0.016670116751010922\n",
      "train loss:0.009114340276262112\n",
      "train loss:0.01358837733144882\n",
      "train loss:0.016786363054575183\n",
      "train loss:0.002569571828171403\n",
      "train loss:0.008723349804838638\n",
      "train loss:0.0022580690565630083\n",
      "train loss:0.006195230748341998\n",
      "train loss:0.012799635434557689\n",
      "train loss:0.014822352922949042\n",
      "train loss:0.005642778382754305\n",
      "train loss:0.0016987624201411943\n",
      "train loss:0.010645051556731561\n",
      "train loss:0.00436319374936352\n",
      "train loss:0.0070169812664766075\n",
      "train loss:0.0006339409785790322\n",
      "train loss:0.0025586924015650807\n",
      "train loss:0.010716704086474398\n",
      "train loss:0.050836727238133254\n",
      "train loss:0.006582682499004915\n",
      "train loss:0.010088472629629952\n",
      "train loss:0.0018610633894680184\n",
      "train loss:0.00773757159267094\n",
      "train loss:0.0026065758835399824\n",
      "train loss:0.04863657170731117\n",
      "train loss:0.0036178696805139686\n",
      "train loss:0.00042095326206927373\n",
      "train loss:0.007623033761886705\n",
      "train loss:0.01209819650215136\n",
      "train loss:0.006907099062408241\n",
      "train loss:0.016962729379992627\n",
      "train loss:0.0030470889685833887\n",
      "train loss:0.004773084862926765\n",
      "train loss:0.00822098991696861\n",
      "train loss:0.01432392171730048\n",
      "train loss:0.0009764872432718367\n",
      "train loss:0.008168146192161201\n",
      "train loss:0.0030715722616251996\n",
      "train loss:0.015263610478468113\n",
      "train loss:0.001923652363063676\n",
      "train loss:0.014049597668745185\n",
      "train loss:0.013286287150460645\n",
      "train loss:0.0003383662358537328\n",
      "train loss:0.015487737938577311\n",
      "train loss:0.0024636738353243145\n",
      "train loss:0.004756436441773606\n",
      "train loss:0.0074632986746660575\n",
      "train loss:0.004353910627028081\n",
      "train loss:0.001123847284840602\n",
      "train loss:0.025356530782075316\n",
      "train loss:0.007800986287235289\n",
      "train loss:0.006506669178880624\n",
      "train loss:0.007068348679053098\n",
      "train loss:0.005805741879219966\n",
      "train loss:0.010327509632131655\n",
      "train loss:0.002555921145107028\n",
      "train loss:0.0012894535757617936\n",
      "train loss:0.00113982527681308\n",
      "train loss:0.012860013980828637\n",
      "train loss:0.01089424239707264\n",
      "train loss:0.0057841769802025565\n",
      "train loss:0.012817972221114854\n",
      "train loss:0.0077825358625223835\n",
      "train loss:0.06673844778999449\n",
      "train loss:0.0033068388497405603\n",
      "train loss:0.0009493587502445622\n",
      "train loss:0.004832016111766647\n",
      "train loss:0.010133950109312926\n",
      "train loss:0.023304110495569862\n",
      "train loss:0.0038464771476157894\n",
      "train loss:0.020100975344291734\n",
      "train loss:0.005242104720674319\n",
      "train loss:0.004932335492250741\n",
      "train loss:0.009088649345015597\n",
      "train loss:0.02084007243984705\n",
      "train loss:0.0069217575995700956\n",
      "train loss:0.01145383502195815\n",
      "train loss:0.008020372632726199\n",
      "train loss:0.016759803914737584\n",
      "train loss:0.0064163895753727405\n",
      "train loss:0.022348044658370106\n",
      "train loss:0.0285810059239109\n",
      "train loss:0.0037327212314040904\n",
      "train loss:0.0037200802571727043\n",
      "train loss:0.04397048815048122\n",
      "train loss:0.03520592380062161\n",
      "train loss:0.0012585400241412275\n",
      "train loss:0.028050224392412476\n",
      "train loss:0.023830099867599937\n",
      "train loss:0.004598521779517829\n",
      "train loss:0.003181144642503886\n",
      "train loss:0.04489255849661854\n",
      "train loss:0.012168047715507764\n",
      "train loss:0.020218508920748623\n",
      "train loss:0.004647631118530048\n",
      "train loss:0.0028996251008975593\n",
      "train loss:0.0038212399743511164\n",
      "train loss:0.009292990144063637\n",
      "train loss:0.032403456080085\n",
      "train loss:0.021780729077797226\n",
      "train loss:0.020810533175428886\n",
      "train loss:0.0024969026909618676\n",
      "train loss:0.0070356855034237435\n",
      "train loss:0.0012205490579310044\n",
      "train loss:0.01345334554487585\n",
      "train loss:0.008611489010361538\n",
      "train loss:0.004871240350551767\n",
      "train loss:0.012356515927628533\n",
      "train loss:0.004218270123819083\n",
      "train loss:0.019643058301143566\n",
      "train loss:0.013404085408399295\n",
      "train loss:0.00432202730772235\n",
      "train loss:0.029063274606518565\n",
      "train loss:0.016569892059354677\n",
      "train loss:0.0032918290039735273\n",
      "train loss:0.011690211114240227\n",
      "train loss:0.006282321515668573\n",
      "train loss:0.005264696944606477\n",
      "train loss:0.0036637377735795366\n",
      "train loss:0.004518146664633882\n",
      "train loss:0.008739891242077287\n",
      "train loss:0.014991165916757539\n",
      "train loss:0.00210344480529001\n",
      "train loss:0.010766663499786073\n",
      "train loss:0.01087435153062258\n",
      "train loss:0.006210242818193642\n",
      "train loss:0.0014927362119196116\n",
      "train loss:0.002672972809512155\n",
      "train loss:0.0006295406834039606\n",
      "train loss:0.017577380878467573\n",
      "train loss:0.004791052001547741\n",
      "train loss:0.010673761463652919\n",
      "train loss:0.004634795773785489\n",
      "train loss:0.012439715456793793\n",
      "train loss:0.010457109556000727\n",
      "train loss:0.004916846003031246\n",
      "train loss:0.0073369804847559835\n",
      "train loss:0.005416916777225556\n",
      "train loss:0.04054783899748581\n",
      "train loss:0.00980499499839095\n",
      "train loss:0.007299480440929709\n",
      "train loss:0.011890048254694361\n",
      "train loss:0.003046418115969838\n",
      "train loss:0.009570411399610212\n",
      "train loss:0.009281693198791895\n",
      "train loss:0.001551327138604007\n",
      "train loss:0.020154487743682593\n",
      "train loss:0.0027133433761943175\n",
      "train loss:0.0019207265435875643\n",
      "train loss:0.01625277473100385\n",
      "train loss:0.015482219228538614\n",
      "train loss:0.010211392700258176\n",
      "train loss:0.023356595283435574\n",
      "train loss:0.011587257232493365\n",
      "train loss:0.0018129567137373536\n",
      "train loss:0.004386716630645435\n",
      "train loss:0.0053609964061231705\n",
      "train loss:0.007750177980792516\n",
      "train loss:0.008544216084689941\n",
      "train loss:0.005839349368835322\n",
      "train loss:0.001631132799279824\n",
      "train loss:0.0014057574163960221\n",
      "train loss:0.02307680314444337\n",
      "train loss:0.006176208957956265\n",
      "train loss:0.005987245157496008\n",
      "train loss:0.012010716601786325\n",
      "train loss:0.007451463214382649\n",
      "train loss:0.019434354105058293\n",
      "train loss:0.0049186335650225135\n",
      "train loss:0.006080768185426021\n",
      "train loss:0.010701236678758674\n",
      "train loss:0.004770911599077582\n",
      "train loss:0.018274572197335658\n",
      "train loss:0.025796645296243058\n",
      "train loss:0.008632365566952231\n",
      "train loss:0.018717327441652932\n",
      "train loss:0.0077936558250836065\n",
      "train loss:0.013584691479462077\n",
      "train loss:0.00355113426969891\n",
      "train loss:0.003571364458563558\n",
      "train loss:0.009662413667899222\n",
      "train loss:0.017118148246160415\n",
      "train loss:0.021724498422032227\n",
      "train loss:0.012369885624782879\n",
      "train loss:0.005278994726982776\n",
      "train loss:0.017230629721815632\n",
      "train loss:0.025679395590271457\n",
      "train loss:0.004866939499610766\n",
      "train loss:0.014208633026831716\n",
      "=== epoch:9, train acc:0.994, test acc:0.987 ===\n",
      "train loss:0.0009482820056849541\n",
      "train loss:0.0041295767569226646\n",
      "train loss:0.016540306589352766\n",
      "train loss:0.005038348972205539\n",
      "train loss:0.01176935538222345\n",
      "train loss:0.006103740036753716\n",
      "train loss:0.0012102066902380697\n",
      "train loss:0.0025649903383336593\n",
      "train loss:0.0065164035372020776\n",
      "train loss:0.001010832637002797\n",
      "train loss:0.0010096801754604127\n",
      "train loss:0.0031937636949290814\n",
      "train loss:0.0016043964800328033\n",
      "train loss:0.0017610278507946408\n",
      "train loss:0.012228553134560987\n",
      "train loss:0.03353788002423697\n",
      "train loss:0.008618462991000308\n",
      "train loss:0.006449620952749019\n",
      "train loss:0.02096691637282223\n",
      "train loss:0.01883390019176953\n",
      "train loss:0.0022050790426966435\n",
      "train loss:0.0019427035761539024\n",
      "train loss:0.004084478771556096\n",
      "train loss:0.010142449123166643\n",
      "train loss:0.01270166021742133\n",
      "train loss:0.007085088897300001\n",
      "train loss:0.0012662982981287278\n",
      "train loss:0.0018657984683685721\n",
      "train loss:0.02706663752296355\n",
      "train loss:0.005977317195631529\n",
      "train loss:0.014061996161414013\n",
      "train loss:0.0035292610836021154\n",
      "train loss:0.007771247340326542\n",
      "train loss:0.002499696318058777\n",
      "train loss:0.009505370132270817\n",
      "train loss:0.003379276441457321\n",
      "train loss:0.0204052672795426\n",
      "train loss:0.0017925135867600714\n",
      "train loss:0.03029204824170062\n",
      "train loss:0.010717289370863074\n",
      "train loss:0.00044175733002200346\n",
      "train loss:0.005344152323663889\n",
      "train loss:0.0022692141210976765\n",
      "train loss:0.06360785044735301\n",
      "train loss:0.0028637906549687886\n",
      "train loss:0.004683753430319454\n",
      "train loss:0.008270430115166455\n",
      "train loss:0.0052176063475088165\n",
      "train loss:0.0013461998904081116\n",
      "train loss:0.00311709201149772\n",
      "train loss:0.010294155755400342\n",
      "train loss:0.0023124722706752954\n",
      "train loss:0.004005990235948324\n",
      "train loss:0.0031732512027127222\n",
      "train loss:0.009571351936581667\n",
      "train loss:0.0240365352884776\n",
      "train loss:0.00977968581521478\n",
      "train loss:0.012544649823841554\n",
      "train loss:0.005247360554732267\n",
      "train loss:0.013720057601506035\n",
      "train loss:0.0003867003030443137\n",
      "train loss:0.006010341633438259\n",
      "train loss:0.002516172593240687\n",
      "train loss:0.014088696763055894\n",
      "train loss:0.0021060039680468943\n",
      "train loss:0.010999985999643145\n",
      "train loss:0.009551383255353714\n",
      "train loss:0.02775593147356259\n",
      "train loss:0.0396694636213508\n",
      "train loss:0.0015634105974354601\n",
      "train loss:0.007414412806225084\n",
      "train loss:0.00734740650259354\n",
      "train loss:0.002435094987862278\n",
      "train loss:0.014273208361989418\n",
      "train loss:0.018955782260225783\n",
      "train loss:0.006106969202743398\n",
      "train loss:0.006377276042189656\n",
      "train loss:0.02971945478037216\n",
      "train loss:0.013726191774005237\n",
      "train loss:0.04010365713740609\n",
      "train loss:0.004799484570359792\n",
      "train loss:0.009351491163297034\n",
      "train loss:0.02061998356306509\n",
      "train loss:0.002654204874588178\n",
      "train loss:0.006088170315752556\n",
      "train loss:0.0035364706140288984\n",
      "train loss:0.007126513117953464\n",
      "train loss:0.0075679614153378125\n",
      "train loss:0.0028352505652725884\n",
      "train loss:0.006802853259203953\n",
      "train loss:0.0017943119199794238\n",
      "train loss:0.006304979962379822\n",
      "train loss:0.0008705863694968699\n",
      "train loss:0.01958231350764003\n",
      "train loss:0.001903593389447359\n",
      "train loss:0.0030519350104822817\n",
      "train loss:0.004571686607656038\n",
      "train loss:0.0035059601425139323\n",
      "train loss:0.006210035166512418\n",
      "train loss:0.0044414033949427\n",
      "train loss:0.00050625793333943\n",
      "train loss:0.011356549497950072\n",
      "train loss:0.005826763964166516\n",
      "train loss:0.002965944701602653\n",
      "train loss:0.006931059219027861\n",
      "train loss:0.0018029707966935324\n",
      "train loss:0.0016690859101774979\n",
      "train loss:0.0009180584540466604\n",
      "train loss:0.003550707881184185\n",
      "train loss:0.0022976485667995485\n",
      "train loss:0.010818912521371748\n",
      "train loss:0.004539656024172671\n",
      "train loss:0.004493817839773939\n",
      "train loss:0.012803761522512527\n",
      "train loss:0.0005471885955111578\n",
      "train loss:0.027492798777194842\n",
      "train loss:0.004641111213328757\n",
      "train loss:0.00154200165312893\n",
      "train loss:0.009618962727286637\n",
      "train loss:0.01324452968627056\n",
      "train loss:0.0032869159723470237\n",
      "train loss:0.005334220080626799\n",
      "train loss:0.004475740637760461\n",
      "train loss:0.0031899506614518542\n",
      "train loss:0.0064890270913498934\n",
      "train loss:0.013620103679659379\n",
      "train loss:0.000660497976191572\n",
      "train loss:0.0017711321744620315\n",
      "train loss:0.0022892982480758285\n",
      "train loss:0.0018847716483305576\n",
      "train loss:0.010111152381277322\n",
      "train loss:0.009920202688531278\n",
      "train loss:0.018368385966317357\n",
      "train loss:0.047676026992921304\n",
      "train loss:0.0013819199977834547\n",
      "train loss:0.03516790266885448\n",
      "train loss:0.01142234466287537\n",
      "train loss:0.005643061100147729\n",
      "train loss:0.0035792086139443064\n",
      "train loss:0.02517676256661397\n",
      "train loss:0.0043003584884582415\n",
      "train loss:0.0010158751388722046\n",
      "train loss:0.0036547512781285297\n",
      "train loss:0.03180872107210357\n",
      "train loss:0.004834937510848302\n",
      "train loss:0.005257392212191926\n",
      "train loss:0.007412082329841537\n",
      "train loss:0.008029697504701469\n",
      "train loss:0.01210767623874278\n",
      "train loss:0.006114224345169412\n",
      "train loss:0.004388803435763346\n",
      "train loss:0.00469420728335078\n",
      "train loss:0.0030325907067599222\n",
      "train loss:0.01410520265057164\n",
      "train loss:0.010597601743972385\n",
      "train loss:0.0179581848294223\n",
      "train loss:0.018337583158304905\n",
      "train loss:0.0020104770720527974\n",
      "train loss:0.007711595261860997\n",
      "train loss:0.008706978034755236\n",
      "train loss:0.006268774633508317\n",
      "train loss:0.020371488407820895\n",
      "train loss:0.048542745124615115\n",
      "train loss:0.025170292634880787\n",
      "train loss:0.004802989476857027\n",
      "train loss:0.05692615436331379\n",
      "train loss:0.0028137263177767107\n",
      "train loss:0.08270667464430101\n",
      "train loss:0.0049874717002267345\n",
      "train loss:0.016312243157612028\n",
      "train loss:0.002981036802789145\n",
      "train loss:0.003968375173724742\n",
      "train loss:0.0707072115444458\n",
      "train loss:0.021742324729944897\n",
      "train loss:0.005229812789727019\n",
      "train loss:0.0019014784538749854\n",
      "train loss:0.023264955147818674\n",
      "train loss:0.011350210349708486\n",
      "train loss:0.003678351423368593\n",
      "train loss:0.0073242243684609465\n",
      "train loss:0.013350067527060949\n",
      "train loss:0.011904493681921502\n",
      "train loss:0.006492476842160272\n",
      "train loss:0.002887448496469467\n",
      "train loss:0.006240456352436287\n",
      "train loss:0.013949002101595172\n",
      "train loss:0.002982084296951546\n",
      "train loss:0.006418486184070444\n",
      "train loss:0.004010722971478909\n",
      "train loss:0.08497969572682235\n",
      "train loss:0.002063673480112577\n",
      "train loss:0.013315495697886164\n",
      "train loss:0.0026704712306624035\n",
      "train loss:0.0013523550559248346\n",
      "train loss:0.0008961881727413828\n",
      "train loss:0.01158722929330974\n",
      "train loss:0.008495940648508534\n",
      "train loss:0.01330410398905366\n",
      "train loss:0.012755640144637839\n",
      "train loss:0.007668155924238106\n",
      "train loss:0.0020015545161695316\n",
      "train loss:0.03759313718443081\n",
      "train loss:0.004633270114532683\n",
      "train loss:0.009324259122768968\n",
      "train loss:0.017400594278062342\n",
      "train loss:0.006716299119239242\n",
      "train loss:0.0035700506347210777\n",
      "train loss:0.0026849696557428872\n",
      "train loss:0.02781055563695687\n",
      "train loss:0.012002858950973052\n",
      "train loss:0.011321680157711225\n",
      "train loss:0.009385371241784088\n",
      "train loss:0.023055552016470204\n",
      "train loss:0.008111144990280072\n",
      "train loss:0.005071034182829885\n",
      "train loss:0.02074661802523727\n",
      "train loss:0.007075114785023\n",
      "train loss:0.006700006724381078\n",
      "train loss:0.004747924322975364\n",
      "train loss:0.007182643161128301\n",
      "train loss:0.004522336567298154\n",
      "train loss:0.027072206847860582\n",
      "train loss:0.0011136212035629555\n",
      "train loss:0.008975052814799095\n",
      "train loss:0.0019266492230026694\n",
      "train loss:0.004864437351511468\n",
      "train loss:0.006351400633761644\n",
      "train loss:0.005465441302675891\n",
      "train loss:0.0057975518293085695\n",
      "train loss:0.0033622201786650497\n",
      "train loss:0.0011584248440362896\n",
      "train loss:0.004391479312890914\n",
      "train loss:0.0038229250033028237\n",
      "train loss:0.0013029369160650081\n",
      "train loss:0.004280602489930829\n",
      "train loss:0.00871434058982243\n",
      "train loss:0.0070442365824089515\n",
      "train loss:0.014038951606426424\n",
      "train loss:0.011115474817453537\n",
      "train loss:0.002812615738589373\n",
      "train loss:0.016928034174704832\n",
      "train loss:0.0016900529609702565\n",
      "train loss:0.008214089732858441\n",
      "train loss:0.003150873947955018\n",
      "train loss:0.004038318275906133\n",
      "train loss:0.022184235752007272\n",
      "train loss:0.005349092436488457\n",
      "train loss:0.012319089097858876\n",
      "train loss:0.0019492162581715704\n",
      "train loss:0.01888250521720652\n",
      "train loss:0.0034388506042716377\n",
      "train loss:0.02216102256087105\n",
      "train loss:0.0026756533242441177\n",
      "train loss:0.0037628569055864025\n",
      "train loss:0.005525461087476895\n",
      "train loss:0.0054031135744661805\n",
      "train loss:0.02954229507560243\n",
      "train loss:0.0024745718972699763\n",
      "train loss:0.00390622964404813\n",
      "train loss:0.005418066627872495\n",
      "train loss:0.015856617448012353\n",
      "train loss:0.005135316474514275\n",
      "train loss:0.0010476616698165294\n",
      "train loss:0.001439810109821543\n",
      "train loss:0.0019580517971778265\n",
      "train loss:0.03938965277260174\n",
      "train loss:0.010042330338180699\n",
      "train loss:0.0008685977180794058\n",
      "train loss:0.002549263194998561\n",
      "train loss:0.004579487809588555\n",
      "train loss:0.007746371605233675\n",
      "train loss:0.0037842513357984973\n",
      "train loss:0.013092271927693432\n",
      "train loss:0.002687556973302868\n",
      "train loss:0.0035126555972783326\n",
      "train loss:0.008698835190321223\n",
      "train loss:0.012346466044469578\n",
      "train loss:0.0006875884122487341\n",
      "train loss:0.0029458718645700935\n",
      "train loss:0.0037337374960591807\n",
      "train loss:0.009316739983454418\n",
      "train loss:0.00885729193769671\n",
      "train loss:0.0012314922636546168\n",
      "train loss:0.020217766592885593\n",
      "train loss:0.009631556035442676\n",
      "train loss:0.0020294751019066613\n",
      "train loss:0.002331834921541772\n",
      "train loss:0.0014715044174147388\n",
      "train loss:0.0034442435874713333\n",
      "train loss:0.002189704531209033\n",
      "train loss:0.005343881040662231\n",
      "train loss:0.002033381202442889\n",
      "train loss:0.001263381123161445\n",
      "train loss:0.00800551168225914\n",
      "train loss:0.001311346639487113\n",
      "train loss:0.025399318314246508\n",
      "train loss:0.0033108725091394865\n",
      "train loss:0.0006357689961680928\n",
      "train loss:0.01566893906180927\n",
      "train loss:0.0010903101382047922\n",
      "train loss:0.003528238174404978\n",
      "train loss:0.014853630022146493\n",
      "train loss:0.0059782014399123295\n",
      "train loss:0.02663165271009168\n",
      "train loss:0.004716540833811004\n",
      "train loss:0.015940745840427897\n",
      "train loss:0.0035520324612338377\n",
      "train loss:0.0020038357713312756\n",
      "train loss:0.03139565991224714\n",
      "train loss:0.010520175684706558\n",
      "train loss:0.011305917261359886\n",
      "train loss:0.007501033912440603\n",
      "train loss:0.011943605012471574\n",
      "train loss:0.006031570805963799\n",
      "train loss:0.006291831348622911\n",
      "train loss:0.007656971904825908\n",
      "train loss:0.010903298239683515\n",
      "train loss:0.00507965311279631\n",
      "train loss:0.0012382161435534234\n",
      "train loss:0.003122487537021569\n",
      "train loss:0.008432001210415544\n",
      "train loss:0.0025708592211072267\n",
      "train loss:0.008321133970130698\n",
      "train loss:0.008286063165537774\n",
      "train loss:0.0075231261631900385\n",
      "train loss:0.005990874471392412\n",
      "train loss:0.003502674850203328\n",
      "train loss:0.015422268728539364\n",
      "train loss:0.009347192284689517\n",
      "train loss:0.0017190188351600979\n",
      "train loss:0.0023376859248600227\n",
      "train loss:0.002635829580169881\n",
      "train loss:0.004619037607833762\n",
      "train loss:0.0011119614771223525\n",
      "train loss:0.012664205935480018\n",
      "train loss:0.03510585963615414\n",
      "train loss:0.0027206426506744884\n",
      "train loss:0.018674533184025403\n",
      "train loss:0.015522807144604396\n",
      "train loss:0.007187595352472164\n",
      "train loss:0.010703500623495739\n",
      "train loss:0.004415246220224934\n",
      "train loss:0.009544537671000814\n",
      "train loss:0.00810222224125114\n",
      "train loss:0.004362582142944332\n",
      "train loss:0.017784076897965916\n",
      "train loss:0.0014211548693998655\n",
      "train loss:0.0021293240532105806\n",
      "train loss:0.003807286255987516\n",
      "train loss:0.012697843434659217\n",
      "train loss:0.0014087062033174033\n",
      "train loss:0.02236946902256417\n",
      "train loss:0.0015062445589471868\n",
      "train loss:0.021706196322633195\n",
      "train loss:0.0010074824301490421\n",
      "train loss:0.004531027707683986\n",
      "train loss:0.0035177746274733983\n",
      "train loss:0.005611314954371239\n",
      "train loss:0.0037287819261158177\n",
      "train loss:0.010553943842835525\n",
      "train loss:0.014675673870485927\n",
      "train loss:0.008515771607892748\n",
      "train loss:0.0125670859794128\n",
      "train loss:0.020843804306382423\n",
      "train loss:0.0013292406739606943\n",
      "train loss:0.017459186200949158\n",
      "train loss:0.0023461948115388194\n",
      "train loss:0.0036028217676623286\n",
      "train loss:0.005264085043133496\n",
      "train loss:0.004841200581242941\n",
      "train loss:0.014270445967555117\n",
      "train loss:0.0023765397839242445\n",
      "train loss:0.013248769782187387\n",
      "train loss:0.00040651735642197094\n",
      "train loss:0.004738169925276651\n",
      "train loss:0.008877774944440621\n",
      "train loss:0.0025791405046466094\n",
      "train loss:0.004977935966677446\n",
      "train loss:0.0015288217588442215\n",
      "train loss:0.002270594861454308\n",
      "train loss:0.005416384256546455\n",
      "train loss:0.0019518662262154012\n",
      "train loss:0.017067535342821624\n",
      "train loss:0.0034770087082238244\n",
      "train loss:0.014427083805799039\n",
      "train loss:0.003647654553681585\n",
      "train loss:0.0006610332528065593\n",
      "train loss:0.0034160475161814784\n",
      "train loss:0.0033896190754230666\n",
      "train loss:0.001052556106400736\n",
      "train loss:0.014245647691711615\n",
      "train loss:0.018621984084452838\n",
      "train loss:0.011683684972843335\n",
      "train loss:0.010285821420830215\n",
      "train loss:0.013916003079609296\n",
      "train loss:0.00569399897106011\n",
      "train loss:0.0008825535838523606\n",
      "train loss:0.006407414803903198\n",
      "train loss:0.0017990592283542199\n",
      "train loss:0.008661177315900425\n",
      "train loss:0.001228067412359804\n",
      "train loss:0.0019881349102118893\n",
      "train loss:0.01292798985282928\n",
      "train loss:0.005019866596932582\n",
      "train loss:0.001991266958542992\n",
      "train loss:0.0015689557874318568\n",
      "train loss:0.00018455109697091288\n",
      "train loss:0.007963364553736385\n",
      "train loss:0.005884123345207513\n",
      "train loss:0.004098137650839542\n",
      "train loss:0.002655507645149425\n",
      "train loss:0.001800815340368687\n",
      "train loss:0.0037181065414101065\n",
      "train loss:0.00228784882778885\n",
      "train loss:0.005313093847548085\n",
      "train loss:0.02235464069203868\n",
      "train loss:0.013030835617829193\n",
      "train loss:0.0028629744214902185\n",
      "train loss:0.008249493785957403\n",
      "train loss:0.004529318054771378\n",
      "train loss:0.004776738035180885\n",
      "train loss:0.003043271744314581\n",
      "train loss:0.0016321374044167996\n",
      "train loss:0.023618888872508847\n",
      "train loss:0.07130390328313672\n",
      "train loss:0.03527582070317549\n",
      "train loss:0.0012220136078994416\n",
      "train loss:0.0036658958757230346\n",
      "train loss:0.012114588365275571\n",
      "train loss:0.016979076182666825\n",
      "train loss:0.004666153874724293\n",
      "train loss:0.000913671550237428\n",
      "train loss:0.00029736522362045344\n",
      "train loss:0.043881262231826035\n",
      "train loss:0.002364858472438797\n",
      "train loss:0.035921822382660215\n",
      "train loss:0.0014528664451981357\n",
      "train loss:0.009400859677074722\n",
      "train loss:0.005711432890381262\n",
      "train loss:0.015419728673077419\n",
      "train loss:0.010596160730962947\n",
      "train loss:0.003661063882309049\n",
      "train loss:0.0058870726918724865\n",
      "train loss:0.011665679490751767\n",
      "train loss:0.002844196522744158\n",
      "train loss:0.009584488528601127\n",
      "train loss:0.013350556885407601\n",
      "train loss:0.010599590997663676\n",
      "train loss:0.01962595035004149\n",
      "train loss:0.004897204629252887\n",
      "train loss:0.0009399577133694821\n",
      "train loss:0.02030608760842689\n",
      "train loss:0.0019287770204542403\n",
      "train loss:0.021486807741486685\n",
      "train loss:0.012473794670368039\n",
      "train loss:0.001946188228927007\n",
      "train loss:0.009425542775438776\n",
      "train loss:0.1618760436668107\n",
      "train loss:0.0033978720915129046\n",
      "train loss:0.005945717724462863\n",
      "train loss:0.010144452039880715\n",
      "train loss:0.0041347152896912\n",
      "train loss:0.0008596934551163942\n",
      "train loss:0.0033582190496837462\n",
      "train loss:0.0014343610374113638\n",
      "train loss:0.007130373671210758\n",
      "train loss:0.02484488232665357\n",
      "train loss:0.0012418358176351663\n",
      "train loss:0.0025766412736233212\n",
      "train loss:0.0030603829108063756\n",
      "train loss:0.005373733227166682\n",
      "train loss:0.014424929948840995\n",
      "train loss:0.005881246055343539\n",
      "train loss:0.0016575541510409795\n",
      "train loss:0.011250740212477224\n",
      "train loss:0.11555669335427829\n",
      "train loss:0.0047071182220721755\n",
      "train loss:0.0017376545856419786\n",
      "train loss:0.005568431537771462\n",
      "train loss:0.004120179642924498\n",
      "train loss:0.013091905855456443\n",
      "train loss:0.030588612677141195\n",
      "train loss:0.001988776917408739\n",
      "train loss:0.012819266213552425\n",
      "train loss:0.013405983866827939\n",
      "train loss:0.006825716652907047\n",
      "train loss:0.0019229426188232736\n",
      "train loss:0.01060157599993665\n",
      "train loss:0.006335000580232519\n",
      "train loss:0.001661628466240624\n",
      "train loss:0.0007215494141223063\n",
      "train loss:0.006266015809513839\n",
      "train loss:0.003301313735430036\n",
      "train loss:0.0025389329342996847\n",
      "train loss:0.0016252219726186823\n",
      "train loss:0.004277145775033889\n",
      "train loss:0.0006793657739240361\n",
      "train loss:0.02603877871374617\n",
      "train loss:0.019866660730577832\n",
      "train loss:0.0019259254041573742\n",
      "train loss:0.002134164045465266\n",
      "train loss:0.007845861486000914\n",
      "train loss:0.011423881923612385\n",
      "train loss:0.00922994628113597\n",
      "train loss:0.008637563799134895\n",
      "train loss:0.0015857105168742224\n",
      "train loss:0.049593258299450736\n",
      "train loss:0.012990231817376281\n",
      "train loss:0.006910617510662354\n",
      "train loss:0.03129132990663655\n",
      "train loss:0.003054351597264227\n",
      "train loss:0.0006976273045457489\n",
      "train loss:0.0015836413075868195\n",
      "train loss:0.004449081926572095\n",
      "train loss:0.00806607154935992\n",
      "train loss:0.006675377944445192\n",
      "train loss:0.006891342170482493\n",
      "train loss:0.005714969229177894\n",
      "train loss:0.008811937863462922\n",
      "train loss:0.0008698506937881614\n",
      "train loss:0.002386879394898615\n",
      "train loss:0.011357190091603811\n",
      "train loss:0.0052910121659416505\n",
      "train loss:0.01350218579272884\n",
      "train loss:0.00046707029783915187\n",
      "train loss:0.005771941166712371\n",
      "train loss:0.0369269605882146\n",
      "train loss:0.00134737911070502\n",
      "train loss:0.002791667508901756\n",
      "train loss:0.008693893740770163\n",
      "train loss:0.0024088839215564064\n",
      "train loss:0.0005710781108730449\n",
      "train loss:0.031073184514183186\n",
      "train loss:0.006397249607557058\n",
      "train loss:0.02496400206158814\n",
      "train loss:0.09103987259645521\n",
      "train loss:0.02981384933716745\n",
      "train loss:0.00966696904690462\n",
      "train loss:0.02079248223138354\n",
      "train loss:0.011280809026265406\n",
      "train loss:0.006492795657079049\n",
      "train loss:0.06595712334712235\n",
      "train loss:0.006596840948349332\n",
      "train loss:0.00259911706680818\n",
      "train loss:0.0043906325730937\n",
      "train loss:0.001239136144986439\n",
      "train loss:0.02745354523465634\n",
      "train loss:0.0005262483926475724\n",
      "train loss:0.002512139276356162\n",
      "train loss:0.09314513399819013\n",
      "train loss:0.01019488779914302\n",
      "train loss:0.0062263538267776294\n",
      "train loss:0.003774478643535943\n",
      "train loss:0.0028553154753706064\n",
      "train loss:0.0017195983911142917\n",
      "train loss:0.014149360055391849\n",
      "train loss:0.0015330431625756077\n",
      "train loss:0.005090238795289578\n",
      "train loss:0.0005766423330292477\n",
      "train loss:0.004329519854331291\n",
      "train loss:0.0076045378749114555\n",
      "train loss:0.002521971285027527\n",
      "train loss:0.06288014862790589\n",
      "train loss:0.08080967371743655\n",
      "train loss:0.006965719525587623\n",
      "train loss:0.04080646355634614\n",
      "train loss:0.0034899634822386505\n",
      "train loss:0.06310668812665411\n",
      "train loss:0.007585568354620349\n",
      "train loss:0.010033138643245017\n",
      "train loss:0.012953841047430088\n",
      "train loss:0.018327143665016626\n",
      "train loss:0.008683037863032839\n",
      "train loss:0.12318702612971892\n",
      "train loss:0.01124361728780241\n",
      "train loss:0.0005865405217748428\n",
      "train loss:0.0010023951857233324\n",
      "train loss:0.01082067289862466\n",
      "train loss:0.01761622803443404\n",
      "train loss:0.008381582638851503\n",
      "train loss:0.00824946784135405\n",
      "train loss:0.006349798953328374\n",
      "train loss:0.018287539046392035\n",
      "train loss:0.00477069258961993\n",
      "train loss:0.045378740729508676\n",
      "train loss:0.03922652093344636\n",
      "train loss:0.007540748638658771\n",
      "train loss:0.015972156821503502\n",
      "train loss:0.00209302206993422\n",
      "train loss:0.009073259115794433\n",
      "train loss:0.021146132813420353\n",
      "train loss:0.037301742943627494\n",
      "train loss:0.0015140434690278691\n",
      "train loss:0.006806215876107966\n",
      "train loss:0.013518743949244617\n",
      "train loss:0.009905362609605107\n",
      "train loss:0.0025661569967998328\n",
      "train loss:0.013025260302738621\n",
      "train loss:0.031443963516552424\n",
      "train loss:0.0019272396861297542\n",
      "=== epoch:10, train acc:0.989, test acc:0.986 ===\n",
      "train loss:0.015996624682685666\n",
      "train loss:0.0016220158008031517\n",
      "train loss:0.0017268482247502382\n",
      "train loss:0.009002900099788418\n",
      "train loss:0.01227626518402883\n",
      "train loss:0.0008402962577848443\n",
      "train loss:0.0009615917475807133\n",
      "train loss:0.0018000592118141054\n",
      "train loss:0.002535200614533386\n",
      "train loss:0.01690542703679426\n",
      "train loss:0.025189848010719596\n",
      "train loss:0.009401000392991856\n",
      "train loss:0.005867186711659014\n",
      "train loss:0.024190559568563086\n",
      "train loss:0.00402025452908901\n",
      "train loss:0.001802188945474331\n",
      "train loss:0.006152266556420041\n",
      "train loss:0.014533194153936502\n",
      "train loss:0.0014269945051957752\n",
      "train loss:0.0033696750135421318\n",
      "train loss:0.0018538708985287508\n",
      "train loss:0.01919255262336302\n",
      "train loss:0.002751690741842026\n",
      "train loss:0.027184783760625986\n",
      "train loss:0.006113318825313796\n",
      "train loss:0.006327257915354578\n",
      "train loss:0.0026923138652516177\n",
      "train loss:0.008102224828758699\n",
      "train loss:0.0009565742508470984\n",
      "train loss:0.06871932422697703\n",
      "train loss:0.055706033643557945\n",
      "train loss:0.005456154105561095\n",
      "train loss:0.0047309735242660285\n",
      "train loss:0.00645994909974572\n",
      "train loss:0.007699100958826913\n",
      "train loss:0.008865832393000596\n",
      "train loss:0.002482983732647531\n",
      "train loss:0.025298662603620615\n",
      "train loss:0.016837498536468442\n",
      "train loss:0.060969236048148495\n",
      "train loss:0.01852570115632586\n",
      "train loss:0.019997936685493586\n",
      "train loss:0.019256129377563495\n",
      "train loss:0.006754233543317751\n",
      "train loss:0.0014450590883598347\n",
      "train loss:0.003269228186084236\n",
      "train loss:0.07703320788333862\n",
      "train loss:0.028605416947872313\n",
      "train loss:0.0362799988171414\n",
      "train loss:0.005088857895595885\n",
      "train loss:0.0033786753015010397\n",
      "train loss:0.020967144639832857\n",
      "train loss:0.007340033290721106\n",
      "train loss:0.005893891055620791\n",
      "train loss:0.0065104158693325255\n",
      "train loss:0.008107200669235462\n",
      "train loss:0.07213279990426531\n",
      "train loss:0.02092880656541508\n",
      "train loss:0.005489288241016728\n",
      "train loss:0.008304863395010464\n",
      "train loss:0.005078551533649215\n",
      "train loss:0.008370015037161494\n",
      "train loss:0.010744146449433898\n",
      "train loss:0.004148425235030268\n",
      "train loss:0.019631610339584563\n",
      "train loss:0.006198094685188565\n",
      "train loss:0.012047195116883028\n",
      "train loss:0.010443671788496136\n",
      "train loss:0.006206377579912857\n",
      "train loss:0.01709508375897067\n",
      "train loss:0.003959177684627609\n",
      "train loss:0.007134008784772083\n",
      "train loss:0.0007888863605661767\n",
      "train loss:0.007300576351461806\n",
      "train loss:0.0021630092543530118\n",
      "train loss:0.021490337719714273\n",
      "train loss:0.004167330057250293\n",
      "train loss:0.0010102422375822576\n",
      "train loss:0.004758468697473967\n",
      "train loss:0.008313374612287028\n",
      "train loss:0.0010569891703630149\n",
      "train loss:0.0057186569879997574\n",
      "train loss:0.0031529279523948035\n",
      "train loss:0.038075240787758854\n",
      "train loss:0.06004351264972143\n",
      "train loss:0.004135902236669545\n",
      "train loss:0.009160077407423813\n",
      "train loss:0.02169242736587631\n",
      "train loss:0.0013218782739603254\n",
      "train loss:0.02527262016627145\n",
      "train loss:0.023299746797637605\n",
      "train loss:0.00872632820648315\n",
      "train loss:0.00984498094698334\n",
      "train loss:0.005443504264005488\n",
      "train loss:0.006413233592517687\n",
      "train loss:0.0014191074350325176\n",
      "train loss:0.0007284531422180625\n",
      "train loss:0.012900702760467356\n",
      "train loss:0.009877462014277727\n",
      "train loss:0.006936014672968147\n",
      "train loss:0.0033593294838577615\n",
      "train loss:0.0003794158765274417\n",
      "train loss:0.010844767814167577\n",
      "train loss:0.0028518390593117475\n",
      "train loss:0.02061451730354914\n",
      "train loss:0.00480477290496024\n",
      "train loss:0.02909824031043467\n",
      "train loss:0.04636735932501396\n",
      "train loss:0.005201888535281397\n",
      "train loss:0.021166356289511735\n",
      "train loss:0.0073346704622334466\n",
      "train loss:0.0025613460417243843\n",
      "train loss:0.008064500194090437\n",
      "train loss:0.0020413807711135794\n",
      "train loss:0.012568589834280568\n",
      "train loss:0.023926098901639832\n",
      "train loss:0.0014418624360377369\n",
      "train loss:0.03160677758216546\n",
      "train loss:0.0011722614252021174\n",
      "train loss:0.009031460198225132\n",
      "train loss:0.025811844695851915\n",
      "train loss:0.0029742251649424573\n",
      "train loss:0.0007874799273902754\n",
      "train loss:0.033795567887140575\n",
      "train loss:0.016431985504092764\n",
      "train loss:0.02096189572958394\n",
      "train loss:0.009397470916109924\n",
      "train loss:0.0027672226706647357\n",
      "train loss:0.007690206002000854\n",
      "train loss:0.041617077341079584\n",
      "train loss:0.0036411657056808072\n",
      "train loss:0.008069724531275102\n",
      "train loss:0.036886372299117384\n",
      "train loss:0.0013462949943405663\n",
      "train loss:0.006268165156249873\n",
      "train loss:0.009737747969143113\n",
      "train loss:0.05072962905767156\n",
      "train loss:0.00592940325413408\n",
      "train loss:0.019419080041595645\n",
      "train loss:0.05010404818349508\n",
      "train loss:0.003374639581465569\n",
      "train loss:0.003251073820443878\n",
      "train loss:0.03498439841487976\n",
      "train loss:0.012196028473403226\n",
      "train loss:0.006620779557465302\n",
      "train loss:0.025862673289408358\n",
      "train loss:0.00520746351144318\n",
      "train loss:0.0031099144597682166\n",
      "train loss:0.005284287425893211\n",
      "train loss:0.009882719591206316\n",
      "train loss:0.0067715235772359715\n",
      "train loss:0.009218811108577907\n",
      "train loss:0.021243037405278543\n",
      "train loss:0.044728946321078\n",
      "train loss:0.0017045692830221066\n",
      "train loss:0.0018752645292005727\n",
      "train loss:0.005660785054598537\n",
      "train loss:0.004108447240999059\n",
      "train loss:0.007947370932272164\n",
      "train loss:0.006958190004105696\n",
      "train loss:0.004719214562251957\n",
      "train loss:0.0047976691662767424\n",
      "train loss:0.0103582060883101\n",
      "train loss:0.0006949488631176309\n",
      "train loss:0.0029048190708092342\n",
      "train loss:0.007867124173970789\n",
      "train loss:0.009385926424445508\n",
      "train loss:0.009405814437276606\n",
      "train loss:0.008163842880788214\n",
      "train loss:0.009163894137196742\n",
      "train loss:0.01138458551874103\n",
      "train loss:0.002659954283927496\n",
      "train loss:0.010922319951707189\n",
      "train loss:0.004740663075915839\n",
      "train loss:0.0006822021480176469\n",
      "train loss:0.0074641066810273125\n",
      "train loss:0.010421249383006963\n",
      "train loss:0.008206646881536661\n",
      "train loss:0.008902156811831936\n",
      "train loss:0.0021478480444360733\n",
      "train loss:0.042698217352701864\n",
      "train loss:0.002677678836794384\n",
      "train loss:0.006280721353734968\n",
      "train loss:0.023149296641420746\n",
      "train loss:0.007827350390335634\n",
      "train loss:0.009406844559561119\n",
      "train loss:0.0031637055264694515\n",
      "train loss:0.008775253114652775\n",
      "train loss:0.013009317437551474\n",
      "train loss:0.0015390926241390585\n",
      "train loss:0.015830700175430156\n",
      "train loss:0.005292580506449434\n",
      "train loss:0.010296394992513856\n",
      "train loss:0.0423299313735474\n",
      "train loss:0.02528526617809744\n",
      "train loss:0.008176392732437575\n",
      "train loss:0.004457203328790082\n",
      "train loss:0.01795687655355906\n",
      "train loss:0.012162218357929314\n",
      "train loss:0.007010561148657566\n",
      "train loss:0.0028769510005593507\n",
      "train loss:0.004300051756389901\n",
      "train loss:0.004972367275050207\n",
      "train loss:0.0033096958747375737\n",
      "train loss:0.011446153950046407\n",
      "train loss:0.004623390019452554\n",
      "train loss:0.014179942824449492\n",
      "train loss:0.016672933668400475\n",
      "train loss:0.0033749253984702954\n",
      "train loss:0.0024321803516896222\n",
      "train loss:0.0052207331601875975\n",
      "train loss:0.002720156049612894\n",
      "train loss:0.0010779796934542157\n",
      "train loss:0.009123710996712282\n",
      "train loss:0.002439491119837518\n",
      "train loss:0.0014164937182769139\n",
      "train loss:0.005108088689591846\n",
      "train loss:0.0038036498689004933\n",
      "train loss:0.006519147135770925\n",
      "train loss:0.01244998056938194\n",
      "train loss:0.0009820096847648035\n",
      "train loss:0.0011583141398185145\n",
      "train loss:0.000498602613106691\n",
      "train loss:0.004544654250025739\n",
      "train loss:0.002992421862521747\n",
      "train loss:0.003427540031713176\n",
      "train loss:0.010304072587825196\n",
      "train loss:0.022211846523249607\n",
      "train loss:0.00393635414381962\n",
      "train loss:0.002812961185629181\n",
      "train loss:0.01596728438670256\n",
      "train loss:0.0024010027025836464\n",
      "train loss:0.004317279926696777\n",
      "train loss:0.006342008955339127\n",
      "train loss:0.0027030995739954805\n",
      "train loss:0.0005434697485811186\n",
      "train loss:0.006125658050412676\n",
      "train loss:0.003712777954690081\n",
      "train loss:0.006041661911905273\n",
      "train loss:0.0029416282609010923\n",
      "train loss:0.01264984484680654\n",
      "train loss:0.007369787966882174\n",
      "train loss:0.003502157689303437\n",
      "train loss:0.01312273067954919\n",
      "train loss:0.0007350110949932838\n",
      "train loss:0.0007160749312840679\n",
      "train loss:0.005937905710379095\n",
      "train loss:0.0006504373658972383\n",
      "train loss:0.002903182385303269\n",
      "train loss:0.006736554573996761\n",
      "train loss:0.02396854469233606\n",
      "train loss:0.004784484575256779\n",
      "train loss:0.002615547549972691\n",
      "train loss:0.004707265313201942\n",
      "train loss:0.008729195074551705\n",
      "train loss:0.0012761066941063234\n",
      "train loss:0.009553630337170958\n",
      "train loss:0.00560617063973617\n",
      "train loss:0.006360152413759254\n",
      "train loss:0.012231036170095135\n",
      "train loss:0.0102864498252791\n",
      "train loss:0.0039769870753050465\n",
      "train loss:0.0026386504280927463\n",
      "train loss:0.0473012919099825\n",
      "train loss:0.009402434286322918\n",
      "train loss:0.0015497855578476025\n",
      "train loss:0.010033793700517496\n",
      "train loss:0.007518438008537729\n",
      "train loss:0.008278012352193633\n",
      "train loss:0.003568342763245623\n",
      "train loss:0.003334631397090454\n",
      "train loss:0.0032400260682613697\n",
      "train loss:0.0014043941151814423\n",
      "train loss:0.11742412535415596\n",
      "train loss:0.007434621090694993\n",
      "train loss:0.02105767504763294\n",
      "train loss:0.0018268485188513905\n",
      "train loss:0.0035719006771739313\n",
      "train loss:0.009255510929124584\n",
      "train loss:0.0015132245464778687\n",
      "train loss:0.002082127772089871\n",
      "train loss:0.028353357582322956\n",
      "train loss:0.012930544482340614\n",
      "train loss:0.0027185492196204687\n",
      "train loss:0.0008855357018587637\n",
      "train loss:0.001548805749173123\n",
      "train loss:0.004963076448301951\n",
      "train loss:0.02444749880939065\n",
      "train loss:0.003507607191953922\n",
      "train loss:0.00815314362518056\n",
      "train loss:0.008082804009471263\n",
      "train loss:0.00411519584117092\n",
      "train loss:0.0013189513864089253\n",
      "train loss:0.0134702885827457\n",
      "train loss:0.009024299758932333\n",
      "train loss:0.006857996881253507\n",
      "train loss:0.0036007427708742833\n",
      "train loss:0.004809536752523324\n",
      "train loss:0.0024854093923369616\n",
      "train loss:0.012558997842385077\n",
      "train loss:0.0032718764378536417\n",
      "train loss:0.0028066116586759264\n",
      "train loss:0.0006707292203316547\n",
      "train loss:0.0025543723724161765\n",
      "train loss:0.0031560705399722305\n",
      "train loss:0.003010494398608974\n",
      "train loss:0.014339091135325216\n",
      "train loss:0.002199901452672051\n",
      "train loss:0.00861077208805473\n",
      "train loss:0.01910729146614807\n",
      "train loss:0.005890400505725411\n",
      "train loss:0.002146083317277511\n",
      "train loss:0.001012864166790338\n",
      "train loss:0.002918239925717569\n",
      "train loss:0.004703548026955399\n",
      "train loss:0.004344817317647485\n",
      "train loss:0.0025482985254025865\n",
      "train loss:0.05066844735210819\n",
      "train loss:0.005332908381615032\n",
      "train loss:0.022655206014425004\n",
      "train loss:0.003956455397823503\n",
      "train loss:0.003977458041810994\n",
      "train loss:0.0006146704857399344\n",
      "train loss:0.0114205977298797\n",
      "train loss:0.004286768820180465\n",
      "train loss:0.0031282664542102937\n",
      "train loss:0.0023542798151926303\n",
      "train loss:0.0041463909467726314\n",
      "train loss:0.0022218848831059455\n",
      "train loss:0.004563042702627997\n",
      "train loss:0.00815542238974059\n",
      "train loss:0.023648313266124755\n",
      "train loss:0.0044345349195983805\n",
      "train loss:0.005802700018402321\n",
      "train loss:0.004925222132569471\n",
      "train loss:0.003625396104335101\n",
      "train loss:0.004635751979985752\n",
      "train loss:0.0038925087714429527\n",
      "train loss:0.012788419321399958\n",
      "train loss:0.01166514320503216\n",
      "train loss:0.011587507179749813\n",
      "train loss:0.027310721199774027\n",
      "train loss:0.01802274107198663\n",
      "train loss:0.003946876247183181\n",
      "train loss:0.017773783044417988\n",
      "train loss:0.0009399424591880782\n",
      "train loss:0.006888089158980538\n",
      "train loss:0.00596635764272854\n",
      "train loss:0.007896777317999752\n",
      "train loss:0.007368908325500232\n",
      "train loss:0.0021342399421971194\n",
      "train loss:0.006029932918642875\n",
      "train loss:0.009984321861925176\n",
      "train loss:0.0016313222903204488\n",
      "train loss:0.051963417242266684\n",
      "train loss:0.039343517624911735\n",
      "train loss:0.0077874862703356425\n",
      "train loss:0.01339795458637589\n",
      "train loss:0.009497213405100024\n",
      "train loss:0.01727697555302441\n",
      "train loss:0.0011493795912610487\n",
      "train loss:0.023757053494945734\n",
      "train loss:0.00918489565348317\n",
      "train loss:0.0005839689313912741\n",
      "train loss:0.0016094455772492809\n",
      "train loss:0.00787740727571221\n",
      "train loss:0.0040601300060732716\n",
      "train loss:0.0023788677295593164\n",
      "train loss:0.014112174016764276\n",
      "train loss:0.0060912176507541315\n",
      "train loss:0.0023936341847340336\n",
      "train loss:0.009053931021412265\n",
      "train loss:0.0006162888978029678\n",
      "train loss:0.017170061153412627\n",
      "train loss:0.0018098990209692123\n",
      "train loss:0.002928671977368095\n",
      "train loss:0.004607613092413513\n",
      "train loss:0.009943008503391151\n",
      "train loss:0.0126194163183344\n",
      "train loss:0.0037841921563654753\n",
      "train loss:0.004922124959967669\n",
      "train loss:0.0037627270689780663\n",
      "train loss:0.0028150703555583495\n",
      "train loss:0.004555264855419209\n",
      "train loss:0.002046369465049008\n",
      "train loss:0.005486340560965821\n",
      "train loss:0.003993919202990776\n",
      "train loss:0.0007622835326216141\n",
      "train loss:0.0025836353903201338\n",
      "train loss:0.008086215398358427\n",
      "train loss:0.010090359934501074\n",
      "train loss:0.0005569365819751938\n",
      "train loss:0.009112371146391263\n",
      "train loss:0.0009650493048310163\n",
      "train loss:0.00024569257474754686\n",
      "train loss:0.006268796289902454\n",
      "train loss:0.02432826944575487\n",
      "train loss:0.0021642674797698923\n",
      "train loss:0.0009682864964806484\n",
      "train loss:0.004808229206251503\n",
      "train loss:0.0037207677097368364\n",
      "train loss:0.009962326147615991\n",
      "train loss:0.09788965950173567\n",
      "train loss:0.0034218617534720374\n",
      "train loss:0.010718877907692244\n",
      "train loss:0.08518312958046297\n",
      "train loss:0.007105575119309118\n",
      "train loss:0.007036059746483604\n",
      "train loss:0.0058221264769884006\n",
      "train loss:0.006435777939637607\n",
      "train loss:0.007530513061423173\n",
      "train loss:0.03999670709361115\n",
      "train loss:0.0010019510096784276\n",
      "train loss:0.002996690277533055\n",
      "train loss:0.0026484120016855395\n",
      "train loss:0.0029504387426562543\n",
      "train loss:0.008093173575210989\n",
      "train loss:0.0383899956601344\n",
      "train loss:0.013006016263397889\n",
      "train loss:0.012817834747441298\n",
      "train loss:0.0013210532551847939\n",
      "train loss:0.0025380349634705314\n",
      "train loss:0.0020208686508128597\n",
      "train loss:0.008749887870783662\n",
      "train loss:0.0092955277277041\n",
      "train loss:0.01746197147019291\n",
      "train loss:0.010123003898666301\n",
      "train loss:0.0012459911997227558\n",
      "train loss:0.0068653535282040936\n",
      "train loss:0.033973471851694644\n",
      "train loss:0.05054862147644751\n",
      "train loss:0.001602662973189878\n",
      "train loss:0.0024301189388487975\n",
      "train loss:0.02739841426838942\n",
      "train loss:0.009171790037978382\n",
      "train loss:0.0026902010441359225\n",
      "train loss:0.03975813738361001\n",
      "train loss:0.00824504464871236\n",
      "train loss:0.0042552133858502405\n",
      "train loss:0.008643421480925935\n",
      "train loss:0.014664560076772488\n",
      "train loss:0.024292090573660365\n",
      "train loss:0.001961613082724108\n",
      "train loss:0.0031477893707206545\n",
      "train loss:0.0192510891813049\n",
      "train loss:0.010125255394156858\n",
      "train loss:0.009136860282117873\n",
      "train loss:0.008283751393948382\n",
      "train loss:0.0024143740062201194\n",
      "train loss:0.0005739501210151262\n",
      "train loss:0.0036228238007052814\n",
      "train loss:0.001586522393676519\n",
      "train loss:0.002307575858630836\n",
      "train loss:0.008980541817163757\n",
      "train loss:0.0005568029847676233\n",
      "train loss:0.0019820136390645685\n",
      "train loss:0.057991005915826684\n",
      "train loss:0.0019502664347499934\n",
      "train loss:0.002142598736146129\n",
      "train loss:0.01002470722485598\n",
      "train loss:0.013206885635238442\n",
      "train loss:0.00790315204034137\n",
      "train loss:0.0025119921947877964\n",
      "train loss:0.004851874144863093\n",
      "train loss:0.0029127548448737942\n",
      "train loss:0.05235971026294821\n",
      "train loss:0.0019532537582662036\n",
      "train loss:0.011582199022064511\n",
      "train loss:0.012494040861007569\n",
      "train loss:0.0036845842187610985\n",
      "train loss:0.014313049487178674\n",
      "train loss:0.003321245519703519\n",
      "train loss:0.006383098327545825\n",
      "train loss:0.001438087046672452\n",
      "train loss:0.002527608923977839\n",
      "train loss:0.0012246659142661168\n",
      "train loss:0.005182090299154552\n",
      "train loss:0.0028905248222735867\n",
      "train loss:0.00326555287905883\n",
      "train loss:0.0072726229551332245\n",
      "train loss:0.0007243944625759365\n",
      "train loss:0.005874530831999725\n",
      "train loss:0.004170473808096525\n",
      "train loss:0.001701979185577576\n",
      "train loss:0.0027664675361784878\n",
      "train loss:0.003937153535241384\n",
      "train loss:0.0033587269576930145\n",
      "train loss:0.01027965264637636\n",
      "train loss:0.010510027792632486\n",
      "train loss:0.004493976443009476\n",
      "train loss:0.001313780373790171\n",
      "train loss:0.004930890109997895\n",
      "train loss:0.034537810262837414\n",
      "train loss:0.00034903549480080654\n",
      "train loss:0.011400579548173806\n",
      "train loss:0.004411368265430035\n",
      "train loss:0.01643654507458605\n",
      "train loss:0.002276275986442443\n",
      "train loss:0.006360625608106695\n",
      "train loss:0.01007067014182985\n",
      "train loss:0.007822357598681742\n",
      "train loss:0.0025520549507859957\n",
      "train loss:0.005279209402601784\n",
      "train loss:0.0012305141016314338\n",
      "train loss:0.010129574796591994\n",
      "train loss:0.0018752215591203707\n",
      "train loss:0.01951731903265888\n",
      "train loss:0.0021240246787796423\n",
      "train loss:0.0070949757225908395\n",
      "train loss:0.0052883063048866745\n",
      "train loss:0.0012599858682285207\n",
      "train loss:0.0425792395408343\n",
      "train loss:0.0008168810175656499\n",
      "train loss:0.03057044001763833\n",
      "train loss:0.01273900370240643\n",
      "train loss:0.004103988902466279\n",
      "train loss:0.0053275620642564145\n",
      "train loss:0.015436758886528878\n",
      "train loss:0.004673105622165702\n",
      "train loss:0.0016574726668949838\n",
      "train loss:0.015979019353301312\n",
      "train loss:0.022046631701762416\n",
      "train loss:0.0007931382347444874\n",
      "train loss:0.0004025364861649755\n",
      "train loss:0.00297477180637093\n",
      "train loss:0.0013796484340658154\n",
      "train loss:0.012559679265241468\n",
      "train loss:0.0018569763981035855\n",
      "train loss:0.001768818095570372\n",
      "train loss:0.0032349756371822095\n",
      "train loss:0.0007765151237819471\n",
      "train loss:0.010878365953577544\n",
      "train loss:0.003612818042784934\n",
      "train loss:0.0051666113334429675\n",
      "train loss:0.009750050850888616\n",
      "train loss:0.00970253869022245\n",
      "train loss:0.003035735736373892\n",
      "train loss:0.023084550893019286\n",
      "train loss:0.0045385649104031905\n",
      "train loss:0.0038751243682528485\n",
      "train loss:0.009126864887375156\n",
      "train loss:0.0017580181992161584\n",
      "train loss:0.0014011497589693528\n",
      "train loss:0.00043884304556030803\n",
      "train loss:0.0026726564250018443\n",
      "train loss:0.005008900136225137\n",
      "train loss:0.014743650930080638\n",
      "train loss:0.008249659683902084\n",
      "train loss:0.010290664550992382\n",
      "train loss:0.0013136945259348146\n",
      "train loss:0.0008942179570358064\n",
      "train loss:0.003003384204174639\n",
      "train loss:0.007861301876275551\n",
      "train loss:0.002415477422428961\n",
      "train loss:0.008374516615140057\n",
      "train loss:0.0036477954668534753\n",
      "train loss:0.003194136092628249\n",
      "train loss:0.008491855285488726\n",
      "train loss:0.002465181086963993\n",
      "train loss:0.009490088948069925\n",
      "train loss:0.009750213323301763\n",
      "train loss:0.0011180828978709014\n",
      "train loss:0.0030949923217300705\n",
      "train loss:0.011622914826359328\n",
      "train loss:0.0031860777456000034\n",
      "train loss:0.0005700890414793481\n",
      "train loss:0.0016267795404055992\n",
      "train loss:0.018797210700793047\n",
      "train loss:0.003990663524295046\n",
      "train loss:0.004328941964452235\n",
      "train loss:0.006247846203819496\n",
      "train loss:0.010659118583765972\n",
      "train loss:0.0060274188342234695\n",
      "train loss:0.019971879284209827\n",
      "train loss:0.004381995707208396\n",
      "train loss:0.00331856220071771\n",
      "train loss:0.003854190310761247\n",
      "train loss:0.038324810139663386\n",
      "train loss:0.003461969908105318\n",
      "train loss:0.011188327856965184\n",
      "train loss:0.010937636563175953\n",
      "train loss:0.010658430416066143\n",
      "train loss:0.0020304070415459864\n",
      "train loss:0.002847323080867157\n",
      "train loss:0.07028819412710491\n",
      "train loss:0.015075805867760656\n",
      "train loss:0.015162164427960178\n",
      "train loss:0.0031393840256585936\n",
      "train loss:0.0035664408430483817\n",
      "train loss:0.008190681694501236\n",
      "train loss:0.0019357089922188253\n",
      "train loss:0.007230080520828506\n",
      "train loss:0.0009565602507821933\n",
      "train loss:0.0057721997825311185\n",
      "train loss:0.004523607533851321\n",
      "train loss:0.002354997962246609\n",
      "train loss:0.004184330720424733\n",
      "train loss:0.0012015502202306287\n",
      "train loss:0.0010043894867900513\n",
      "train loss:0.015826525420283705\n",
      "=== epoch:11, train acc:0.998, test acc:0.987 ===\n",
      "train loss:0.005262112801152372\n",
      "train loss:0.003881334804702225\n",
      "train loss:0.00997660895592206\n",
      "train loss:0.011234926021246611\n",
      "train loss:0.0037569202430777883\n",
      "train loss:0.004623343929573535\n",
      "train loss:0.001901187623811311\n",
      "train loss:0.012059527422464767\n",
      "train loss:0.03183583221534318\n",
      "train loss:0.038910130158547974\n",
      "train loss:0.0006413062228863172\n",
      "train loss:0.0013485003622353672\n",
      "train loss:0.010652483800740147\n",
      "train loss:0.006064245419506766\n",
      "train loss:0.00255655361197795\n",
      "train loss:0.0077004027998944105\n",
      "train loss:0.004675352505361736\n",
      "train loss:0.0034409528949584066\n",
      "train loss:0.006232767491095288\n",
      "train loss:0.00886636951092382\n",
      "train loss:0.009951432410486209\n",
      "train loss:0.0019477483084845654\n",
      "train loss:0.010126167860676076\n",
      "train loss:0.0043861789638994396\n",
      "train loss:0.00048242373296030123\n",
      "train loss:0.010590727194148553\n",
      "train loss:0.0903360827255431\n",
      "train loss:0.0005457858240619818\n",
      "train loss:0.008921736458842639\n",
      "train loss:0.007808467923004317\n",
      "train loss:0.0030378559610033506\n",
      "train loss:0.008067736875509768\n",
      "train loss:0.0032947730836781625\n",
      "train loss:0.012475380572173438\n",
      "train loss:0.017446383601987654\n",
      "train loss:0.00638368782978943\n",
      "train loss:0.00021268951870580106\n",
      "train loss:0.011988980155199008\n",
      "train loss:0.004421776673865057\n",
      "train loss:0.004795833880041333\n",
      "train loss:0.03308019365423465\n",
      "train loss:0.002199402706591847\n",
      "train loss:0.0058874231434522814\n",
      "train loss:0.0019239302855856106\n",
      "train loss:0.004032522405961427\n",
      "train loss:0.004804930719416148\n",
      "train loss:0.0089305192934949\n",
      "train loss:0.013644741541908274\n",
      "train loss:0.0007178881855398328\n",
      "train loss:0.0022834688107795046\n",
      "train loss:0.0029495179908478253\n",
      "train loss:0.005889483661871669\n",
      "train loss:0.005339209601702343\n",
      "train loss:0.0022986501430959944\n",
      "train loss:0.004432122307850656\n",
      "train loss:0.03197013389550657\n",
      "train loss:0.00263719387761261\n",
      "train loss:0.0018871215249732253\n",
      "train loss:0.0031877884015898855\n",
      "train loss:0.006938601344216758\n",
      "train loss:0.0009380048098958601\n",
      "train loss:0.004495103140574619\n",
      "train loss:0.001609535372695934\n",
      "train loss:0.006544880759242779\n",
      "train loss:0.012059068765479757\n",
      "train loss:0.001114956958145541\n",
      "train loss:0.008140692716883531\n",
      "train loss:0.0034412490662528265\n",
      "train loss:0.003758120390357466\n",
      "train loss:0.005155379545861039\n",
      "train loss:0.004694374116563989\n",
      "train loss:0.010540335001519643\n",
      "train loss:0.02126011235279413\n",
      "train loss:0.00354931756489143\n",
      "train loss:0.004086177006442097\n",
      "train loss:0.0037350120295703633\n",
      "train loss:0.0004994300750529217\n",
      "train loss:0.002898365027532359\n",
      "train loss:0.005478098985745\n",
      "train loss:0.0009884964821039557\n",
      "train loss:0.0015243010788197617\n",
      "train loss:0.007897918899364774\n",
      "train loss:0.0011904827277602974\n",
      "train loss:0.0034440329205106634\n",
      "train loss:0.001197251648364862\n",
      "train loss:0.0054540287733577195\n",
      "train loss:0.0026586904120249983\n",
      "train loss:0.007149115448556243\n",
      "train loss:0.020918502033851302\n",
      "train loss:0.0040091859012024375\n",
      "train loss:0.0024163791259425886\n",
      "train loss:0.006495265121318084\n",
      "train loss:0.0012966171275633827\n",
      "train loss:0.0052639416676414676\n",
      "train loss:0.010996846482707417\n",
      "train loss:0.001094665652238535\n",
      "train loss:0.013266999543062753\n",
      "train loss:0.002930718742484019\n",
      "train loss:0.0026424478090258867\n",
      "train loss:0.0014838021334158088\n",
      "train loss:0.0014166863533924988\n",
      "train loss:0.0014746210475545033\n",
      "train loss:0.006060078561633915\n",
      "train loss:0.012580996725772247\n",
      "train loss:0.026512044857815217\n",
      "train loss:0.002535648152977739\n",
      "train loss:0.004163343720738257\n",
      "train loss:0.007826849275810259\n",
      "train loss:0.0009985786646069766\n",
      "train loss:0.010777596809537606\n",
      "train loss:0.011740882896528393\n",
      "train loss:0.019491989340890213\n",
      "train loss:0.0074952491074521226\n",
      "train loss:0.0015796086897138877\n",
      "train loss:0.0012530321599098315\n",
      "train loss:0.011551548158782439\n",
      "train loss:0.004322483438426336\n",
      "train loss:0.00284850829602739\n",
      "train loss:0.0018419645758506304\n",
      "train loss:0.003342489356988302\n",
      "train loss:0.001310958437671682\n",
      "train loss:0.0024755801330301874\n",
      "train loss:0.002879008568366902\n",
      "train loss:0.0012515535171505245\n",
      "train loss:0.01164228996751141\n",
      "train loss:0.002564109766736533\n",
      "train loss:0.0021603339296405923\n",
      "train loss:0.006578771725748721\n",
      "train loss:0.0044002425314913\n",
      "train loss:0.018928729815321618\n",
      "train loss:0.010604691616461315\n",
      "train loss:0.005469999328542057\n",
      "train loss:0.00047717585144139095\n",
      "train loss:0.008618572560176545\n",
      "train loss:0.0028633314771902884\n",
      "train loss:0.04201709749912466\n",
      "train loss:0.0037944528718065495\n",
      "train loss:0.011784852488354253\n",
      "train loss:0.003278974744999025\n",
      "train loss:0.0062873900309754335\n",
      "train loss:0.0005339958156124578\n",
      "train loss:0.005155297161478129\n",
      "train loss:0.003138351001661234\n",
      "train loss:0.0003406953505235733\n",
      "train loss:0.0005040312681710512\n",
      "train loss:0.00038625487449362795\n",
      "train loss:0.0040741496722854365\n",
      "train loss:0.002862621777903111\n",
      "train loss:0.0055353845307689045\n",
      "train loss:0.005852953902143533\n",
      "train loss:0.007419165074443613\n",
      "train loss:0.002201910575393588\n",
      "train loss:0.012448659865003549\n",
      "train loss:0.0006689707150258023\n",
      "train loss:0.01977442768605131\n",
      "train loss:0.012858239358041024\n",
      "train loss:0.0034330078375777056\n",
      "train loss:0.014002264988380092\n",
      "train loss:0.0015595128161393584\n",
      "train loss:0.00201192795405886\n",
      "train loss:0.006381046472380735\n",
      "train loss:0.002847494131630917\n",
      "train loss:0.0021235496046187892\n",
      "train loss:0.0017368773133049661\n",
      "train loss:0.0018442878672591913\n",
      "train loss:0.00849025373874094\n",
      "train loss:0.008598774340231813\n",
      "train loss:0.005887881164927694\n",
      "train loss:0.003443209840241452\n",
      "train loss:0.0024783036288214674\n",
      "train loss:0.009521939523995313\n",
      "train loss:0.012791741935687242\n",
      "train loss:0.0008905061689564055\n",
      "train loss:0.0075994262589760995\n",
      "train loss:0.0033713461075776142\n",
      "train loss:0.01302656413870697\n",
      "train loss:0.011750859565378186\n",
      "train loss:0.004397453172404037\n",
      "train loss:0.00023856509971129195\n",
      "train loss:0.0037269934674429117\n",
      "train loss:0.0018082193262232086\n",
      "train loss:0.05046450944646486\n",
      "train loss:0.00874689968691292\n",
      "train loss:0.0019043084647319833\n",
      "train loss:0.0037213884904283316\n",
      "train loss:0.0043126158231014105\n",
      "train loss:0.01715056844180884\n",
      "train loss:0.004063047746581727\n",
      "train loss:0.002417051929418488\n",
      "train loss:0.0003668015375745989\n",
      "train loss:0.006103712543839051\n",
      "train loss:0.009799734669609761\n",
      "train loss:0.002287391843811293\n",
      "train loss:0.008275316450404016\n",
      "train loss:0.003010921634979094\n",
      "train loss:0.013046462637473449\n",
      "train loss:0.0029856786400116577\n",
      "train loss:0.011007323061072279\n",
      "train loss:0.005876037620551614\n",
      "train loss:0.004693726650766734\n",
      "train loss:0.003687954708047761\n",
      "train loss:0.0005390137437133898\n",
      "train loss:0.001292751056766975\n",
      "train loss:0.11768686974891143\n",
      "train loss:0.005368707806353089\n",
      "train loss:0.0039554700246339685\n",
      "train loss:0.0013978670388939227\n",
      "train loss:0.009879352969090198\n",
      "train loss:0.0008635782670848422\n",
      "train loss:0.0021177986164617295\n",
      "train loss:0.0016405039047097001\n",
      "train loss:0.002948394208488571\n",
      "train loss:0.0029192918523130817\n",
      "train loss:0.004113632260917128\n",
      "train loss:0.0011747586338356328\n",
      "train loss:0.0061516553796468094\n",
      "train loss:0.00699041608891016\n",
      "train loss:0.021508097022935546\n",
      "train loss:0.0018733539853058195\n",
      "train loss:0.024957951768008513\n",
      "train loss:0.006541341672584348\n",
      "train loss:0.005417964111486414\n",
      "train loss:0.0009484849829451841\n",
      "train loss:0.00448391903928057\n",
      "train loss:0.008413143050522183\n",
      "train loss:0.011382994138834151\n",
      "train loss:0.016325344436458424\n",
      "train loss:0.011916444034145122\n",
      "train loss:0.0021563617592437272\n",
      "train loss:0.0018561407782602608\n",
      "train loss:0.004549202807228717\n",
      "train loss:0.0035510524345431114\n",
      "train loss:0.005773916275808696\n",
      "train loss:0.0021705568749904496\n",
      "train loss:0.0016460857454067687\n",
      "train loss:0.006524299010949518\n",
      "train loss:0.0008433131763134676\n",
      "train loss:0.003310700508399649\n",
      "train loss:0.00942358911684891\n",
      "train loss:0.003013894907242457\n",
      "train loss:0.002133731756230432\n",
      "train loss:0.003228215735459829\n",
      "train loss:0.0025226310104340703\n",
      "train loss:0.006533391054272696\n",
      "train loss:0.002903368837884135\n",
      "train loss:0.0011394349858625027\n",
      "train loss:0.006100247350277589\n",
      "train loss:0.002311627308515004\n",
      "train loss:0.007364571762531633\n",
      "train loss:0.0081374406160597\n",
      "train loss:0.0005468936742324896\n",
      "train loss:0.0011856639462804203\n",
      "train loss:0.004229727636467319\n",
      "train loss:0.003394881505570683\n",
      "train loss:0.004360343265740504\n",
      "train loss:0.001792292755253779\n",
      "train loss:0.048090982824438265\n",
      "train loss:0.0040423367139429504\n",
      "train loss:0.0013255843637705053\n",
      "train loss:0.007795287743871188\n",
      "train loss:0.0041270455144873895\n",
      "train loss:0.001075886118965761\n",
      "train loss:0.004435456127556149\n",
      "train loss:0.00837709260650926\n",
      "train loss:0.003068545809052649\n",
      "train loss:0.006327820317734455\n",
      "train loss:0.06622238311283885\n",
      "train loss:0.021183247709632237\n",
      "train loss:0.003330698000597911\n",
      "train loss:0.004330319864362728\n",
      "train loss:0.012568126293996793\n",
      "train loss:0.010949163572202764\n",
      "train loss:0.0016997192977615838\n",
      "train loss:0.00422933659479728\n",
      "train loss:0.02256256272823247\n",
      "train loss:0.009506433361042834\n",
      "train loss:0.00289250704039983\n",
      "train loss:0.028136648053387395\n",
      "train loss:0.0030220617844517465\n",
      "train loss:0.026265429483944405\n",
      "train loss:0.003616251704114482\n",
      "train loss:0.015548366088181428\n",
      "train loss:0.0024898846702384255\n",
      "train loss:0.0029341254061515604\n",
      "train loss:0.020571934838411042\n",
      "train loss:0.007980233985864471\n",
      "train loss:0.0070906319178558\n",
      "train loss:0.002047848217521946\n",
      "train loss:0.0008483081702562894\n",
      "train loss:0.0020869333631877212\n",
      "train loss:0.0008887772596295633\n",
      "train loss:0.0007740523559694135\n",
      "train loss:0.0895480900958208\n",
      "train loss:0.009935971153305195\n",
      "train loss:0.003710425547003539\n",
      "train loss:0.007013202393493747\n",
      "train loss:0.0006668517023855318\n",
      "train loss:0.013639686920968373\n",
      "train loss:0.0035992595986986857\n",
      "train loss:0.004641796371630087\n",
      "train loss:0.028546072657559783\n",
      "train loss:0.012360581761031462\n",
      "train loss:0.02073208679800112\n",
      "train loss:0.007704405767747851\n",
      "train loss:0.004889061651152136\n",
      "train loss:0.005123429472888393\n",
      "train loss:0.011574447666146131\n",
      "train loss:0.0010718975449473785\n",
      "train loss:0.005410058421205499\n",
      "train loss:0.002121629099716446\n",
      "train loss:0.022775288422700025\n",
      "train loss:0.019525569685585064\n",
      "train loss:0.01642077052556168\n",
      "train loss:0.0060428636682296226\n",
      "train loss:0.013540446170691051\n",
      "train loss:0.003359477005187665\n",
      "train loss:0.008126529017170734\n",
      "train loss:0.003897802586205324\n",
      "train loss:0.03064459195251705\n",
      "train loss:0.0035275200178047812\n",
      "train loss:0.0018525378628590841\n",
      "train loss:0.009342622430608995\n",
      "train loss:0.0022171164189259338\n",
      "train loss:0.008824725100969245\n",
      "train loss:0.00665666229011586\n",
      "train loss:0.00737050153063345\n",
      "train loss:0.003233670760565777\n",
      "train loss:0.007351403283293355\n",
      "train loss:0.05019898317492518\n",
      "train loss:0.010638265254897266\n",
      "train loss:0.01475147340692668\n",
      "train loss:0.004347077186426623\n",
      "train loss:0.005067609105285903\n",
      "train loss:0.0027735557080427863\n",
      "train loss:0.0026065996579775898\n",
      "train loss:0.029880155491164934\n",
      "train loss:0.0026438287167723706\n",
      "train loss:0.0015625649215789258\n",
      "train loss:0.001639560450882269\n",
      "train loss:0.004296760965538441\n",
      "train loss:0.003211485894641861\n",
      "train loss:0.014622149764944864\n",
      "train loss:0.0012081998652307682\n",
      "train loss:0.0011131597523768293\n",
      "train loss:0.0472520223373365\n",
      "train loss:0.021001115198473322\n",
      "train loss:0.006097823018588123\n",
      "train loss:0.005159820174826481\n",
      "train loss:0.0015534370788262233\n",
      "train loss:0.003157219003359704\n",
      "train loss:0.020629902469026917\n",
      "train loss:0.0005968982627699028\n",
      "train loss:0.029286510873300507\n",
      "train loss:0.010327112108245058\n",
      "train loss:0.007979667065880502\n",
      "train loss:0.004182396004339086\n",
      "train loss:0.007076927679806551\n",
      "train loss:0.005312403265390153\n",
      "train loss:0.000923313941770649\n",
      "train loss:0.0029062668866091266\n",
      "train loss:0.0026151179938496655\n",
      "train loss:0.010310194742216878\n",
      "train loss:0.0017927309776437242\n",
      "train loss:0.005434145723957416\n",
      "train loss:0.005441906957906781\n",
      "train loss:0.010646872604679194\n",
      "train loss:0.003662860958108866\n",
      "train loss:0.006497312409400755\n",
      "train loss:0.009304766343648893\n",
      "train loss:0.00014521745446463945\n",
      "train loss:0.012385154575239142\n",
      "train loss:0.0008492350837468977\n",
      "train loss:0.009092784416503288\n",
      "train loss:0.0037820333712627312\n",
      "train loss:0.004726195988227773\n",
      "train loss:0.00381057687823101\n",
      "train loss:0.07587322311003294\n",
      "train loss:0.0035278349528490276\n",
      "train loss:0.005144209954947005\n",
      "train loss:0.03192360772933363\n",
      "train loss:0.008649284330904746\n",
      "train loss:0.008145023278875299\n",
      "train loss:0.003012401383201654\n",
      "train loss:0.05627762151207816\n",
      "train loss:0.0012339084335449035\n",
      "train loss:0.004835205460163792\n",
      "train loss:0.01235381367376449\n",
      "train loss:0.012262540722734843\n",
      "train loss:0.0027074249949666195\n",
      "train loss:0.0062729381900931995\n",
      "train loss:0.00694507033308641\n",
      "train loss:0.013068458109434088\n",
      "train loss:0.004084519779697237\n",
      "train loss:0.008956506409578274\n",
      "train loss:0.003456984296679405\n",
      "train loss:0.005937820622778738\n",
      "train loss:0.007277204279163112\n",
      "train loss:0.0023525648416700034\n",
      "train loss:0.021253664416125403\n",
      "train loss:0.001022806657172317\n",
      "train loss:0.003575749744804133\n",
      "train loss:0.013812378068450986\n",
      "train loss:0.012429978484804254\n",
      "train loss:0.0037098084513430426\n",
      "train loss:0.009460731359001778\n",
      "train loss:0.006275800007286697\n",
      "train loss:0.006518654638775146\n",
      "train loss:0.00828076466562563\n",
      "train loss:0.0005659374846357137\n",
      "train loss:0.0018926143551891907\n",
      "train loss:0.004697151331308481\n",
      "train loss:0.0034584635882317196\n",
      "train loss:0.022743543077022362\n",
      "train loss:0.008025880284753291\n",
      "train loss:0.0034195426169068605\n",
      "train loss:0.0014000228692664\n",
      "train loss:0.0057681310914324225\n",
      "train loss:0.0066415706038381195\n",
      "train loss:0.039303658712718875\n",
      "train loss:0.005037930029744654\n",
      "train loss:0.00534378006329366\n",
      "train loss:0.001417070974778203\n",
      "train loss:0.013007886966375006\n",
      "train loss:0.008069741341661186\n",
      "train loss:0.0059770700736921645\n",
      "train loss:0.004211461061732042\n",
      "train loss:0.0014514509569526985\n",
      "train loss:0.006751992935589923\n",
      "train loss:0.0010836099570279347\n",
      "train loss:0.002825740150383183\n",
      "train loss:0.014180378618767689\n",
      "train loss:0.006498779887574718\n",
      "train loss:0.006719675585649486\n",
      "train loss:0.0032423184737376577\n",
      "train loss:0.0032748348540364196\n",
      "train loss:0.003993637545052039\n",
      "train loss:0.008017303129842356\n",
      "train loss:0.004220144893600825\n",
      "train loss:0.008966198780096947\n",
      "train loss:0.0007878506135238848\n",
      "train loss:0.004864146031691503\n",
      "train loss:0.0035603649554861666\n",
      "train loss:0.0006553278251480075\n",
      "train loss:0.006282519291915223\n",
      "train loss:0.0021425526645851584\n",
      "train loss:0.0012915528544475753\n",
      "train loss:0.004069930131131884\n",
      "train loss:0.0022454239502534452\n",
      "train loss:0.006677579925944638\n",
      "train loss:0.003128070160903459\n",
      "train loss:0.01028242580107399\n",
      "train loss:0.017350866115440497\n",
      "train loss:0.002717974024150976\n",
      "train loss:0.0008508724056681171\n",
      "train loss:0.0037672622406175475\n",
      "train loss:0.0141046211376823\n",
      "train loss:0.0018495635941977286\n",
      "train loss:0.00022650634257083264\n",
      "train loss:0.001141923561035318\n",
      "train loss:0.0006133587434154488\n",
      "train loss:0.004996683478340176\n",
      "train loss:0.012517054713014935\n",
      "train loss:0.001407869366991445\n",
      "train loss:0.0013758520112163162\n",
      "train loss:0.019698112218405738\n",
      "train loss:0.00541032644572558\n",
      "train loss:0.008910352364681011\n",
      "train loss:0.001106203509989485\n",
      "train loss:0.0014144511805244792\n",
      "train loss:0.002240621223617338\n",
      "train loss:0.0021330438963453767\n",
      "train loss:0.0007348871746343525\n",
      "train loss:0.000632665539619148\n",
      "train loss:0.002990441976159336\n",
      "train loss:0.0021630408506542777\n",
      "train loss:0.0005009779401578078\n",
      "train loss:0.004962363250305369\n",
      "train loss:0.007990496395892608\n",
      "train loss:0.005183791176430521\n",
      "train loss:0.000440344668746491\n",
      "train loss:0.001421254330464116\n",
      "train loss:0.00792118620783436\n",
      "train loss:0.0008274595133394907\n",
      "train loss:0.0026961018130241543\n",
      "train loss:0.005485360675864346\n",
      "train loss:0.001562628228473673\n",
      "train loss:0.0032144706936065633\n",
      "train loss:0.07764392025270993\n",
      "train loss:0.004314337329982439\n",
      "train loss:0.00157627139581955\n",
      "train loss:0.0013213041121568772\n",
      "train loss:0.0021913111821889224\n",
      "train loss:0.008434796545866392\n",
      "train loss:0.0018846130154449156\n",
      "train loss:0.012108940756697375\n",
      "train loss:0.006416292274333129\n",
      "train loss:0.0035019833825266217\n",
      "train loss:0.000781545653075258\n",
      "train loss:0.00029358765916152717\n",
      "train loss:0.002478824520901632\n",
      "train loss:0.0005057642159103253\n",
      "train loss:0.006824958153353986\n",
      "train loss:0.006054354666960213\n",
      "train loss:0.007154084497084655\n",
      "train loss:0.0025581232606855468\n",
      "train loss:0.004381831448768304\n",
      "train loss:0.00261227935764044\n",
      "train loss:0.0014827377166375178\n",
      "train loss:0.0030694203134339964\n",
      "train loss:0.0010863195556679972\n",
      "train loss:0.011609912774353541\n",
      "train loss:0.0010943146006235655\n",
      "train loss:0.002769441109998808\n",
      "train loss:0.021257548711780476\n",
      "train loss:0.01891337474763049\n",
      "train loss:0.0027852483087359523\n",
      "train loss:0.0012534941861173901\n",
      "train loss:0.000947636213405022\n",
      "train loss:0.008983613250916227\n",
      "train loss:0.0011948360260050786\n",
      "train loss:0.007562420924323417\n",
      "train loss:0.007340956283934148\n",
      "train loss:0.0003909057705242567\n",
      "train loss:0.008462251853698172\n",
      "train loss:0.0009734046116214133\n",
      "train loss:0.006301569910399972\n",
      "train loss:0.013282186620513427\n",
      "train loss:0.0012730543215302003\n",
      "train loss:0.0017710093316028483\n",
      "train loss:0.006337078584197382\n",
      "train loss:0.003366893957531097\n",
      "train loss:0.0003016007581524166\n",
      "train loss:0.004701203487732753\n",
      "train loss:0.005619167458115707\n",
      "train loss:0.007195459434442993\n",
      "train loss:0.0025208371796885958\n",
      "train loss:0.005093236650175883\n",
      "train loss:0.006314308778998021\n",
      "train loss:0.029434984623205648\n",
      "train loss:0.003965201123972791\n",
      "train loss:0.0029041462272022455\n",
      "train loss:0.03329919294799822\n",
      "train loss:0.008085654459755452\n",
      "train loss:0.002973958365529687\n",
      "train loss:0.0034229549596401137\n",
      "train loss:0.0050313191433329265\n",
      "train loss:0.008863402651364633\n",
      "train loss:0.00011434609148094125\n",
      "train loss:0.006976907957511388\n",
      "train loss:0.0026897884278062317\n",
      "train loss:0.011008461302488426\n",
      "train loss:0.0034045008014021694\n",
      "train loss:0.004162760493890088\n",
      "train loss:0.009063585604248081\n",
      "train loss:0.0023749596873235287\n",
      "train loss:0.01672484450769378\n",
      "train loss:0.004563454642026174\n",
      "train loss:0.001843345193351239\n",
      "train loss:0.0019462959323520532\n",
      "train loss:0.004393412771111909\n",
      "train loss:0.0004601300030196428\n",
      "train loss:0.0025990937819183766\n",
      "train loss:0.0007193870070317404\n",
      "train loss:0.003189399867185733\n",
      "train loss:0.001785433097268262\n",
      "train loss:0.00036927353704485264\n",
      "train loss:0.005734840429707916\n",
      "train loss:0.0021562729744175006\n",
      "train loss:0.005099270526057119\n",
      "train loss:0.0023465451214766207\n",
      "train loss:0.00982507567944409\n",
      "train loss:0.0015762483908448263\n",
      "train loss:0.003469418984002516\n",
      "train loss:0.009232468052398893\n",
      "train loss:0.0010297429955253296\n",
      "train loss:0.0024160389797180846\n",
      "train loss:0.000371897489103214\n",
      "train loss:0.004493585100418041\n",
      "train loss:0.0028840529820806445\n",
      "train loss:0.005730102068089988\n",
      "train loss:0.006478010942411826\n",
      "train loss:0.00540023022508954\n",
      "train loss:0.0008726109742617412\n",
      "train loss:0.0001540678945805623\n",
      "train loss:0.0014989991682077151\n",
      "train loss:0.011946420906082942\n",
      "train loss:0.0023439751364276143\n",
      "train loss:0.0003445822803659103\n",
      "train loss:0.0038586610504379353\n",
      "train loss:0.006275308489766721\n",
      "train loss:0.00014403050241231104\n",
      "train loss:0.0011725367311505785\n",
      "train loss:0.0019049003569339457\n",
      "train loss:0.007796037529823136\n",
      "train loss:0.0018254708899640077\n",
      "train loss:0.004816614579015629\n",
      "train loss:0.001335750818677431\n",
      "train loss:0.0018436763593466222\n",
      "train loss:0.0033375026762395643\n",
      "train loss:0.0034018620055606117\n",
      "=== epoch:12, train acc:0.995, test acc:0.987 ===\n",
      "train loss:0.00030099722606342395\n",
      "train loss:0.002951979934325917\n",
      "train loss:0.0011785361048161857\n",
      "train loss:0.003719702222978897\n",
      "train loss:0.01826319538360647\n",
      "train loss:0.004472733074313067\n",
      "train loss:0.001441372365769629\n",
      "train loss:0.005838965175720885\n",
      "train loss:0.0022615933078986854\n",
      "train loss:0.015126563929868119\n",
      "train loss:0.0009778044969008175\n",
      "train loss:0.004504338720788691\n",
      "train loss:0.022055506869866518\n",
      "train loss:0.004075440067543169\n",
      "train loss:0.01212949499633818\n",
      "train loss:0.02498559229170184\n",
      "train loss:0.005710689302320839\n",
      "train loss:0.002283234204162792\n",
      "train loss:0.0028502253901129425\n",
      "train loss:0.010810937761395712\n",
      "train loss:0.0013783130428472615\n",
      "train loss:0.024700497911749553\n",
      "train loss:0.002851609235186273\n",
      "train loss:0.002594233415374863\n",
      "train loss:0.00235628549509239\n",
      "train loss:0.0004683789722443921\n",
      "train loss:0.0022552128212753905\n",
      "train loss:0.0007309835016074749\n",
      "train loss:0.006121415128139228\n",
      "train loss:0.011976371928583411\n",
      "train loss:0.0007375825987970255\n",
      "train loss:0.011537189646844132\n",
      "train loss:0.005708850535504726\n",
      "train loss:0.0073046433746334774\n",
      "train loss:0.0007784838031998993\n",
      "train loss:0.0026517353316901555\n",
      "train loss:0.00456915027964159\n",
      "train loss:0.005582406701105141\n",
      "train loss:0.014792189000280048\n",
      "train loss:0.006126873499778466\n",
      "train loss:0.003268923917787609\n",
      "train loss:0.006282128794268739\n",
      "train loss:0.0018248198608563485\n",
      "train loss:0.018330164760828993\n",
      "train loss:0.003457354124851519\n",
      "train loss:0.000585193842120519\n",
      "train loss:0.00690356911220252\n",
      "train loss:0.0015239018503384902\n",
      "train loss:0.008896840968773398\n",
      "train loss:0.04389092515622948\n",
      "train loss:0.004056699202403909\n",
      "train loss:0.00020034041340990356\n",
      "train loss:0.0022576007399148336\n",
      "train loss:0.0010404089040557437\n",
      "train loss:0.007706517214270469\n",
      "train loss:0.0053465060996594985\n",
      "train loss:0.0018086287502760297\n",
      "train loss:0.0053596012037330845\n",
      "train loss:0.006420645354795592\n",
      "train loss:0.011559462044390192\n",
      "train loss:0.004844618780579099\n",
      "train loss:0.0021357003524153473\n",
      "train loss:0.003748047568743486\n",
      "train loss:0.0035141556818690366\n",
      "train loss:0.0021598683190986834\n",
      "train loss:0.01076480603808018\n",
      "train loss:0.0003906613511871871\n",
      "train loss:0.001850643336612282\n",
      "train loss:0.00124881711317889\n",
      "train loss:0.001436841980188744\n",
      "train loss:0.014577049511682583\n",
      "train loss:0.0024207383563238693\n",
      "train loss:0.0001477248055616588\n",
      "train loss:0.002470431925268049\n",
      "train loss:0.045113948822783245\n",
      "train loss:0.001441711335265856\n",
      "train loss:0.009159585285467755\n",
      "train loss:0.005278670489970875\n",
      "train loss:0.0005306069257459353\n",
      "train loss:0.004434801720056974\n",
      "train loss:0.022184527745567513\n",
      "train loss:0.0031591419586887827\n",
      "train loss:0.0019509136139995076\n",
      "train loss:0.00033234722072748504\n",
      "train loss:0.009898017583298621\n",
      "train loss:0.015894178094903982\n",
      "train loss:0.0034817105052608994\n",
      "train loss:0.0032678177010566144\n",
      "train loss:0.004022749871968875\n",
      "train loss:0.00337241282569927\n",
      "train loss:0.0051843405263198835\n",
      "train loss:0.006279216009756964\n",
      "train loss:0.0019908814649802335\n",
      "train loss:0.005912126280293468\n",
      "train loss:0.00851153994131549\n",
      "train loss:0.008402197387609304\n",
      "train loss:0.0013869658736061117\n",
      "train loss:0.011934426711785951\n",
      "train loss:0.0023947978566808688\n",
      "train loss:0.006945641306400071\n",
      "train loss:0.0050740726778064895\n",
      "train loss:0.0124326534985288\n",
      "train loss:0.0248999104959467\n",
      "train loss:0.0027896930818311445\n",
      "train loss:0.0044746432120973305\n",
      "train loss:0.014417811471857911\n",
      "train loss:0.001971485695427837\n",
      "train loss:0.00352568632789009\n",
      "train loss:0.00928570356281647\n",
      "train loss:0.00280038595717276\n",
      "train loss:0.003640446218761477\n",
      "train loss:0.008238721287351265\n",
      "train loss:0.0022935105176550633\n",
      "train loss:0.0008127850723807644\n",
      "train loss:0.012890267143283262\n",
      "train loss:0.0007751338702403879\n",
      "train loss:0.0023714245853384787\n",
      "train loss:0.0016108780330602993\n",
      "train loss:0.009843314720847289\n",
      "train loss:0.0006161660215656413\n",
      "train loss:0.004883838125261689\n",
      "train loss:0.013671984115274503\n",
      "train loss:0.0024086814701594255\n",
      "train loss:0.006562960500881901\n",
      "train loss:0.0036981695371258185\n",
      "train loss:0.00015407672736051847\n",
      "train loss:0.005992650711506073\n",
      "train loss:0.0036650786722002677\n",
      "train loss:0.021196214386328332\n",
      "train loss:0.00036040674817470537\n",
      "train loss:0.004463300508457633\n",
      "train loss:0.007013082023700893\n",
      "train loss:0.0012497295693074046\n",
      "train loss:0.04070474786938512\n",
      "train loss:0.006858711225508125\n",
      "train loss:0.009325102967068021\n",
      "train loss:0.019424502409957446\n",
      "train loss:0.002311546342063077\n",
      "train loss:0.0024822694227711974\n",
      "train loss:0.0018455882836370613\n",
      "train loss:0.0025719186041537386\n",
      "train loss:0.0012472572529295314\n",
      "train loss:0.006615806787345904\n",
      "train loss:0.009797815948818097\n",
      "train loss:0.003143340173472669\n",
      "train loss:0.011791173988013772\n",
      "train loss:0.005260479273891704\n",
      "train loss:0.003508913028911798\n",
      "train loss:0.004516595981642462\n",
      "train loss:0.0045405181982680195\n",
      "train loss:0.0018379079872133156\n",
      "train loss:0.0010651275162969409\n",
      "train loss:0.0010376616982053972\n",
      "train loss:0.0012875470493945943\n",
      "train loss:0.0007250586531521784\n",
      "train loss:0.005352729164225454\n",
      "train loss:0.001007806366218631\n",
      "train loss:0.0006949565339543309\n",
      "train loss:0.0009661130017121929\n",
      "train loss:0.000602795645917965\n",
      "train loss:0.010242385979504975\n",
      "train loss:0.005621278296753503\n",
      "train loss:0.005248825864361118\n",
      "train loss:0.00041621186170815306\n",
      "train loss:0.0017080307124533014\n",
      "train loss:0.0006745650439869088\n",
      "train loss:0.0005911120962071758\n",
      "train loss:0.00047832640606841945\n",
      "train loss:0.03182069204463783\n",
      "train loss:0.0007978694766096198\n",
      "train loss:0.0013786682860912696\n",
      "train loss:0.0003125350929690262\n",
      "train loss:0.003592347136506112\n",
      "train loss:0.0011768821428235886\n",
      "train loss:0.0010806790176880274\n",
      "train loss:0.0035603746616223388\n",
      "train loss:0.0029580339884577285\n",
      "train loss:0.002810018111946507\n",
      "train loss:0.014842575623052\n",
      "train loss:0.00614295551198579\n",
      "train loss:0.000515713906250726\n",
      "train loss:0.017265878522570565\n",
      "train loss:0.009422144748614051\n",
      "train loss:0.0002488034621158339\n",
      "train loss:0.003023121758618168\n",
      "train loss:0.00720770885604237\n",
      "train loss:0.0011771880714999809\n",
      "train loss:0.016065278680916353\n",
      "train loss:0.0013593009422166802\n",
      "train loss:0.003464377916549084\n",
      "train loss:0.0007119983236651559\n",
      "train loss:0.009496925629054595\n",
      "train loss:0.008415469676941394\n",
      "train loss:0.0052578826022724205\n",
      "train loss:0.00360023430232192\n",
      "train loss:0.0025327119123276258\n",
      "train loss:0.002428028201583079\n",
      "train loss:0.0010091517732463019\n",
      "train loss:0.0014460774906844373\n",
      "train loss:0.010366435640715193\n",
      "train loss:0.013837437506437426\n",
      "train loss:0.011369505470316604\n",
      "train loss:0.0021281003296003327\n",
      "train loss:0.005003972964695628\n",
      "train loss:0.0021663026241975996\n",
      "train loss:0.004692958810560435\n",
      "train loss:0.00013625425682451555\n",
      "train loss:0.007265854285011245\n",
      "train loss:0.00961400199907779\n",
      "train loss:0.0015846741178068653\n",
      "train loss:0.00587922763863763\n",
      "train loss:0.019962852921989408\n",
      "train loss:0.0015256397045475445\n",
      "train loss:0.003325183715047126\n",
      "train loss:0.0005131802168538356\n",
      "train loss:0.009092519699421226\n",
      "train loss:0.0027922468770128993\n",
      "train loss:0.003583112412115643\n",
      "train loss:0.005100786449225406\n",
      "train loss:0.0008220910450738431\n",
      "train loss:0.0040610219157908\n",
      "train loss:0.001519745337652752\n",
      "train loss:0.0031054304604250915\n",
      "train loss:0.007404946412259914\n",
      "train loss:0.0005363566526298263\n",
      "train loss:0.006098382929610108\n",
      "train loss:0.0023473383769665695\n",
      "train loss:0.02106436399457479\n",
      "train loss:0.010077308903242085\n",
      "train loss:0.0008431406196097189\n",
      "train loss:0.01384882817469296\n",
      "train loss:0.0039016464992965376\n",
      "train loss:0.0071242383683489076\n",
      "train loss:0.0015669131626469637\n",
      "train loss:0.012916121342836711\n",
      "train loss:0.0012016980894226587\n",
      "train loss:0.0032915541701388357\n",
      "train loss:0.003127099714501157\n",
      "train loss:0.0010558127288648371\n",
      "train loss:0.00020710366591761685\n",
      "train loss:0.0021470107163457713\n",
      "train loss:0.006206374562421794\n",
      "train loss:0.0052843097299375245\n",
      "train loss:0.011294092723772263\n",
      "train loss:0.0034624716950192765\n",
      "train loss:0.0013228170000014654\n",
      "train loss:0.008166048771688424\n",
      "train loss:0.0010246375106206702\n",
      "train loss:0.002493885712789652\n",
      "train loss:0.00506557647360066\n",
      "train loss:0.004453417091554227\n",
      "train loss:0.013715858183610959\n",
      "train loss:0.005546102058968172\n",
      "train loss:0.0008919033117396445\n",
      "train loss:0.000750083022124123\n",
      "train loss:0.0018064099785943136\n",
      "train loss:0.0006915004239236554\n",
      "train loss:0.0013692555396340835\n",
      "train loss:0.008552049820605267\n",
      "train loss:0.0007537527451984685\n",
      "train loss:0.004571539431702595\n",
      "train loss:0.0007855074154669637\n",
      "train loss:0.00023064930876885354\n",
      "train loss:0.00039527495927452785\n",
      "train loss:0.0003696210797639911\n",
      "train loss:0.007556826169351387\n",
      "train loss:0.005897084697583854\n",
      "train loss:0.0006667493055944567\n",
      "train loss:0.033178636921090715\n",
      "train loss:0.0008061836772572447\n",
      "train loss:0.0038991518593419518\n",
      "train loss:0.003627067217435896\n",
      "train loss:0.004277703823084871\n",
      "train loss:0.005502012852063538\n",
      "train loss:0.001713448130107257\n",
      "train loss:0.004186639188730795\n",
      "train loss:0.000253591063170094\n",
      "train loss:0.0005057804173937984\n",
      "train loss:0.00846790577053758\n",
      "train loss:0.03006503675361312\n",
      "train loss:0.0014995189463108283\n",
      "train loss:0.0014598507145006689\n",
      "train loss:0.0011310117815574788\n",
      "train loss:0.004410214934179317\n",
      "train loss:0.0068085990558501\n",
      "train loss:0.003259492236427222\n",
      "train loss:0.0039828569493134845\n",
      "train loss:0.001404366903245084\n",
      "train loss:0.029702365793963573\n",
      "train loss:0.001960260607642553\n",
      "train loss:0.0006954855865216618\n",
      "train loss:0.00276383336457865\n",
      "train loss:0.0007713029067776267\n",
      "train loss:0.0007381850480973537\n",
      "train loss:0.00197098431941657\n",
      "train loss:0.0031519531831883055\n",
      "train loss:0.0039304670824837\n",
      "train loss:0.0019864820769811627\n",
      "train loss:0.004600351761128744\n",
      "train loss:0.005908468966673438\n",
      "train loss:0.0016226908381462035\n",
      "train loss:0.0023636361572067894\n",
      "train loss:0.041314621544027076\n",
      "train loss:0.004752674235062587\n",
      "train loss:0.001116866384943442\n",
      "train loss:0.006763936959072693\n",
      "train loss:0.0007365691099006332\n",
      "train loss:0.0008348467944668045\n",
      "train loss:0.0009297160874011752\n",
      "train loss:0.009015518283282705\n",
      "train loss:0.0022777328644351265\n",
      "train loss:0.001651659193804821\n",
      "train loss:0.07314836271668453\n",
      "train loss:0.009477596512586449\n",
      "train loss:0.0005082637001033417\n",
      "train loss:0.005852808705847666\n",
      "train loss:0.0005459209541195982\n",
      "train loss:0.006611521816762434\n",
      "train loss:0.0011404635535618823\n",
      "train loss:0.0007947276311024198\n",
      "train loss:0.001560369820566656\n",
      "train loss:0.0032981761505024767\n",
      "train loss:0.004119461806086049\n",
      "train loss:0.001395560914634083\n",
      "train loss:7.824230214968293e-05\n",
      "train loss:0.0006055289431874252\n",
      "train loss:0.008437437540277115\n",
      "train loss:0.049557396388555025\n",
      "train loss:0.005431523887760684\n",
      "train loss:0.002697440792923765\n",
      "train loss:0.006181026234577971\n",
      "train loss:0.019948633931766754\n",
      "train loss:0.001677061126598752\n",
      "train loss:0.0059224646439911145\n",
      "train loss:0.0025000197206006447\n",
      "train loss:0.0017248805061896339\n",
      "train loss:0.003747841471796475\n",
      "train loss:0.002310455920282969\n",
      "train loss:0.004891151162418996\n",
      "train loss:0.005359901510102579\n",
      "train loss:0.0061162365737556165\n",
      "train loss:0.002890001080588872\n",
      "train loss:0.0016300665546868692\n",
      "train loss:0.007481843593952054\n",
      "train loss:0.0015217243835056488\n",
      "train loss:0.005606290602185695\n",
      "train loss:0.0014446778752700699\n",
      "train loss:0.0026635026715906046\n",
      "train loss:0.0018186371248815902\n",
      "train loss:0.0031647819631161395\n",
      "train loss:0.0006211971789486729\n",
      "train loss:0.03319163802049944\n",
      "train loss:0.0011690724871859035\n",
      "train loss:0.0008606302713909061\n",
      "train loss:0.000551199218995322\n",
      "train loss:0.00666895103443503\n",
      "train loss:0.0007003279432463894\n",
      "train loss:0.002661722956268453\n",
      "train loss:0.008218675285224375\n",
      "train loss:0.002804460263623059\n",
      "train loss:0.08538753996387667\n",
      "train loss:0.0012962523371718901\n",
      "train loss:0.003467177432215228\n",
      "train loss:0.0006153116132476037\n",
      "train loss:0.0009441592572703026\n",
      "train loss:0.005217472675732299\n",
      "train loss:0.006884817094687341\n",
      "train loss:0.007254427447624483\n",
      "train loss:0.00017735705127473349\n",
      "train loss:0.01312198258559226\n",
      "train loss:0.005688534762816673\n",
      "train loss:0.0007519875787952931\n",
      "train loss:0.004127717629997166\n",
      "train loss:0.0009829416662478717\n",
      "train loss:0.0030488938990310627\n",
      "train loss:0.0027282142970500934\n",
      "train loss:0.0005674908939213885\n",
      "train loss:0.0034993195290199995\n",
      "train loss:0.0027060292275288365\n",
      "train loss:0.0026388972730058287\n",
      "train loss:0.006571084683829735\n",
      "train loss:0.009543051088844779\n",
      "train loss:0.0018295558993266898\n",
      "train loss:0.0032566236569243887\n",
      "train loss:0.0005948486823666026\n",
      "train loss:0.0005807322545602286\n",
      "train loss:0.001551775926403042\n",
      "train loss:0.0050937763374385246\n",
      "train loss:0.0006427898282016167\n",
      "train loss:0.0003232643904316545\n",
      "train loss:0.0019927796738382257\n",
      "train loss:0.0005068952956663482\n",
      "train loss:0.0002904368456832861\n",
      "train loss:0.0013042785995432337\n",
      "train loss:0.009346091643834806\n",
      "train loss:0.00229963553622747\n",
      "train loss:0.0030763105624881893\n",
      "train loss:0.004605517506891743\n",
      "train loss:0.003720979541654733\n",
      "train loss:0.0009982152206873694\n",
      "train loss:0.0013171673126717307\n",
      "train loss:0.002474134493303121\n",
      "train loss:0.050509327813274246\n",
      "train loss:0.0014794845724919081\n",
      "train loss:0.0018758928305367538\n",
      "train loss:0.010474948627138471\n",
      "train loss:0.00351328333039109\n",
      "train loss:0.0010851588307270485\n",
      "train loss:0.002790872780848517\n",
      "train loss:0.0006685927454190896\n",
      "train loss:0.002686599784273093\n",
      "train loss:0.012116787595785584\n",
      "train loss:0.0007855033924577993\n",
      "train loss:0.00610732361201721\n",
      "train loss:0.0009675932398109269\n",
      "train loss:0.006137840549347944\n",
      "train loss:0.0024537624623887735\n",
      "train loss:0.008038088983340679\n",
      "train loss:0.006595990268550572\n",
      "train loss:0.0009191691642770003\n",
      "train loss:0.006048777503178698\n",
      "train loss:0.0019023433220326862\n",
      "train loss:0.007056371913735197\n",
      "train loss:0.003989439837038415\n",
      "train loss:0.019399377714186837\n",
      "train loss:0.006051029761949984\n",
      "train loss:0.021613201836077595\n",
      "train loss:0.00510222611687507\n",
      "train loss:0.005889289083197725\n",
      "train loss:0.0025623831643656093\n",
      "train loss:0.0016459640487683253\n",
      "train loss:0.0037519458244653454\n",
      "train loss:0.004290935615439373\n",
      "train loss:0.0011776518091950552\n",
      "train loss:0.0018572721692477288\n",
      "train loss:0.005650995155911154\n",
      "train loss:0.0023691442634102243\n",
      "train loss:0.0010017033853830782\n",
      "train loss:0.01732383233822603\n",
      "train loss:0.0016729515144708687\n",
      "train loss:0.005311586544369859\n",
      "train loss:0.006022960983859259\n",
      "train loss:0.015813923787090713\n",
      "train loss:0.005523405995553815\n",
      "train loss:0.004653304028797942\n",
      "train loss:0.006840015777110308\n",
      "train loss:0.0012150395207110526\n",
      "train loss:0.010785048824302167\n",
      "train loss:0.004777465861616545\n",
      "train loss:0.002692913622909291\n",
      "train loss:0.000617333058591682\n",
      "train loss:0.005477762858675465\n",
      "train loss:0.0025894515363122993\n",
      "train loss:0.0003577416893443284\n",
      "train loss:0.004472763974947982\n",
      "train loss:0.016901431807816863\n",
      "train loss:0.0076690300841590585\n",
      "train loss:0.011921859165225488\n",
      "train loss:0.005981821592936136\n",
      "train loss:0.0036118276282385868\n",
      "train loss:0.006217678693778713\n",
      "train loss:0.002787040853112948\n",
      "train loss:0.001947957731719536\n",
      "train loss:0.04198717933034489\n",
      "train loss:0.007198172234195292\n",
      "train loss:0.003966818386531686\n",
      "train loss:0.0003311867788268865\n",
      "train loss:0.01777419544710074\n",
      "train loss:0.0003683914759378371\n",
      "train loss:0.0025014317331886566\n",
      "train loss:0.00651744595982728\n",
      "train loss:0.008456415840807682\n",
      "train loss:0.007436552081181103\n",
      "train loss:0.00044884714773153045\n",
      "train loss:0.004549156871876575\n",
      "train loss:0.0006879814876220907\n",
      "train loss:0.007613358318706884\n",
      "train loss:0.010353761372831727\n",
      "train loss:0.004546017067177295\n",
      "train loss:0.0009689598744412915\n",
      "train loss:0.0009234016736665057\n",
      "train loss:0.0012893876376117263\n",
      "train loss:0.03069051122753745\n",
      "train loss:0.004477275117215568\n",
      "train loss:0.005096553338794895\n",
      "train loss:0.00028614637210580844\n",
      "train loss:0.0032020535860932133\n",
      "train loss:0.013356182721934142\n",
      "train loss:0.0016922505696423983\n",
      "train loss:0.0010740517001633025\n",
      "train loss:0.0012455389892504884\n",
      "train loss:0.004239517246123457\n",
      "train loss:0.00318820592912709\n",
      "train loss:0.0013900253214974903\n",
      "train loss:0.001364108121103871\n",
      "train loss:0.002634067801348165\n",
      "train loss:0.00456885276772928\n",
      "train loss:0.0021753321186204076\n",
      "train loss:0.0007208777513968012\n",
      "train loss:0.0007093151471580781\n",
      "train loss:0.0030934602440171525\n",
      "train loss:0.0048433197441633165\n",
      "train loss:0.0077393138136313214\n",
      "train loss:0.0001781520838341633\n",
      "train loss:0.00011293225769119852\n",
      "train loss:0.0034232593997382934\n",
      "train loss:0.00024532049288179103\n",
      "train loss:0.0024953380782009858\n",
      "train loss:0.003976096644750699\n",
      "train loss:0.0007853664725026057\n",
      "train loss:0.005434514167947546\n",
      "train loss:0.0004259545343292666\n",
      "train loss:0.0012498244043855624\n",
      "train loss:0.004171711514203504\n",
      "train loss:0.0012009596630413709\n",
      "train loss:0.002981405419495384\n",
      "train loss:0.02640152551455645\n",
      "train loss:0.0012769316912433271\n",
      "train loss:0.0035003798231425685\n",
      "train loss:0.01316000406808413\n",
      "train loss:0.00274613137230635\n",
      "train loss:0.0073033911103629785\n",
      "train loss:0.007649255100336271\n",
      "train loss:0.001896793768402049\n",
      "train loss:0.00358533289124604\n",
      "train loss:0.0010826488958474282\n",
      "train loss:0.008624948880066995\n",
      "train loss:0.00023488508636835035\n",
      "train loss:0.00480581457790413\n",
      "train loss:0.0015827773025366546\n",
      "train loss:0.004329450139368628\n",
      "train loss:0.03348083872898169\n",
      "train loss:0.0016348384414813007\n",
      "train loss:0.0012808623184766524\n",
      "train loss:0.0019911972710744395\n",
      "train loss:0.003858971060645861\n",
      "train loss:0.0128436712473844\n",
      "train loss:0.0033084038489975716\n",
      "train loss:0.0021669822826305913\n",
      "train loss:0.0022314334932649884\n",
      "train loss:0.004784750662807971\n",
      "train loss:0.0023026489487272777\n",
      "train loss:0.001124426996313308\n",
      "train loss:0.0008410810202120623\n",
      "train loss:0.003957283544813735\n",
      "train loss:0.004556758779048185\n",
      "train loss:0.002541179476612643\n",
      "train loss:0.004214589318469114\n",
      "train loss:0.0005080622223138657\n",
      "train loss:0.0017677898053431648\n",
      "train loss:0.002085388913635255\n",
      "train loss:0.012039309442177307\n",
      "train loss:0.0016891271331157835\n",
      "train loss:0.0014791685443723676\n",
      "train loss:0.0021554911205391197\n",
      "train loss:0.0004640934133227991\n",
      "train loss:0.0053700037707207285\n",
      "train loss:0.0036061574951947815\n",
      "train loss:0.0012957673583834316\n",
      "train loss:0.003803840024290339\n",
      "train loss:0.0027511298600537116\n",
      "train loss:0.0047793743025439425\n",
      "train loss:0.013443239771758526\n",
      "train loss:0.00519062786785628\n",
      "train loss:0.004246823919890317\n",
      "train loss:0.0008206501089752713\n",
      "train loss:0.001664058986677758\n",
      "train loss:0.00010313268672667927\n",
      "train loss:0.007880558274713307\n",
      "train loss:0.0019556563665825643\n",
      "train loss:0.0013895038302660495\n",
      "train loss:0.0022907160106493906\n",
      "train loss:0.0061457226841964375\n",
      "train loss:0.012459190783978282\n",
      "train loss:0.0017265255828570805\n",
      "train loss:0.0058704234594484025\n",
      "train loss:0.0006689324010351078\n",
      "train loss:0.004651697051914471\n",
      "train loss:0.003136694381132673\n",
      "train loss:0.0015642256908290839\n",
      "train loss:0.0035369638421105026\n",
      "train loss:0.0012406939076159058\n",
      "train loss:0.00131375633935691\n",
      "train loss:0.0002366195590844344\n",
      "train loss:0.0025007462988840294\n",
      "train loss:0.0032689738836419475\n",
      "train loss:0.014960905993340934\n",
      "train loss:0.015406750829075184\n",
      "train loss:0.00042652460055892536\n",
      "train loss:0.000186311127195351\n",
      "train loss:0.0016339974594691451\n",
      "train loss:0.0011292647246086565\n",
      "train loss:0.0029095841004107108\n",
      "train loss:0.011708198141792619\n",
      "train loss:0.017191146304733347\n",
      "train loss:0.0037533711945396224\n",
      "train loss:0.005180441326008027\n",
      "train loss:0.0006632648391037084\n",
      "train loss:0.001026138395794879\n",
      "train loss:0.0002740339046813962\n",
      "=== epoch:13, train acc:0.994, test acc:0.987 ===\n",
      "train loss:0.016983099780345158\n",
      "train loss:0.006497491067929376\n",
      "train loss:0.0025809584793302\n",
      "train loss:0.004918703021202139\n",
      "train loss:0.011020968620888812\n",
      "train loss:0.005866571065804853\n",
      "train loss:0.0013486232417394273\n",
      "train loss:0.006048751929042992\n",
      "train loss:0.005373480380136884\n",
      "train loss:0.011116441393725822\n",
      "train loss:0.0010687060134924797\n",
      "train loss:0.0006220137231419259\n",
      "train loss:0.0009066249705878356\n",
      "train loss:0.002781522707696271\n",
      "train loss:0.003794420936823055\n",
      "train loss:0.0006059867384048418\n",
      "train loss:0.0017556232663417251\n",
      "train loss:0.013702795008924067\n",
      "train loss:0.00128136705769009\n",
      "train loss:0.04197146271290122\n",
      "train loss:0.0041814918595005635\n",
      "train loss:0.005574969669566942\n",
      "train loss:0.00014791936671119474\n",
      "train loss:0.002635920107458126\n",
      "train loss:0.006153066063283236\n",
      "train loss:0.0004281469013062479\n",
      "train loss:0.004302376536739185\n",
      "train loss:0.00024081690764991392\n",
      "train loss:0.00891569267277207\n",
      "train loss:0.007492188303119818\n",
      "train loss:0.011513074220822771\n",
      "train loss:0.010491470809191333\n",
      "train loss:0.009414095115616332\n",
      "train loss:0.0016560263346347365\n",
      "train loss:0.007256492471452668\n",
      "train loss:0.00036358104143360544\n",
      "train loss:0.0033449081478521985\n",
      "train loss:0.006686429692014241\n",
      "train loss:0.003149343574652953\n",
      "train loss:0.018818427359165047\n",
      "train loss:0.0011151653324453677\n",
      "train loss:0.0015044983285919648\n",
      "train loss:0.005190340401253483\n",
      "train loss:0.0009754305414945377\n",
      "train loss:0.004516675612731198\n",
      "train loss:0.0013716165786447614\n",
      "train loss:0.0013499117878978584\n",
      "train loss:0.011562947929104807\n",
      "train loss:0.0011734912768627621\n",
      "train loss:0.004464585424244511\n",
      "train loss:0.01880109657932871\n",
      "train loss:0.001049567848866467\n",
      "train loss:0.0034008188407332767\n",
      "train loss:0.0004318466739059213\n",
      "train loss:0.01090351412729056\n",
      "train loss:0.0004952328260560681\n",
      "train loss:0.004286316674172957\n",
      "train loss:0.008233965078506593\n",
      "train loss:0.00021186723042548522\n",
      "train loss:0.005738861768470289\n",
      "train loss:0.010333867265398169\n",
      "train loss:0.0036765375055722114\n",
      "train loss:0.010337258060111598\n",
      "train loss:0.003563802712888191\n",
      "train loss:0.0028841887890659634\n",
      "train loss:0.0018634718955624716\n",
      "train loss:0.0026185047796014583\n",
      "train loss:0.004463411229906629\n",
      "train loss:0.015292578624384339\n",
      "train loss:0.005132738850401663\n",
      "train loss:0.002227516942114626\n",
      "train loss:0.00038094073677818525\n",
      "train loss:0.0010076164638324308\n",
      "train loss:0.004193463760971892\n",
      "train loss:0.002703914606400521\n",
      "train loss:0.004996624093230142\n",
      "train loss:0.0016535854063621893\n",
      "train loss:0.0005198670474967378\n",
      "train loss:0.0008762945120822751\n",
      "train loss:0.009099240257317273\n",
      "train loss:0.000397808570820372\n",
      "train loss:0.00981269356851566\n",
      "train loss:0.0012708163099930845\n",
      "train loss:0.0032840470402508283\n",
      "train loss:0.0017083709416220924\n",
      "train loss:0.018856499516306167\n",
      "train loss:0.00023414337307518137\n",
      "train loss:0.0035501532244725255\n",
      "train loss:0.0014210564072160805\n",
      "train loss:0.004822507729776601\n",
      "train loss:0.001912652507205705\n",
      "train loss:0.009987690711452463\n",
      "train loss:0.005796308481169698\n",
      "train loss:0.002787778630486284\n",
      "train loss:0.00033627139179224954\n",
      "train loss:0.006350428301984191\n",
      "train loss:0.007382163629971271\n",
      "train loss:0.004710087659501216\n",
      "train loss:0.00027311439432578885\n",
      "train loss:0.0005043279566549997\n",
      "train loss:0.00043768893036232016\n",
      "train loss:0.02064815037111818\n",
      "train loss:0.001025607332104357\n",
      "train loss:0.00012938507703640648\n",
      "train loss:0.002455516607723383\n",
      "train loss:0.0008456155397394813\n",
      "train loss:0.0020728808262431074\n",
      "train loss:0.0011507337204045291\n",
      "train loss:0.00955642550371767\n",
      "train loss:0.00579409238908865\n",
      "train loss:0.0010466314812306477\n",
      "train loss:0.0014740367104229915\n",
      "train loss:0.0004365916265028851\n",
      "train loss:0.0010654454455536252\n",
      "train loss:0.006372790838461417\n",
      "train loss:0.00565264359886699\n",
      "train loss:0.03646731963326707\n",
      "train loss:0.0042462479343101265\n",
      "train loss:0.0027773062003421194\n",
      "train loss:0.0022257330272771056\n",
      "train loss:0.0031522722737579757\n",
      "train loss:0.0020751026019809275\n",
      "train loss:0.0013890099605037348\n",
      "train loss:0.0017606560563526786\n",
      "train loss:0.00626253180262916\n",
      "train loss:0.0013753012251237148\n",
      "train loss:0.0050498279566954605\n",
      "train loss:0.0020352792444523916\n",
      "train loss:0.010708126562260296\n",
      "train loss:0.0007676034624856426\n",
      "train loss:0.0015545414238555444\n",
      "train loss:0.0024499808128108404\n",
      "train loss:0.00045382753231101907\n",
      "train loss:0.0035551388372882325\n",
      "train loss:0.0072253949285456445\n",
      "train loss:0.0029405559506604745\n",
      "train loss:0.0019952151182100293\n",
      "train loss:0.004077335662070898\n",
      "train loss:0.002435122940487516\n",
      "train loss:0.006712814518057096\n",
      "train loss:0.0009203277745223665\n",
      "train loss:0.003986260800100707\n",
      "train loss:0.0006607220926681172\n",
      "train loss:0.0021401430568528785\n",
      "train loss:0.003051628283119978\n",
      "train loss:0.002796150627580366\n",
      "train loss:0.0015162408557982498\n",
      "train loss:0.006333467369586427\n",
      "train loss:0.023853078641724812\n",
      "train loss:0.000861403191208155\n",
      "train loss:0.0037405958555357176\n",
      "train loss:0.0023671867633295114\n",
      "train loss:0.0010788641129010764\n",
      "train loss:0.0002676315631891699\n",
      "train loss:0.0006672048070461485\n",
      "train loss:0.005368120194128393\n",
      "train loss:0.012437973586052187\n",
      "train loss:0.007399954519592247\n",
      "train loss:0.0010279107948361268\n",
      "train loss:0.0004785247430531292\n",
      "train loss:0.000669164928955044\n",
      "train loss:0.0007171259577877118\n",
      "train loss:0.00129474006954237\n",
      "train loss:0.0029634677371200052\n",
      "train loss:0.003573072121599885\n",
      "train loss:0.0029820466432868313\n",
      "train loss:0.004804305798276594\n",
      "train loss:0.0009798921798018555\n",
      "train loss:0.00036073417852918165\n",
      "train loss:0.0006963363760967056\n",
      "train loss:0.0025489435765045886\n",
      "train loss:0.0034462933099601374\n",
      "train loss:0.002057281146280278\n",
      "train loss:0.0005126797746028664\n",
      "train loss:0.0014396723238220832\n",
      "train loss:0.0001660197513161178\n",
      "train loss:0.006602734556872428\n",
      "train loss:0.011480392839986309\n",
      "train loss:0.00221377703755138\n",
      "train loss:0.0003784731556477595\n",
      "train loss:0.00020531029884307897\n",
      "train loss:0.002138982913608484\n",
      "train loss:0.010679265884680121\n",
      "train loss:0.0030808786062619066\n",
      "train loss:0.002816520709445533\n",
      "train loss:0.0037708071541988023\n",
      "train loss:0.008968303176055563\n",
      "train loss:0.002455199511471994\n",
      "train loss:0.003528381416003433\n",
      "train loss:0.0035751119542759054\n",
      "train loss:0.0018221063861229864\n",
      "train loss:0.014706947392741643\n",
      "train loss:0.0011363254986661785\n",
      "train loss:0.0008515105267252029\n",
      "train loss:0.000845433901393862\n",
      "train loss:0.0036783838850580506\n",
      "train loss:0.0010252668545257147\n",
      "train loss:0.04235781390757461\n",
      "train loss:0.0026647554006932694\n",
      "train loss:0.0021631919308138883\n",
      "train loss:0.0018120310723905581\n",
      "train loss:0.00043773239572719383\n",
      "train loss:0.0005761202763343894\n",
      "train loss:0.0031460750872262964\n",
      "train loss:0.00324734575493895\n",
      "train loss:0.0006018006877538768\n",
      "train loss:0.003976645960869149\n",
      "train loss:0.0010196064396989655\n",
      "train loss:0.0018715362660918402\n",
      "train loss:0.0021291894073040447\n",
      "train loss:0.004432357294319074\n",
      "train loss:0.0024257091883725483\n",
      "train loss:0.0029045570310588147\n",
      "train loss:0.0026175662807854717\n",
      "train loss:0.009319214633550192\n",
      "train loss:0.011201405185190723\n",
      "train loss:0.004727035146900781\n",
      "train loss:0.0048253577622571105\n",
      "train loss:0.013571012611951204\n",
      "train loss:0.0029284791603168664\n",
      "train loss:0.0011809045689034426\n",
      "train loss:0.0019708299376667167\n",
      "train loss:0.00669812457515128\n",
      "train loss:0.0008926858772743103\n",
      "train loss:0.0004827137081644144\n",
      "train loss:0.007969140438183704\n",
      "train loss:0.004529466791374786\n",
      "train loss:0.007131711600884774\n",
      "train loss:0.003044831253303953\n",
      "train loss:0.01114308260418771\n",
      "train loss:0.009244028780710432\n",
      "train loss:0.03718690260656225\n",
      "train loss:0.004874612271352047\n",
      "train loss:0.0050256458316892075\n",
      "train loss:0.003714373461163898\n",
      "train loss:0.0010083762142648366\n",
      "train loss:0.0053871572481879616\n",
      "train loss:0.006208327822778564\n",
      "train loss:0.006934226917225947\n",
      "train loss:0.00546323938814981\n",
      "train loss:0.0011291412591051993\n",
      "train loss:0.010771808462216504\n",
      "train loss:0.000252685288490291\n",
      "train loss:0.0038819149817037528\n",
      "train loss:0.025643545366867002\n",
      "train loss:0.003451025212792675\n",
      "train loss:0.005999783229204845\n",
      "train loss:0.0045345215754136105\n",
      "train loss:0.0006660834816522145\n",
      "train loss:0.005141940370111587\n",
      "train loss:0.0005819594577540286\n",
      "train loss:0.0003203644162976116\n",
      "train loss:0.0012837198953183481\n",
      "train loss:0.0008127127664911347\n",
      "train loss:0.0006651900531996541\n",
      "train loss:0.0030472118539442193\n",
      "train loss:0.0034527530909428837\n",
      "train loss:0.00026560890483297707\n",
      "train loss:0.0032253596007521352\n",
      "train loss:0.0037568993718977996\n",
      "train loss:0.00016574513750391585\n",
      "train loss:0.004390878606237958\n",
      "train loss:0.003440247691970599\n",
      "train loss:0.014197058966281744\n",
      "train loss:0.0011631127908323085\n",
      "train loss:0.0034945695025671826\n",
      "train loss:0.0004886682585524118\n",
      "train loss:0.00313537095775663\n",
      "train loss:0.0040749203690213795\n",
      "train loss:0.003210204690133668\n",
      "train loss:0.0006335010740217946\n",
      "train loss:0.005257745895113221\n",
      "train loss:0.001167611251245758\n",
      "train loss:0.0007159866381709795\n",
      "train loss:0.0008165049395688015\n",
      "train loss:0.0007846150390538899\n",
      "train loss:0.002158628051613418\n",
      "train loss:0.0013666597593243247\n",
      "train loss:0.0004784263911716646\n",
      "train loss:0.004431144888061181\n",
      "train loss:0.0076286115724973095\n",
      "train loss:0.008985497938221279\n",
      "train loss:0.00033780286323158485\n",
      "train loss:0.00019459574999366555\n",
      "train loss:0.0004318471789819498\n",
      "train loss:0.0008024379271068993\n",
      "train loss:0.006914508876745312\n",
      "train loss:0.0015171793823357802\n",
      "train loss:0.011907590482275531\n",
      "train loss:0.01560726490096205\n",
      "train loss:0.00223061111766761\n",
      "train loss:0.00552890059439447\n",
      "train loss:0.007945331341813132\n",
      "train loss:0.0014127761663230826\n",
      "train loss:0.001557913647286064\n",
      "train loss:0.0015630867642490245\n",
      "train loss:0.0009321522075599432\n",
      "train loss:0.02109228572640474\n",
      "train loss:0.0049705535940157645\n",
      "train loss:0.0003703181424053493\n",
      "train loss:0.0023390096368894804\n",
      "train loss:0.009383342110989\n",
      "train loss:0.0030309088994059518\n",
      "train loss:0.003960052449616836\n",
      "train loss:0.0007162851406619956\n",
      "train loss:0.002329117845876576\n",
      "train loss:0.004833056458232485\n",
      "train loss:0.0020800664588435354\n",
      "train loss:0.011401673860925854\n",
      "train loss:0.0021457782003436662\n",
      "train loss:0.0050743570678093\n",
      "train loss:0.00252204139043748\n",
      "train loss:0.00046792456418230855\n",
      "train loss:0.00557477710597761\n",
      "train loss:0.0022199102354815937\n",
      "train loss:0.022581162730819115\n",
      "train loss:0.0026632267548844448\n",
      "train loss:0.0007778803448569462\n",
      "train loss:0.000853811626833307\n",
      "train loss:0.04948815205086092\n",
      "train loss:0.0018579096685641017\n",
      "train loss:0.0030489001295396444\n",
      "train loss:0.0002390421876307092\n",
      "train loss:9.875864425823866e-05\n",
      "train loss:0.0005442942419757803\n",
      "train loss:0.01250218621199843\n",
      "train loss:0.007557774898416069\n",
      "train loss:0.00256206289845209\n",
      "train loss:0.006996219146460938\n",
      "train loss:0.006679608230260924\n",
      "train loss:0.006869991661714555\n",
      "train loss:0.005307479165136118\n",
      "train loss:0.002726365245535681\n",
      "train loss:0.004276574691166579\n",
      "train loss:0.00014113286207667462\n",
      "train loss:0.0018313532085561675\n",
      "train loss:0.005291810157834705\n",
      "train loss:0.001099624627835679\n",
      "train loss:0.0010666500011955854\n",
      "train loss:0.003338895071293267\n",
      "train loss:0.002367677500313646\n",
      "train loss:0.003561499422336649\n",
      "train loss:0.000480016768489101\n",
      "train loss:0.0021063742780187585\n",
      "train loss:0.0031936885514455924\n",
      "train loss:0.00025009668632818173\n",
      "train loss:0.0006124647938590315\n",
      "train loss:0.0010225184072790773\n",
      "train loss:0.001560820681570708\n",
      "train loss:0.0009805937376050357\n",
      "train loss:0.010469411892284442\n",
      "train loss:0.0025830329021111127\n",
      "train loss:0.0005613967874806151\n",
      "train loss:0.014778217637069041\n",
      "train loss:0.003567916426616604\n",
      "train loss:0.0005042013826895384\n",
      "train loss:0.002956888598757515\n",
      "train loss:0.014940431586650432\n",
      "train loss:0.0012767927023942102\n",
      "train loss:0.0005248099177019893\n",
      "train loss:0.0027918528346793808\n",
      "train loss:0.0027860095066609566\n",
      "train loss:0.000182548608345356\n",
      "train loss:0.001627794217944118\n",
      "train loss:0.004090153956590424\n",
      "train loss:0.0031920616333404634\n",
      "train loss:0.0006198248369642316\n",
      "train loss:0.001679859454203294\n",
      "train loss:0.00473699943996355\n",
      "train loss:0.0016861517689972876\n",
      "train loss:0.0011214781535516004\n",
      "train loss:0.00041540199348694324\n",
      "train loss:0.0004247146483274285\n",
      "train loss:0.006066358455887956\n",
      "train loss:0.0055336126682165275\n",
      "train loss:0.001743348507498554\n",
      "train loss:0.001228508343630394\n",
      "train loss:0.0037493276809011343\n",
      "train loss:0.005420896764921599\n",
      "train loss:0.0012069574838476974\n",
      "train loss:0.005960133205813554\n",
      "train loss:0.0008217631247077442\n",
      "train loss:0.0015027388381188634\n",
      "train loss:0.0019997586183468545\n",
      "train loss:0.0039576931637496735\n",
      "train loss:0.0033872401597468617\n",
      "train loss:0.0038230975301963917\n",
      "train loss:0.005051887273822196\n",
      "train loss:0.008023204879346237\n",
      "train loss:0.007140057216160713\n",
      "train loss:0.008343498928606346\n",
      "train loss:0.006011340467888816\n",
      "train loss:0.0020372340426527222\n",
      "train loss:0.0009697038628399559\n",
      "train loss:0.003242630611339945\n",
      "train loss:0.0030464316749228486\n",
      "train loss:0.005897919435205675\n",
      "train loss:0.005530936496360359\n",
      "train loss:0.03623911891755845\n",
      "train loss:0.0037748592588983916\n",
      "train loss:0.0022047749877817917\n",
      "train loss:0.0032850770404406625\n",
      "train loss:0.0033218782563154883\n",
      "train loss:0.0018809057245117065\n",
      "train loss:0.004973595571284163\n",
      "train loss:0.012152893914396586\n",
      "train loss:0.01923490844327231\n",
      "train loss:0.0016496630351176158\n",
      "train loss:0.0037305138146589865\n",
      "train loss:0.0007404682984452355\n",
      "train loss:0.00233483492398068\n",
      "train loss:0.0008100677826987908\n",
      "train loss:0.003419516893217958\n",
      "train loss:0.0017096510415320941\n",
      "train loss:0.0027581813454719438\n",
      "train loss:0.0011106520564928763\n",
      "train loss:0.005813866312019916\n",
      "train loss:0.029306486221348113\n",
      "train loss:0.004731073885795665\n",
      "train loss:0.004215083556075624\n",
      "train loss:0.005709677882338842\n",
      "train loss:0.013671110499403576\n",
      "train loss:0.0043914001692415975\n",
      "train loss:0.0012442724740359627\n",
      "train loss:0.005456693451482665\n",
      "train loss:0.0029155830798019543\n",
      "train loss:0.030654748279682377\n",
      "train loss:0.0006170717386096939\n",
      "train loss:0.006679825761944639\n",
      "train loss:0.0002954339992796759\n",
      "train loss:0.0007135083452244412\n",
      "train loss:0.0014651919360172355\n",
      "train loss:0.0038208946584781106\n",
      "train loss:0.00043642159494296386\n",
      "train loss:0.0004898915773677361\n",
      "train loss:0.0014045705220155874\n",
      "train loss:0.006559930527502573\n",
      "train loss:0.0011464403456864837\n",
      "train loss:0.0004662636623042714\n",
      "train loss:0.002903174775086737\n",
      "train loss:0.0009924923703253825\n",
      "train loss:0.010080300885320994\n",
      "train loss:0.004556465137034757\n",
      "train loss:0.005196617217323762\n",
      "train loss:0.0020062287340526447\n",
      "train loss:0.01037005099241459\n",
      "train loss:0.00957280015988627\n",
      "train loss:0.0016505031687395982\n",
      "train loss:0.0008272055850103061\n",
      "train loss:0.0055157751380346355\n",
      "train loss:0.0012011546037734452\n",
      "train loss:0.0021603872492799592\n",
      "train loss:0.027735140737715453\n",
      "train loss:0.002835188945255884\n",
      "train loss:0.0030598335590887042\n",
      "train loss:0.005717599058799012\n",
      "train loss:0.0082772768892534\n",
      "train loss:0.0017628245448817056\n",
      "train loss:0.0005071361330857023\n",
      "train loss:0.002176450047212086\n",
      "train loss:0.0012925452642434296\n",
      "train loss:0.0018426815671281432\n",
      "train loss:0.0008959811584712511\n",
      "train loss:0.010055461396524933\n",
      "train loss:0.009446890250644789\n",
      "train loss:0.0034887973308640272\n",
      "train loss:0.010679665877714285\n",
      "train loss:0.008315733594388378\n",
      "train loss:0.0020643972597849222\n",
      "train loss:0.0044889365953473\n",
      "train loss:0.0006226995122765928\n",
      "train loss:0.002353641738132213\n",
      "train loss:0.005076371699524378\n",
      "train loss:0.001633192014575014\n",
      "train loss:0.005256802378199305\n",
      "train loss:0.008623005013366875\n",
      "train loss:0.007650268477462589\n",
      "train loss:0.0026384506734846054\n",
      "train loss:0.0010551704290198978\n",
      "train loss:0.017656241791013973\n",
      "train loss:0.0014426532049337023\n",
      "train loss:0.004987551482782477\n",
      "train loss:0.007494219676360956\n",
      "train loss:0.0006319208155316829\n",
      "train loss:0.01696082198409336\n",
      "train loss:0.003547585761206181\n",
      "train loss:0.0026426615776256944\n",
      "train loss:0.012284596719117209\n",
      "train loss:0.007538218120261932\n",
      "train loss:0.0018961424258023545\n",
      "train loss:0.0016497542913074634\n",
      "train loss:0.0005435776723207204\n",
      "train loss:0.011463433940870717\n",
      "train loss:0.0029028252971788843\n",
      "train loss:0.0016522778985103978\n",
      "train loss:0.0028304222329688534\n",
      "train loss:0.002043543629437175\n",
      "train loss:0.002711801081471324\n",
      "train loss:0.0001365538965576434\n",
      "train loss:0.001678855205109128\n",
      "train loss:0.0014929937722353798\n",
      "train loss:0.004314078257694475\n",
      "train loss:0.0004889330606730009\n",
      "train loss:0.007836951788249379\n",
      "train loss:0.002336774065687266\n",
      "train loss:0.02178554792683383\n",
      "train loss:0.002961600108050518\n",
      "train loss:0.0012440614250117726\n",
      "train loss:0.0007634903884768277\n",
      "train loss:0.000369774371660302\n",
      "train loss:0.0011565774110705114\n",
      "train loss:0.008109762904137969\n",
      "train loss:0.0019695163188130325\n",
      "train loss:0.0032845359983938115\n",
      "train loss:0.008247133939555014\n",
      "train loss:0.0016458368818030855\n",
      "train loss:0.003960775322165031\n",
      "train loss:0.00304292459137832\n",
      "train loss:0.001003336603592585\n",
      "train loss:0.006640135843387082\n",
      "train loss:0.004005919054828421\n",
      "train loss:0.001939594380118922\n",
      "train loss:0.004330070176667055\n",
      "train loss:0.0002592516523306686\n",
      "train loss:0.0019493621353996189\n",
      "train loss:0.02423751504935495\n",
      "train loss:0.0022221727703766996\n",
      "train loss:0.0011810107718598626\n",
      "train loss:0.00022688766229799943\n",
      "train loss:0.0006134614203369126\n",
      "train loss:0.0017714679113086562\n",
      "train loss:0.0033544873236754907\n",
      "train loss:0.0004208258144787021\n",
      "train loss:0.0008093438930119872\n",
      "train loss:0.00032638178138764127\n",
      "train loss:0.0010477979597995077\n",
      "train loss:0.0019666676669481973\n",
      "train loss:0.0021056750238662328\n",
      "train loss:0.0056197557050008074\n",
      "train loss:0.006526172863392283\n",
      "train loss:0.010576304994743865\n",
      "train loss:0.006880292086273094\n",
      "train loss:0.005753366726244236\n",
      "train loss:0.004245779386420116\n",
      "train loss:0.0008948991081968037\n",
      "train loss:0.0010419824160233248\n",
      "train loss:0.004872270894096784\n",
      "train loss:0.0030566261980885636\n",
      "train loss:0.004989067308521434\n",
      "train loss:0.00014423935630870726\n",
      "train loss:0.002764364611710243\n",
      "train loss:0.0003978241764430808\n",
      "train loss:5.480515455543531e-05\n",
      "train loss:0.0015187083611331574\n",
      "train loss:0.015609952698887824\n",
      "train loss:0.0010724412024772255\n",
      "train loss:0.002510571141518831\n",
      "train loss:0.018843778842767393\n",
      "train loss:0.0035210441084872948\n",
      "train loss:0.0006455959662087267\n",
      "train loss:0.0019282424153609999\n",
      "train loss:0.0027757971791476043\n",
      "train loss:0.0007072062104507982\n",
      "train loss:0.0006672690064099729\n",
      "train loss:0.002030972125170866\n",
      "train loss:0.0020991492374139655\n",
      "train loss:0.004802533558560384\n",
      "train loss:0.0036055472590198934\n",
      "train loss:0.007235759676908319\n",
      "train loss:0.0026926284371331806\n",
      "train loss:0.0021030817051306484\n",
      "train loss:0.0007208559353591183\n",
      "train loss:0.0009483640316706543\n",
      "train loss:0.002751111824639907\n",
      "train loss:0.003046128633962055\n",
      "train loss:0.002583337366203131\n",
      "train loss:0.0004677498578984894\n",
      "train loss:0.0006052656536918751\n",
      "train loss:0.003801037986907243\n",
      "train loss:0.005302373556546041\n",
      "train loss:0.0022775298043791374\n",
      "train loss:0.0017029148720519823\n",
      "train loss:0.0010006191234163571\n",
      "train loss:0.0012591492613300184\n",
      "train loss:0.031385820979625854\n",
      "train loss:0.003364177579555672\n",
      "train loss:0.001991389417755321\n",
      "train loss:0.01052300566063999\n",
      "train loss:0.0006602195079604814\n",
      "train loss:0.013231673873663828\n",
      "train loss:0.013262158983858461\n",
      "train loss:0.010894908705590036\n",
      "train loss:0.0054131615812689615\n",
      "train loss:0.0004787694968944333\n",
      "train loss:0.0010514404998406104\n",
      "train loss:0.0035567312778548737\n",
      "train loss:0.00041733846349812373\n",
      "train loss:0.005898077455275566\n",
      "train loss:0.00037007687447964364\n",
      "train loss:0.0008391012506580115\n",
      "=== epoch:14, train acc:0.995, test acc:0.986 ===\n",
      "train loss:0.002855933853604968\n",
      "train loss:0.005199782520216455\n",
      "train loss:0.002391542407232554\n",
      "train loss:0.0031799487864805115\n",
      "train loss:0.007145962572088379\n",
      "train loss:0.0011332043395238134\n",
      "train loss:0.0010069534416358133\n",
      "train loss:0.005009892622269804\n",
      "train loss:0.00393868131477066\n",
      "train loss:0.0074080987301123995\n",
      "train loss:0.0018793803303677034\n",
      "train loss:0.0022832456030353414\n",
      "train loss:0.021795398345074938\n",
      "train loss:0.0029446112283034965\n",
      "train loss:0.00036728906559515396\n",
      "train loss:0.0019373842707456632\n",
      "train loss:0.003767190393064236\n",
      "train loss:0.0012494245167144116\n",
      "train loss:0.006564370616176283\n",
      "train loss:0.0012434546135331266\n",
      "train loss:0.009184532709856747\n",
      "train loss:0.010753970503316692\n",
      "train loss:0.0037091563693173444\n",
      "train loss:0.003464140289224752\n",
      "train loss:0.001020803304338004\n",
      "train loss:0.0021984032911521864\n",
      "train loss:0.0014973197478968811\n",
      "train loss:0.002487802762451927\n",
      "train loss:0.004380701884604604\n",
      "train loss:0.0008259668117967714\n",
      "train loss:0.01770925303940439\n",
      "train loss:0.00463712450155979\n",
      "train loss:0.00021598704828741815\n",
      "train loss:0.004599114198041707\n",
      "train loss:0.01242700353561062\n",
      "train loss:0.0056840063745918976\n",
      "train loss:0.001203004253990629\n",
      "train loss:0.00020222560509690002\n",
      "train loss:0.0011073175952350876\n",
      "train loss:0.00044503758806250024\n",
      "train loss:0.0020817574747959574\n",
      "train loss:0.0022601741252767605\n",
      "train loss:0.0017212444550753839\n",
      "train loss:0.001884185048554432\n",
      "train loss:0.005621296475445003\n",
      "train loss:0.00021927025497076467\n",
      "train loss:0.0010648482826497174\n",
      "train loss:0.0007560690685407636\n",
      "train loss:0.00165213464931526\n",
      "train loss:0.0013930345402721118\n",
      "train loss:0.0004940675046841474\n",
      "train loss:0.0014260919954405095\n",
      "train loss:0.004232726087314873\n",
      "train loss:0.00935165747023106\n",
      "train loss:0.002818205465629999\n",
      "train loss:0.0004710995354478513\n",
      "train loss:0.003323469648289002\n",
      "train loss:0.00225475191607817\n",
      "train loss:0.002404461452770927\n",
      "train loss:0.008455246086922087\n",
      "train loss:0.0013898723502349913\n",
      "train loss:0.0009530508762055411\n",
      "train loss:0.007254588743306676\n",
      "train loss:0.004051833673184079\n",
      "train loss:0.021367844657926717\n",
      "train loss:0.0023858715600337836\n",
      "train loss:0.003916733161471762\n",
      "train loss:0.0005217680854554046\n",
      "train loss:0.0007465541622253715\n",
      "train loss:0.0004159924195304787\n",
      "train loss:0.005089041024835563\n",
      "train loss:0.006935168951043816\n",
      "train loss:0.0032137322893548387\n",
      "train loss:0.0017935569707499689\n",
      "train loss:0.0005846113333101273\n",
      "train loss:0.0011498089068671727\n",
      "train loss:0.003582243470456032\n",
      "train loss:0.0013394531226385525\n",
      "train loss:0.001300068341914304\n",
      "train loss:0.001405076077913904\n",
      "train loss:0.00282917262253097\n",
      "train loss:0.004393134957891125\n",
      "train loss:0.0027938004943285615\n",
      "train loss:0.00457055780493426\n",
      "train loss:0.0036367956563346793\n",
      "train loss:0.002180769964107915\n",
      "train loss:0.00035950577804884\n",
      "train loss:0.0005867951400672772\n",
      "train loss:0.0031130718439429028\n",
      "train loss:0.0023057012448967663\n",
      "train loss:0.0006056062965565645\n",
      "train loss:0.0005189334437408107\n",
      "train loss:0.004203559763638362\n",
      "train loss:0.0014422475820098926\n",
      "train loss:0.002113487440080653\n",
      "train loss:0.0038459878963231875\n",
      "train loss:0.0013871901219883497\n",
      "train loss:0.002228919621377078\n",
      "train loss:0.0015913056274699294\n",
      "train loss:0.00040616800257397087\n",
      "train loss:0.0011484395869079636\n",
      "train loss:0.0018297906037009106\n",
      "train loss:0.00029140214651738987\n",
      "train loss:0.00016832143879217946\n",
      "train loss:0.002783356898786119\n",
      "train loss:0.0001569759219973917\n",
      "train loss:0.0002854197078841161\n",
      "train loss:0.012789939501509306\n",
      "train loss:0.0005108350146970091\n",
      "train loss:0.0013614747927884903\n",
      "train loss:0.002073339855769063\n",
      "train loss:0.001414437084683477\n",
      "train loss:0.0003214045599627053\n",
      "train loss:0.0024050331063630473\n",
      "train loss:0.001253984929151067\n",
      "train loss:0.0014152914076187397\n",
      "train loss:0.000615110292609352\n",
      "train loss:0.0028273924717311344\n",
      "train loss:0.00014586984740689386\n",
      "train loss:0.0018965486310077153\n",
      "train loss:0.0008546778631819085\n",
      "train loss:5.542748746542687e-05\n",
      "train loss:0.00024396643821065925\n",
      "train loss:0.00030780621795354333\n",
      "train loss:0.0007751269494313811\n",
      "train loss:0.00015617870082721257\n",
      "train loss:0.01252650309073316\n",
      "train loss:0.0015459529734415242\n",
      "train loss:0.00019052386039154534\n",
      "train loss:0.003833665467118051\n",
      "train loss:0.002574312257989778\n",
      "train loss:0.0033551874374370605\n",
      "train loss:0.0005635558219956033\n",
      "train loss:0.002174056001960862\n",
      "train loss:0.003263448016026811\n",
      "train loss:0.0005028022266377811\n",
      "train loss:0.0007678987130216966\n",
      "train loss:0.00039556569668820134\n",
      "train loss:0.0010685557469074527\n",
      "train loss:0.0012095097132719362\n",
      "train loss:0.00015820563873034695\n",
      "train loss:0.006975376794393315\n",
      "train loss:0.0018574732695306115\n",
      "train loss:0.011091131332843427\n",
      "train loss:0.0017588494655960934\n",
      "train loss:0.003906637311144417\n",
      "train loss:0.0014698600455715996\n",
      "train loss:0.0006176177200004175\n",
      "train loss:0.002321273629055318\n",
      "train loss:0.0007935648808426829\n",
      "train loss:0.0006929182937796503\n",
      "train loss:0.0026786124030158173\n",
      "train loss:0.0005110102587974632\n",
      "train loss:0.0034727803555140648\n",
      "train loss:0.0014131663193125313\n",
      "train loss:0.0025432511990434174\n",
      "train loss:0.0017250099337106548\n",
      "train loss:0.0005658831287200265\n",
      "train loss:0.0006008984192199577\n",
      "train loss:0.00043602692592038415\n",
      "train loss:0.0004936263686465177\n",
      "train loss:0.0011162618554545229\n",
      "train loss:0.0004616078802833376\n",
      "train loss:6.664639018115312e-05\n",
      "train loss:0.0666408291719887\n",
      "train loss:0.0009700689046216709\n",
      "train loss:0.0023249332700673543\n",
      "train loss:0.006072185045779512\n",
      "train loss:0.0028729703445299166\n",
      "train loss:0.0002116941684896655\n",
      "train loss:0.00036314936360159827\n",
      "train loss:0.000725247789947843\n",
      "train loss:0.0011613472390915077\n",
      "train loss:0.001329226353038392\n",
      "train loss:0.00014579836425328548\n",
      "train loss:0.0016582087062573506\n",
      "train loss:0.003145655897686216\n",
      "train loss:0.0020649526835293318\n",
      "train loss:0.010413839531600258\n",
      "train loss:0.0033889041536525276\n",
      "train loss:0.0024537151989576867\n",
      "train loss:0.012079005123876696\n",
      "train loss:0.003266611292404332\n",
      "train loss:0.0009201968024654049\n",
      "train loss:0.006034325098696471\n",
      "train loss:0.00033338354479626743\n",
      "train loss:0.015310515059213197\n",
      "train loss:9.586370789214302e-05\n",
      "train loss:0.00039935889638761344\n",
      "train loss:0.002769577615952391\n",
      "train loss:0.002585460967794223\n",
      "train loss:0.0008127145493122425\n",
      "train loss:0.00019271739542633734\n",
      "train loss:0.01381870524685755\n",
      "train loss:0.0008848266219400068\n",
      "train loss:0.001585283631737328\n",
      "train loss:0.0005981371488582798\n",
      "train loss:0.004283139288481533\n",
      "train loss:0.0003812865621732247\n",
      "train loss:0.0016427262750513317\n",
      "train loss:0.00024512159823827386\n",
      "train loss:0.011861072602615036\n",
      "train loss:0.007018049320517908\n",
      "train loss:0.001531840276249888\n",
      "train loss:0.001381879388420675\n",
      "train loss:0.004893707102429593\n",
      "train loss:0.002337599907313614\n",
      "train loss:0.0016711493616651488\n",
      "train loss:0.0024090387593185404\n",
      "train loss:0.001055163454396026\n",
      "train loss:0.012303581362881805\n",
      "train loss:9.082253366936604e-05\n",
      "train loss:0.001078470634465628\n",
      "train loss:0.000829498156447867\n",
      "train loss:0.0007019540004342105\n",
      "train loss:0.004570183383230098\n",
      "train loss:0.003578960375857902\n",
      "train loss:0.002705709254819456\n",
      "train loss:0.001528374655649404\n",
      "train loss:0.014897194750179739\n",
      "train loss:0.0009638071738707433\n",
      "train loss:0.0018873249152892855\n",
      "train loss:0.0006386262449551027\n",
      "train loss:0.0001750911138240836\n",
      "train loss:0.0005012876737521516\n",
      "train loss:8.859019042858623e-05\n",
      "train loss:0.0016738355922537487\n",
      "train loss:0.0006342854005438228\n",
      "train loss:0.0041967463605233945\n",
      "train loss:0.003304229768384495\n",
      "train loss:0.0002914846262292262\n",
      "train loss:0.0013299534115626427\n",
      "train loss:0.0003013177898930241\n",
      "train loss:0.0033084326352463994\n",
      "train loss:0.002704095339107839\n",
      "train loss:0.001877121036843325\n",
      "train loss:0.001503509658425952\n",
      "train loss:0.002765555717069139\n",
      "train loss:0.0003982608477439757\n",
      "train loss:0.0010544668072090875\n",
      "train loss:0.003107647589771188\n",
      "train loss:0.002976213614169486\n",
      "train loss:0.0021174649952726194\n",
      "train loss:0.0022244721222009598\n",
      "train loss:0.00036156565581063724\n",
      "train loss:0.0001423225388740362\n",
      "train loss:0.00852671932319046\n",
      "train loss:0.005994623382229447\n",
      "train loss:0.0013446820211206808\n",
      "train loss:0.0002459327458481005\n",
      "train loss:0.0012397715981728064\n",
      "train loss:0.014517320760984087\n",
      "train loss:0.001041793821996703\n",
      "train loss:0.0012699045795464903\n",
      "train loss:0.006082639240374274\n",
      "train loss:0.0012194654723721306\n",
      "train loss:0.00044760643756873957\n",
      "train loss:0.0033676536014495924\n",
      "train loss:0.004322039613195264\n",
      "train loss:0.0017387170315646158\n",
      "train loss:0.004983303175078108\n",
      "train loss:0.0034545766430445148\n",
      "train loss:0.00044495816916762974\n",
      "train loss:0.0014272120590025105\n",
      "train loss:0.0005887037033233376\n",
      "train loss:0.0016268401484354218\n",
      "train loss:0.0006684539205354701\n",
      "train loss:0.0007993602564619215\n",
      "train loss:0.0015036283812204507\n",
      "train loss:0.0023028906298277097\n",
      "train loss:0.00039454118502733193\n",
      "train loss:0.0008114362249257308\n",
      "train loss:0.007326177124771472\n",
      "train loss:0.0023421122813131515\n",
      "train loss:0.0027887499062913945\n",
      "train loss:0.006553785452301801\n",
      "train loss:0.00015517583125425422\n",
      "train loss:0.003439322260291442\n",
      "train loss:0.007330099437200464\n",
      "train loss:0.0037398114741793577\n",
      "train loss:0.00047677814624775405\n",
      "train loss:0.0014440178082417037\n",
      "train loss:0.0010053868367257911\n",
      "train loss:0.0003985312260988744\n",
      "train loss:0.0015496212129516046\n",
      "train loss:0.0020161933644446024\n",
      "train loss:0.0004581069441299519\n",
      "train loss:0.00039844059348326374\n",
      "train loss:0.005465394563291294\n",
      "train loss:0.0020082589755415488\n",
      "train loss:0.0004426446979656606\n",
      "train loss:0.0006658507685721238\n",
      "train loss:0.0007973044097066437\n",
      "train loss:0.00030000669348908117\n",
      "train loss:0.0018886803416288235\n",
      "train loss:0.0013113734603418072\n",
      "train loss:0.00038275933785779305\n",
      "train loss:0.0038899867089200275\n",
      "train loss:0.003451077107972996\n",
      "train loss:0.001599804656392028\n",
      "train loss:0.0002220773978150295\n",
      "train loss:0.0006018003363632765\n",
      "train loss:0.0004146975645243199\n",
      "train loss:0.0023102732677891245\n",
      "train loss:0.00044286956480735017\n",
      "train loss:0.0007407350265579616\n",
      "train loss:0.0002967433214415417\n",
      "train loss:0.0028230839808234083\n",
      "train loss:0.0016383179960935967\n",
      "train loss:0.0012812394296286525\n",
      "train loss:0.0013022340366054502\n",
      "train loss:0.00042291129345068074\n",
      "train loss:0.0002548653143757708\n",
      "train loss:0.0003141388214554468\n",
      "train loss:0.00696774308815796\n",
      "train loss:0.0003592192239196862\n",
      "train loss:0.0012757677557607217\n",
      "train loss:0.007090979832684545\n",
      "train loss:0.00034608392306425997\n",
      "train loss:0.0026168348465940515\n",
      "train loss:0.0016339676633345874\n",
      "train loss:0.00036719504931384284\n",
      "train loss:0.008685556140289059\n",
      "train loss:0.000492390763965741\n",
      "train loss:0.0010208015746866654\n",
      "train loss:0.0004969623366558263\n",
      "train loss:0.002882278370535613\n",
      "train loss:0.017035776446233792\n",
      "train loss:0.002286456083728265\n",
      "train loss:0.0034671172522866994\n",
      "train loss:0.000255239076173934\n",
      "train loss:0.002848054022256819\n",
      "train loss:0.0010800597837220644\n",
      "train loss:0.00033628332723731444\n",
      "train loss:0.004781541299857906\n",
      "train loss:0.00027794408898496503\n",
      "train loss:0.0017558944232455346\n",
      "train loss:0.006643243173081632\n",
      "train loss:0.0023439002882996974\n",
      "train loss:0.028202454376213874\n",
      "train loss:0.0016205478711015225\n",
      "train loss:0.01706767458727594\n",
      "train loss:0.0005269374052226637\n",
      "train loss:0.0010301165398774654\n",
      "train loss:0.003287482957920351\n",
      "train loss:0.0034600796810219524\n",
      "train loss:0.00573021736443307\n",
      "train loss:0.002978971835607461\n",
      "train loss:0.005429804179621355\n",
      "train loss:0.0021543511171543297\n",
      "train loss:0.001052435552106361\n",
      "train loss:0.004173222173414235\n",
      "train loss:0.021474166960930997\n",
      "train loss:0.01967154191749786\n",
      "train loss:0.01761960909689091\n",
      "train loss:0.0012414941051273147\n",
      "train loss:0.0009298874678508927\n",
      "train loss:0.0005862871244071337\n",
      "train loss:0.002926355425507754\n",
      "train loss:0.0028449431980935463\n",
      "train loss:0.010829916585712996\n",
      "train loss:0.00023463938532759847\n",
      "train loss:0.003607722288864937\n",
      "train loss:0.001827527819147605\n",
      "train loss:0.00926094065721475\n",
      "train loss:0.0008265755903418746\n",
      "train loss:0.0012394453093897005\n",
      "train loss:0.008371591625001434\n",
      "train loss:0.009021868956071502\n",
      "train loss:7.153747193271513e-05\n",
      "train loss:0.0007199738038650987\n",
      "train loss:0.0042607162677936366\n",
      "train loss:0.0013869404387585479\n",
      "train loss:0.00029069914453837156\n",
      "train loss:0.0007002030293794635\n",
      "train loss:0.001483696641633951\n",
      "train loss:0.017455488650385344\n",
      "train loss:0.0008415211379938472\n",
      "train loss:0.0006451060009160961\n",
      "train loss:0.0001632899387667092\n",
      "train loss:0.00848866037354939\n",
      "train loss:0.00025054998628477735\n",
      "train loss:0.0033561551804986074\n",
      "train loss:0.0016963122733323142\n",
      "train loss:0.0015409965828634025\n",
      "train loss:0.010458878122452293\n",
      "train loss:0.001944297400344421\n",
      "train loss:0.003864870454058448\n",
      "train loss:0.001116854507950669\n",
      "train loss:0.00012459194123569218\n",
      "train loss:0.0016320219249971349\n",
      "train loss:0.001235319187294065\n",
      "train loss:0.011283403470691447\n",
      "train loss:0.013306153441996038\n",
      "train loss:0.007456475233172809\n",
      "train loss:0.001180604396977397\n",
      "train loss:0.001144383260951583\n",
      "train loss:0.0029860560924477525\n",
      "train loss:0.0003268551773182558\n",
      "train loss:0.00026164458184651677\n",
      "train loss:0.0004311323674556024\n",
      "train loss:0.0017778488116201063\n",
      "train loss:0.0010748731674635945\n",
      "train loss:0.006148177745052159\n",
      "train loss:0.0066204140128689\n",
      "train loss:0.0022267603565486755\n",
      "train loss:0.0031329414744232092\n",
      "train loss:0.000996499939552424\n",
      "train loss:0.0006754462881986019\n",
      "train loss:0.00523323129678541\n",
      "train loss:0.0071210346049194176\n",
      "train loss:0.0007610005252365498\n",
      "train loss:0.002144791110331918\n",
      "train loss:0.013237177053834499\n",
      "train loss:0.007475171265132716\n",
      "train loss:0.0006442837718430012\n",
      "train loss:0.0009039766359221417\n",
      "train loss:0.011373124707556631\n",
      "train loss:0.0004244418739696956\n",
      "train loss:0.0038134169602589397\n",
      "train loss:0.0012231382016529525\n",
      "train loss:0.0026513178792296043\n",
      "train loss:0.000492698960438144\n",
      "train loss:0.00011903833987061956\n",
      "train loss:0.0004049406539048491\n",
      "train loss:0.05909892009111851\n",
      "train loss:0.0008731275437167728\n",
      "train loss:0.005769261071154875\n",
      "train loss:0.004587758418571026\n",
      "train loss:0.0032816076599026228\n",
      "train loss:0.0011374071663799333\n",
      "train loss:0.0021518218317440768\n",
      "train loss:0.002561719025888061\n",
      "train loss:0.003342040148360254\n",
      "train loss:0.002100303788198794\n",
      "train loss:0.0016725003870945454\n",
      "train loss:0.002221562532759746\n",
      "train loss:0.001767916175130457\n",
      "train loss:0.0015766845791064519\n",
      "train loss:0.00026517758322449215\n",
      "train loss:0.0013171273405995242\n",
      "train loss:0.0003844642222804808\n",
      "train loss:0.0007843248097235635\n",
      "train loss:0.0017135553626744554\n",
      "train loss:0.002744024344082175\n",
      "train loss:0.014244906010696622\n",
      "train loss:0.0012735808241139037\n",
      "train loss:0.00030182354264482567\n",
      "train loss:0.008667939652834668\n",
      "train loss:0.0013290529807876345\n",
      "train loss:0.004561109663467235\n",
      "train loss:0.0006653609382056016\n",
      "train loss:0.009339240917735003\n",
      "train loss:0.0001083661986966767\n",
      "train loss:0.00023495260819637387\n",
      "train loss:0.001284232008574439\n",
      "train loss:0.0025001419828296397\n",
      "train loss:0.002406248010229041\n",
      "train loss:0.000625601156112022\n",
      "train loss:0.0006503205558195365\n",
      "train loss:0.003604865648863707\n",
      "train loss:0.004120020064157053\n",
      "train loss:0.001513412122407455\n",
      "train loss:0.0013995661256768306\n",
      "train loss:0.0010509283704757805\n",
      "train loss:0.0016472440323144735\n",
      "train loss:0.0030420126544197573\n",
      "train loss:0.0029738252932676695\n",
      "train loss:0.001290428555694687\n",
      "train loss:0.0006200989803148171\n",
      "train loss:0.002359606357010322\n",
      "train loss:0.004148839697816936\n",
      "train loss:0.0006336702354603511\n",
      "train loss:0.0024542771692672655\n",
      "train loss:0.0006704141939336158\n",
      "train loss:0.00017131886295502144\n",
      "train loss:0.001466943727489744\n",
      "train loss:0.0006465753307354549\n",
      "train loss:0.013834136062223829\n",
      "train loss:0.0014731010555345157\n",
      "train loss:0.002287848973323004\n",
      "train loss:0.0025827715977464446\n",
      "train loss:0.003109185842404524\n",
      "train loss:0.0007333147469820518\n",
      "train loss:0.002360144735921735\n",
      "train loss:0.0030803333171996066\n",
      "train loss:0.0021577992143506143\n",
      "train loss:0.007689119394808208\n",
      "train loss:0.0016558471378659476\n",
      "train loss:0.000518403431022673\n",
      "train loss:0.0004085049850260288\n",
      "train loss:0.00021481780558239825\n",
      "train loss:0.0045968988833111875\n",
      "train loss:0.0004807833629046798\n",
      "train loss:0.003595711562508963\n",
      "train loss:0.0008768444497785359\n",
      "train loss:0.0021874231256775807\n",
      "train loss:0.0005060700948364047\n",
      "train loss:0.0035803104033483436\n",
      "train loss:0.003870236313989042\n",
      "train loss:0.0004787190735733428\n",
      "train loss:0.003129337889809702\n",
      "train loss:0.0007387979160307273\n",
      "train loss:0.0026637412897357765\n",
      "train loss:0.0006535094364923717\n",
      "train loss:0.0002467625996418814\n",
      "train loss:0.0012615826662123381\n",
      "train loss:0.0012799802975390598\n",
      "train loss:0.0020301643433618868\n",
      "train loss:7.930481367456915e-05\n",
      "train loss:0.002853241816751752\n",
      "train loss:0.001417843672340279\n",
      "train loss:0.0002428063624537134\n",
      "train loss:0.002322810521742422\n",
      "train loss:0.0017770934617457681\n",
      "train loss:0.0035429739825344354\n",
      "train loss:0.008059617173069376\n",
      "train loss:0.004181150333424404\n",
      "train loss:0.00949815482373773\n",
      "train loss:0.002416218905981972\n",
      "train loss:0.001236974241383845\n",
      "train loss:0.0032348370773874436\n",
      "train loss:0.0056106040855639595\n",
      "train loss:0.004393614277672235\n",
      "train loss:0.0013811162350800227\n",
      "train loss:0.0018850018542462435\n",
      "train loss:0.0023351904194549343\n",
      "train loss:0.003127715002392868\n",
      "train loss:0.004474164448701301\n",
      "train loss:0.0016986818661472388\n",
      "train loss:0.0014592094607496003\n",
      "train loss:0.0009583793706142104\n",
      "train loss:0.005034086337107392\n",
      "train loss:0.0008477780125180961\n",
      "train loss:0.0012360222527154914\n",
      "train loss:0.0011181617990998372\n",
      "train loss:0.0007517058349634627\n",
      "train loss:0.01448659011592808\n",
      "train loss:0.0033428053902766273\n",
      "train loss:0.0024358647343269537\n",
      "train loss:0.005430654753423621\n",
      "train loss:0.003797801374373787\n",
      "train loss:0.009577560346014093\n",
      "train loss:0.007651344473168025\n",
      "train loss:0.003621521526854622\n",
      "train loss:3.9258924500797066e-05\n",
      "train loss:0.004308295212768472\n",
      "train loss:0.004145544490019135\n",
      "train loss:0.005476990651966499\n",
      "train loss:0.000774132263583407\n",
      "train loss:0.0012744925062696767\n",
      "train loss:0.0024354135054043056\n",
      "train loss:0.028443503161421413\n",
      "train loss:0.008529167235629654\n",
      "train loss:0.002318747599735805\n",
      "train loss:0.0003381017860573161\n",
      "train loss:0.005149346347730278\n",
      "train loss:0.002737264046510495\n",
      "train loss:0.0018016620454527194\n",
      "train loss:0.0010741615041898662\n",
      "train loss:0.0006651118093476736\n",
      "train loss:0.001166878980367973\n",
      "train loss:0.0017364371527733035\n",
      "train loss:0.00046386661257155723\n",
      "train loss:0.0019414777179840473\n",
      "train loss:0.011746879217565127\n",
      "train loss:0.0020504853556695306\n",
      "train loss:0.00914230396728714\n",
      "train loss:0.0019300469862745069\n",
      "train loss:0.000820085506780881\n",
      "train loss:0.0017685042735621733\n",
      "train loss:0.01158136776055857\n",
      "train loss:0.0020409235662408208\n",
      "train loss:0.003010843758217256\n",
      "train loss:0.001807646053304203\n",
      "train loss:0.0018173631696519988\n",
      "train loss:0.001620754101240962\n",
      "train loss:0.0057613909218689665\n",
      "train loss:0.0010009701429197256\n",
      "train loss:0.003697369714465868\n",
      "train loss:0.0011983720451956344\n",
      "train loss:0.0033111346701235373\n",
      "train loss:0.005869371356244641\n",
      "train loss:0.018173976364364856\n",
      "train loss:0.0032000270273627686\n",
      "train loss:0.001323388517952722\n",
      "train loss:0.0002331250448735496\n",
      "train loss:0.000836093299519179\n",
      "train loss:0.004094455852690925\n",
      "train loss:0.0033936091328929485\n",
      "train loss:0.004700337566958952\n",
      "train loss:0.003024492171052792\n",
      "train loss:0.006455491902695003\n",
      "train loss:0.0006463850198214864\n",
      "train loss:0.00056171023648316\n",
      "train loss:0.0011946469735107286\n",
      "train loss:0.0004329315976584884\n",
      "train loss:0.0014875119216231864\n",
      "train loss:0.002399068958260386\n",
      "train loss:0.0050048525338286754\n",
      "=== epoch:15, train acc:0.996, test acc:0.984 ===\n",
      "train loss:0.005562255616154425\n",
      "train loss:0.027519542303979638\n",
      "train loss:0.004872671541480092\n",
      "train loss:0.009046936316894422\n",
      "train loss:0.002063589478992528\n",
      "train loss:0.0013457136433932437\n",
      "train loss:0.0020868269938135015\n",
      "train loss:0.010316229905519581\n",
      "train loss:0.0031995871665416228\n",
      "train loss:0.005259424831062234\n",
      "train loss:0.0023993928045384764\n",
      "train loss:0.0007546615284032608\n",
      "train loss:0.0021624838279438617\n",
      "train loss:0.003884868657691368\n",
      "train loss:0.0020836607624166283\n",
      "train loss:0.07832552128927736\n",
      "train loss:0.0009281880195553757\n",
      "train loss:0.011651921135177736\n",
      "train loss:0.008059228541130352\n",
      "train loss:0.0014182224544467545\n",
      "train loss:0.008268328656404337\n",
      "train loss:0.001236679208457027\n",
      "train loss:0.0014616301432700512\n",
      "train loss:0.00030717245510596694\n",
      "train loss:0.004381731756132961\n",
      "train loss:0.0005604242352662311\n",
      "train loss:0.0011391996209882804\n",
      "train loss:0.0024940683974861017\n",
      "train loss:0.00028421623185618584\n",
      "train loss:0.0010280222712041485\n",
      "train loss:0.009846928806784054\n",
      "train loss:0.0023779899387848936\n",
      "train loss:0.0013064857946718394\n",
      "train loss:0.0015667668886730718\n",
      "train loss:0.0016040270452333306\n",
      "train loss:0.0008907669852892135\n",
      "train loss:0.0026203708144243564\n",
      "train loss:0.00945057034432047\n",
      "train loss:0.000709442435882606\n",
      "train loss:0.0022321617330425024\n",
      "train loss:0.0028604078441984288\n",
      "train loss:0.0008793143203832876\n",
      "train loss:0.0012416752645074381\n",
      "train loss:0.001412960315246605\n",
      "train loss:0.0013810209830783294\n",
      "train loss:0.0016420604129506475\n",
      "train loss:0.00018628885346425842\n",
      "train loss:0.015651865554113898\n",
      "train loss:0.0034525042543958016\n",
      "train loss:0.0022961240215715524\n",
      "train loss:0.003002126494348705\n",
      "train loss:0.0015260514588459207\n",
      "train loss:0.0042383917388438665\n",
      "train loss:0.0009166358914130815\n",
      "train loss:0.05193973007060302\n",
      "train loss:0.009411623568760767\n",
      "train loss:0.0009452372876420779\n",
      "train loss:0.00556750836905981\n",
      "train loss:0.008432689461424211\n",
      "train loss:0.0016723552927691604\n",
      "train loss:0.0027744636111890295\n",
      "train loss:0.004039919628034117\n",
      "train loss:0.0042552330410568825\n",
      "train loss:0.01169261335606525\n",
      "train loss:0.0020268587510820567\n",
      "train loss:0.000734033150550384\n",
      "train loss:0.0012327884763337182\n",
      "train loss:0.0003147772572933512\n",
      "train loss:0.00350351270149181\n",
      "train loss:0.0023834550653115422\n",
      "train loss:0.001638011228521819\n",
      "train loss:0.004805527911140386\n",
      "train loss:0.010757735286319512\n",
      "train loss:0.002783475331744098\n",
      "train loss:0.0024713032911678023\n",
      "train loss:0.0004276321193418004\n",
      "train loss:0.02001394523350015\n",
      "train loss:0.0002792267949028542\n",
      "train loss:0.0032270937713662286\n",
      "train loss:0.0038810297031683016\n",
      "train loss:0.005175561856758334\n",
      "train loss:0.0034074314633480733\n",
      "train loss:0.0006885665759485355\n",
      "train loss:0.0015165401684671678\n",
      "train loss:0.007370155736172411\n",
      "train loss:0.0023909581737819405\n",
      "train loss:0.014816495884210752\n",
      "train loss:0.002474850627774459\n",
      "train loss:0.004833112928922748\n",
      "train loss:0.00577936926332647\n",
      "train loss:0.0022901545959697015\n",
      "train loss:0.0012980767920267023\n",
      "train loss:0.004377780564030268\n",
      "train loss:0.00116735805572298\n",
      "train loss:0.0016689682660174035\n",
      "train loss:0.001956021379844912\n",
      "train loss:0.0003273266206447333\n",
      "train loss:0.0003901763122232764\n",
      "train loss:0.00027317740017417563\n",
      "train loss:0.0004342851859816861\n",
      "train loss:0.006599526265785779\n",
      "train loss:0.00406079112536256\n",
      "train loss:0.001224973180814266\n",
      "train loss:0.004353044665578209\n",
      "train loss:0.00047961372665970706\n",
      "train loss:0.0022074577343234704\n",
      "train loss:0.0008423761048410404\n",
      "train loss:0.0036963311972388617\n",
      "train loss:0.006307594527968989\n",
      "train loss:0.0010774778486330122\n",
      "train loss:0.001502379388445735\n",
      "train loss:0.0031461590092290792\n",
      "train loss:0.007871271384619736\n",
      "train loss:0.0010584649513440702\n",
      "train loss:0.0022422144652988205\n",
      "train loss:0.000994543557899243\n",
      "train loss:0.0010573259454359143\n",
      "train loss:0.039376536623674456\n",
      "train loss:0.00044685619555337555\n",
      "train loss:0.0025798625334499804\n",
      "train loss:0.0005868810222755211\n",
      "train loss:0.004939694333189823\n",
      "train loss:0.0017418368453223146\n",
      "train loss:0.0027888988740166665\n",
      "train loss:0.0015301690799501982\n",
      "train loss:0.0013494081954718606\n",
      "train loss:0.00038020498252079864\n",
      "train loss:0.0006353774403274658\n",
      "train loss:0.00032606462536759645\n",
      "train loss:0.0007823400420105863\n",
      "train loss:0.01163859863275118\n",
      "train loss:0.000677419782945094\n",
      "train loss:0.0015285893595491146\n",
      "train loss:0.00019153404504107795\n",
      "train loss:0.012949286925360909\n",
      "train loss:0.0008386013886448897\n",
      "train loss:4.8721054680114254e-05\n",
      "train loss:0.00042661002831664944\n",
      "train loss:0.00751928441263763\n",
      "train loss:0.001368642754070379\n",
      "train loss:0.0007201695701226006\n",
      "train loss:0.00013703567877825992\n",
      "train loss:0.003740667661033221\n",
      "train loss:0.0009577642960443354\n",
      "train loss:0.00016819037183087197\n",
      "train loss:0.00018096745688979477\n",
      "train loss:0.000329391025715731\n",
      "train loss:0.0003112099612261633\n",
      "train loss:0.0038511541907658587\n",
      "train loss:0.0010739927889408497\n",
      "train loss:0.0022482615682075328\n",
      "train loss:0.0038057064140651358\n",
      "train loss:0.0022750342763965072\n",
      "train loss:0.002093370241797218\n",
      "train loss:0.0004145056509204736\n",
      "train loss:0.005717567421985294\n",
      "train loss:0.0002985843583902125\n",
      "train loss:0.0025079272654065033\n",
      "train loss:0.006267192297328509\n",
      "train loss:0.012182208899340588\n",
      "train loss:0.0006959945146680527\n",
      "train loss:0.0002834048435818038\n",
      "train loss:0.0005637483413201264\n",
      "train loss:0.0014850443802116257\n",
      "train loss:0.0005068851543807484\n",
      "train loss:0.0008733438326124192\n",
      "train loss:0.00046125751816096175\n",
      "train loss:0.005488606865193654\n",
      "train loss:0.0008019093443963685\n",
      "train loss:0.0008378755917943687\n",
      "train loss:0.011038948662673338\n",
      "train loss:0.011523828605781465\n",
      "train loss:0.00025900534653197085\n",
      "train loss:0.0005837840542092223\n",
      "train loss:0.0018880817668099504\n",
      "train loss:0.004053430736775686\n",
      "train loss:0.0008485781382913414\n",
      "train loss:0.0010859499680966514\n",
      "train loss:0.00011770744320108844\n",
      "train loss:0.001672403437412508\n",
      "train loss:0.0001807137593365208\n",
      "train loss:0.0006826149112302276\n",
      "train loss:0.001805958619688092\n",
      "train loss:0.0014836393801437842\n",
      "train loss:0.009937954829347662\n",
      "train loss:0.002162797530859376\n",
      "train loss:0.002461464188696341\n",
      "train loss:0.0036974467751102027\n",
      "train loss:0.00016467561193441287\n",
      "train loss:0.0012130822243046871\n",
      "train loss:0.00012499437786248755\n",
      "train loss:0.000300352229130372\n",
      "train loss:0.00042433419767730606\n",
      "train loss:0.0007006130398554383\n",
      "train loss:0.006140383824246302\n",
      "train loss:0.002164517204850358\n",
      "train loss:0.0005714901394716232\n",
      "train loss:0.0002920057581015608\n",
      "train loss:0.00010718045491120396\n",
      "train loss:0.002861514814892255\n",
      "train loss:0.0005841364653537103\n",
      "train loss:0.0010803600806782094\n",
      "train loss:0.0010814786315610431\n",
      "train loss:0.00023151188824728172\n",
      "train loss:0.0002985070208895366\n",
      "train loss:0.0009534719918564266\n",
      "train loss:0.0021098639282527863\n",
      "train loss:0.0007752929277282254\n",
      "train loss:0.0001481423951097054\n",
      "train loss:0.0004049866340978378\n",
      "train loss:0.00011752903229525111\n",
      "train loss:0.002842212877573158\n",
      "train loss:0.0006547260500890358\n",
      "train loss:0.0019572671344672498\n",
      "train loss:0.0015276535473713853\n",
      "train loss:0.002690319775032633\n",
      "train loss:0.00023891609418851274\n",
      "train loss:0.0005700257122570614\n",
      "train loss:0.0013845780633379783\n",
      "train loss:0.0006035122732774208\n",
      "train loss:0.0013392224998246402\n",
      "train loss:0.0013828978855821902\n",
      "train loss:0.0018968110641620671\n",
      "train loss:0.000425224624841087\n",
      "train loss:0.002915579426098077\n",
      "train loss:0.001075428364440911\n",
      "train loss:0.0023037396070668685\n",
      "train loss:0.0025159148638966024\n",
      "train loss:0.00025724220106646586\n",
      "train loss:0.00107601721986601\n",
      "train loss:0.0013899203374916347\n",
      "train loss:0.0002280011497205639\n",
      "train loss:0.004106555450163632\n",
      "train loss:0.001044713301224397\n",
      "train loss:0.0006510131820351155\n",
      "train loss:0.0004383625839107146\n",
      "train loss:0.000321657259385066\n",
      "train loss:0.0007781815199246932\n",
      "train loss:0.007427360966759098\n",
      "train loss:0.004908948330213806\n",
      "train loss:0.000580972987625487\n",
      "train loss:0.0002398122622337698\n",
      "train loss:0.0016877635683124601\n",
      "train loss:0.0010315716049328087\n",
      "train loss:0.002557267564452271\n",
      "train loss:0.00031175811207743916\n",
      "train loss:0.000836467906201709\n",
      "train loss:0.0006836897236817746\n",
      "train loss:0.0013224690302259582\n",
      "train loss:0.0005769374872427293\n",
      "train loss:0.00038241953866672835\n",
      "train loss:0.0008996079502819346\n",
      "train loss:0.002074765043747714\n",
      "train loss:0.00033658071407369547\n",
      "train loss:0.0013782230149357332\n",
      "train loss:0.00042540142589135495\n",
      "train loss:0.000520024185672836\n",
      "train loss:0.0005775827013288474\n",
      "train loss:0.00293915453796992\n",
      "train loss:0.0009764866156661071\n",
      "train loss:0.0007399039112423153\n",
      "train loss:0.005187690481659103\n",
      "train loss:0.0002208068037289893\n",
      "train loss:0.0003963612400763731\n",
      "train loss:0.001063516241520135\n",
      "train loss:0.0038773603379653387\n",
      "train loss:0.0015566020263499463\n",
      "train loss:0.0005076072131296872\n",
      "train loss:8.804872242496469e-05\n",
      "train loss:0.0006625706491409734\n",
      "train loss:0.002613259524721557\n",
      "train loss:0.0002118005713941383\n",
      "train loss:0.0008688626288048868\n",
      "train loss:0.0009998650454111644\n",
      "train loss:0.0003199876588013309\n",
      "train loss:0.0021411072898068245\n",
      "train loss:0.0003457826376696459\n",
      "train loss:0.0014240206374338351\n",
      "train loss:0.0002728246370674817\n",
      "train loss:0.004027624987084413\n",
      "train loss:0.00038026098863661533\n",
      "train loss:0.0010458721956888768\n",
      "train loss:0.0034211455903134123\n",
      "train loss:0.0015491907321615065\n",
      "train loss:0.0009114260329722425\n",
      "train loss:0.0008803182379782941\n",
      "train loss:0.0007151437315972932\n",
      "train loss:0.0002398837675480056\n",
      "train loss:0.0008557552026251183\n",
      "train loss:0.002671858430243705\n",
      "train loss:0.0017952681743482648\n",
      "train loss:0.0007404152559279487\n",
      "train loss:0.0020467154004203767\n",
      "train loss:0.0015392728201607771\n",
      "train loss:0.0017640970567897667\n",
      "train loss:0.0028786175991899836\n",
      "train loss:0.0022755956912911767\n",
      "train loss:0.00011954213723837366\n",
      "train loss:0.00017069267129435982\n",
      "train loss:0.003819851205847344\n",
      "train loss:0.0018922526979367924\n",
      "train loss:0.0017541164200242211\n",
      "train loss:0.0010975589924243272\n",
      "train loss:0.0009118835442106504\n",
      "train loss:0.0016350640690615678\n",
      "train loss:0.00030104730685994314\n",
      "train loss:0.0013886340283211735\n",
      "train loss:0.0230945164556593\n",
      "train loss:0.000167276590268732\n",
      "train loss:0.0005058453218849078\n",
      "train loss:0.0009908616459567453\n",
      "train loss:0.001854581917079346\n",
      "train loss:0.00012972607384093782\n",
      "train loss:0.00282957766762078\n",
      "train loss:0.03927753567226783\n",
      "train loss:0.0006633659029086597\n",
      "train loss:0.024756856127711627\n",
      "train loss:0.0004063038276096982\n",
      "train loss:0.0017596630378072815\n",
      "train loss:0.00014397369317840626\n",
      "train loss:0.0005020845456162618\n",
      "train loss:5.867823531384622e-05\n",
      "train loss:0.002392488841941813\n",
      "train loss:0.003890992326943262\n",
      "train loss:0.0032043838943440965\n",
      "train loss:0.002668709690933571\n",
      "train loss:3.663544712881938e-05\n",
      "train loss:0.0001849946412703204\n",
      "train loss:0.0019273035331338264\n",
      "train loss:0.006603804218191244\n",
      "train loss:0.00015419827337330934\n",
      "train loss:0.00030956436471449034\n",
      "train loss:0.00434702620008723\n",
      "train loss:0.00043793548697884237\n",
      "train loss:0.0009142530656930639\n",
      "train loss:0.005176627246364734\n",
      "train loss:0.00020814789180299817\n",
      "train loss:0.0014248130711186793\n",
      "train loss:0.0011989072647735585\n",
      "train loss:0.0005130566864046867\n",
      "train loss:0.0035307214443512872\n",
      "train loss:0.004012056818247476\n",
      "train loss:0.0017273010657178195\n",
      "train loss:0.0023145722446100163\n",
      "train loss:0.0008961345679128923\n",
      "train loss:0.0008264262982318455\n",
      "train loss:5.153996810664812e-05\n",
      "train loss:0.0013752364707450989\n",
      "train loss:0.0017200357319592738\n",
      "train loss:0.0036601524956164006\n",
      "train loss:0.0037819664866989722\n",
      "train loss:0.0001652434597315544\n",
      "train loss:0.003219291832540866\n",
      "train loss:4.069374446256931e-05\n",
      "train loss:0.001529984576220172\n",
      "train loss:0.00125305720476631\n",
      "train loss:0.0019485104889239403\n",
      "train loss:0.00084042782365204\n",
      "train loss:0.00022329718042413515\n",
      "train loss:0.014173336370348762\n",
      "train loss:0.010995106660273112\n",
      "train loss:0.00029997237704892357\n",
      "train loss:0.0003977862594377597\n",
      "train loss:0.00038923986964843567\n",
      "train loss:0.0013155405144854252\n",
      "train loss:0.0001741844192330481\n",
      "train loss:0.010258655172035079\n",
      "train loss:0.0009192428293244375\n",
      "train loss:0.0045550034324901185\n",
      "train loss:0.015318619943954458\n",
      "train loss:0.00011032275286642105\n",
      "train loss:0.001094144800341195\n",
      "train loss:0.0008783886678527937\n",
      "train loss:0.00021302717776380386\n",
      "train loss:0.0010285477289264542\n",
      "train loss:0.00199702072728869\n",
      "train loss:0.0005630980548271169\n",
      "train loss:0.0001247508711519586\n",
      "train loss:0.0007338479542721904\n",
      "train loss:0.00019434153587470727\n",
      "train loss:0.0002507173819627362\n",
      "train loss:0.0007041687377174293\n",
      "train loss:0.001399819017810183\n",
      "train loss:0.0021696089743407374\n",
      "train loss:0.000276434597325281\n",
      "train loss:0.0004563759229758023\n",
      "train loss:0.0023507759714366527\n",
      "train loss:0.0013889122249302578\n",
      "train loss:0.004711386539530046\n",
      "train loss:0.006025330193824611\n",
      "train loss:0.00044194287445817554\n",
      "train loss:7.880906510448205e-05\n",
      "train loss:0.0008561773234925839\n",
      "train loss:0.012612688738110006\n",
      "train loss:0.0009097417091548723\n",
      "train loss:0.0005214070208801599\n",
      "train loss:0.0008641226919040097\n",
      "train loss:0.0057110209155597225\n",
      "train loss:0.00035325761723999804\n",
      "train loss:0.000983639544392345\n",
      "train loss:0.0007385614222018107\n",
      "train loss:0.0019211707646684564\n",
      "train loss:0.0004316305744323108\n",
      "train loss:0.00015147340019471605\n",
      "train loss:0.0010267843942757518\n",
      "train loss:0.0016599365987004658\n",
      "train loss:0.00041695067633952266\n",
      "train loss:0.0013368334804020653\n",
      "train loss:0.007004127802779387\n",
      "train loss:0.0018275398091143304\n",
      "train loss:0.003390261484235798\n",
      "train loss:0.0015246355099384334\n",
      "train loss:0.0002832151386665003\n",
      "train loss:0.00026012863169529\n",
      "train loss:0.0019342408352668928\n",
      "train loss:0.002649760593027715\n",
      "train loss:0.003603423750299325\n",
      "train loss:0.0021357007446861547\n",
      "train loss:0.0017647422339835295\n",
      "train loss:0.002119567563326369\n",
      "train loss:0.000188046666435671\n",
      "train loss:0.0007061234434993885\n",
      "train loss:0.000797849383734864\n",
      "train loss:0.0036731331813226816\n",
      "train loss:0.0023717293982814325\n",
      "train loss:0.00010640885519089114\n",
      "train loss:0.0007101955841318628\n",
      "train loss:0.0013122949332958934\n",
      "train loss:0.0027775635271362884\n",
      "train loss:0.000523728537892454\n",
      "train loss:0.0015254976558593974\n",
      "train loss:8.971712287137255e-05\n",
      "train loss:0.0005807007498045668\n",
      "train loss:0.0006229860586611908\n",
      "train loss:0.0029721407700288817\n",
      "train loss:0.0005118813233364801\n",
      "train loss:0.0037194238320333974\n",
      "train loss:0.010449081852351138\n",
      "train loss:0.004521331409466366\n",
      "train loss:0.0029484203605772085\n",
      "train loss:0.003164058633820087\n",
      "train loss:0.000988209752391008\n",
      "train loss:0.00026219293094085377\n",
      "train loss:0.0020717893066967046\n",
      "train loss:0.0005056490506856041\n",
      "train loss:0.0018572045061997441\n",
      "train loss:0.0047161070365960844\n",
      "train loss:0.0023386146398090414\n",
      "train loss:0.003777736657573035\n",
      "train loss:0.0033086275567156213\n",
      "train loss:0.0017797918368660031\n",
      "train loss:0.005624754409488435\n",
      "train loss:0.002337156736584406\n",
      "train loss:0.0011456837282794012\n",
      "train loss:0.00027729726077848715\n",
      "train loss:3.355844261247029e-05\n",
      "train loss:0.0006883065652535226\n",
      "train loss:0.004089870730134127\n",
      "train loss:0.0015752205051012302\n",
      "train loss:0.0012655901621166652\n",
      "train loss:0.0001310406605267466\n",
      "train loss:0.0012264615258661751\n",
      "train loss:0.020025981102734795\n",
      "train loss:0.0355110712829294\n",
      "train loss:0.004920804324560777\n",
      "train loss:0.001999992298955871\n",
      "train loss:0.0016935428356151091\n",
      "train loss:0.0008331438391302052\n",
      "train loss:0.011326089525258057\n",
      "train loss:0.0074086935469368335\n",
      "train loss:0.0008252506626985601\n",
      "train loss:0.0003783587991768898\n",
      "train loss:0.0006930056658147063\n",
      "train loss:0.0010162214677217859\n",
      "train loss:0.010413511131540749\n",
      "train loss:0.0013897308890835692\n",
      "train loss:0.002014863797749237\n",
      "train loss:0.005285250953694941\n",
      "train loss:0.0002602963355882236\n",
      "train loss:0.0009237200071458893\n",
      "train loss:0.0009683872372436441\n",
      "train loss:0.0027279519223952524\n",
      "train loss:0.0019648316869120444\n",
      "train loss:0.00027156722609851823\n",
      "train loss:0.0009357087354155661\n",
      "train loss:0.0071157252724043185\n",
      "train loss:0.0006638074757517206\n",
      "train loss:0.0015221009620435314\n",
      "train loss:0.0002649492113926626\n",
      "train loss:0.001901433534239343\n",
      "train loss:0.0002983946668962772\n",
      "train loss:0.0004584981563020961\n",
      "train loss:0.00016541387206033995\n",
      "train loss:0.00022358894162348707\n",
      "train loss:0.003693036516609171\n",
      "train loss:0.0033829491414333773\n",
      "train loss:0.00012110184713291601\n",
      "train loss:0.0010282497916962359\n",
      "train loss:0.0037915264622710855\n",
      "train loss:0.00025447751651078816\n",
      "train loss:0.0001832549826255032\n",
      "train loss:0.0029717799848513827\n",
      "train loss:0.0020569673883506228\n",
      "train loss:0.0021311042946741176\n",
      "train loss:0.0004691867978214439\n",
      "train loss:0.00045868447414456145\n",
      "train loss:0.000633251899463827\n",
      "train loss:0.0010629280634848145\n",
      "train loss:0.0037837413951755934\n",
      "train loss:0.0014369804734602892\n",
      "train loss:0.0032796335362149567\n",
      "train loss:0.007551712015411921\n",
      "train loss:0.0018095551557131937\n",
      "train loss:0.0026535744663234856\n",
      "train loss:0.0009037351246409744\n",
      "train loss:0.0016898681262246945\n",
      "train loss:0.00038881418692533125\n",
      "train loss:0.00011117853606845636\n",
      "train loss:0.0014519340774025277\n",
      "train loss:0.0012706041909976277\n",
      "train loss:0.0020447822477342534\n",
      "train loss:0.001783965046313725\n",
      "train loss:0.0004019997507318214\n",
      "train loss:0.0025819367975770985\n",
      "train loss:0.0015475715977423237\n",
      "train loss:0.00012218583336419674\n",
      "train loss:0.018627070628571402\n",
      "train loss:0.0004571661288733997\n",
      "train loss:0.0007042485007834176\n",
      "train loss:0.00022062603659874703\n",
      "train loss:0.00010275586958129375\n",
      "train loss:0.0037090228697994453\n",
      "train loss:0.009689588617737435\n",
      "train loss:0.0006542495921509259\n",
      "train loss:0.0004141177460943988\n",
      "train loss:0.00012392559523856673\n",
      "train loss:0.00024413763699855277\n",
      "train loss:0.0011819175709890679\n",
      "train loss:0.021436665014383363\n",
      "train loss:0.0006893361555734219\n",
      "train loss:0.0007666182399766973\n",
      "train loss:0.0006954661703172206\n",
      "train loss:0.00024197203950265946\n",
      "train loss:0.002836179822485931\n",
      "train loss:0.0009258636961408706\n",
      "train loss:0.00040290559911570497\n",
      "train loss:0.0033371913794000736\n",
      "train loss:0.005218662434509794\n",
      "train loss:0.0009128938331722466\n",
      "train loss:0.0019155256041630115\n",
      "train loss:0.0009562150397187043\n",
      "train loss:0.0020150613181409414\n",
      "train loss:0.0009842069908250243\n",
      "train loss:0.010681743790657085\n",
      "train loss:0.0014182431778677507\n",
      "train loss:0.0002776287351112029\n",
      "train loss:0.0019092391823409974\n",
      "train loss:0.00021092425563978788\n",
      "train loss:0.00025772879880425444\n",
      "train loss:0.0007097421846280945\n",
      "train loss:0.0005958841725534471\n",
      "train loss:0.001891306654881831\n",
      "train loss:5.076642561948124e-05\n",
      "train loss:0.00042649316537442527\n",
      "train loss:0.0032233718105069776\n",
      "train loss:0.0012846409574941906\n",
      "train loss:0.0014429870270249185\n",
      "train loss:0.0016234116663839183\n",
      "train loss:0.0018553746793601893\n",
      "train loss:8.91326352352859e-05\n",
      "train loss:0.0004209398510338132\n",
      "train loss:0.0006443971039822488\n",
      "train loss:0.0006310077995248286\n",
      "train loss:0.0011244704325155425\n",
      "train loss:0.00016244287000866272\n",
      "train loss:0.01536795034283644\n",
      "train loss:0.0002507473060132329\n",
      "train loss:0.0009608071740814091\n",
      "train loss:0.0008360581977558014\n",
      "train loss:0.0013580689775257338\n",
      "train loss:7.971803330714835e-05\n",
      "train loss:0.0010497952822642587\n",
      "train loss:0.0006360563659161447\n",
      "train loss:0.004700650329882542\n",
      "train loss:0.0025749237115967887\n",
      "train loss:0.0004425665148214178\n",
      "train loss:0.00030001230925925183\n",
      "train loss:0.0019583963393235617\n",
      "train loss:0.0006969286471539904\n",
      "train loss:0.0019017835522500603\n",
      "train loss:0.002246176215447852\n",
      "train loss:3.2974481276735275e-05\n",
      "train loss:0.0007048843801669931\n",
      "train loss:0.0005461895837315296\n",
      "train loss:0.0008171510464319317\n",
      "train loss:0.0013419272922550684\n",
      "train loss:0.0004812140840622077\n",
      "train loss:0.00032088247625861933\n",
      "train loss:0.0014140232183894664\n",
      "train loss:0.0023108876179563035\n",
      "=== epoch:16, train acc:0.996, test acc:0.991 ===\n",
      "train loss:0.014218589071818982\n",
      "train loss:0.0002815048988164658\n",
      "train loss:0.00019881901082608743\n",
      "train loss:0.00047893271903575883\n",
      "train loss:0.001503066032361327\n",
      "train loss:0.000865108901921848\n",
      "train loss:0.00479868772050059\n",
      "train loss:0.0005837945647431604\n",
      "train loss:0.0009224007813123476\n",
      "train loss:0.0018308756024704154\n",
      "train loss:0.0013819299660908818\n",
      "train loss:0.0004729375035862901\n",
      "train loss:0.002186800794161344\n",
      "train loss:0.0049661267351762085\n",
      "train loss:0.0001623469544474836\n",
      "train loss:0.0004073117955098879\n",
      "train loss:0.0031766899150336824\n",
      "train loss:0.0020998219949713306\n",
      "train loss:0.0021948852797415114\n",
      "train loss:0.0014512505644683172\n",
      "train loss:0.00045586249769394337\n",
      "train loss:0.0027739672173227254\n",
      "train loss:0.00011490519800488992\n",
      "train loss:0.0033257439709191666\n",
      "train loss:0.0058407455557664\n",
      "train loss:0.011457313553273056\n",
      "train loss:0.0001666052657819172\n",
      "train loss:0.0019064261065909408\n",
      "train loss:0.0002784688419325488\n",
      "train loss:0.0005896612466960549\n",
      "train loss:0.0018200051344310147\n",
      "train loss:0.000267916482810802\n",
      "train loss:0.001389311061816047\n",
      "train loss:0.0009207730679011157\n",
      "train loss:0.0013741215736391924\n",
      "train loss:0.030236588870502613\n",
      "train loss:0.0026378753036541947\n",
      "train loss:0.001330640504896211\n",
      "train loss:0.00017951666447071552\n",
      "train loss:0.0013850199841413024\n",
      "train loss:0.0039043844914094456\n",
      "train loss:0.004818705520044243\n",
      "train loss:0.05394819675179404\n",
      "train loss:0.005996128575217741\n",
      "train loss:0.002655856681969085\n",
      "train loss:0.003493183409936302\n",
      "train loss:0.006115074228588678\n",
      "train loss:0.00038868521260841026\n",
      "train loss:0.00015100856126976857\n",
      "train loss:0.0015581312703258912\n",
      "train loss:0.001956751566281945\n",
      "train loss:0.0002507378272856984\n",
      "train loss:0.003844904976951792\n",
      "train loss:0.00037537069136436797\n",
      "train loss:0.00039048135847004486\n",
      "train loss:0.0006333359182951293\n",
      "train loss:0.0009680812488173147\n",
      "train loss:0.0008542597363294142\n",
      "train loss:0.002811075785472847\n",
      "train loss:0.0035040432338300357\n",
      "train loss:0.006433375584761073\n",
      "train loss:0.004658989691174332\n",
      "train loss:2.3407055098414002e-05\n",
      "train loss:0.00014716157991707504\n",
      "train loss:0.0024480255763182973\n",
      "train loss:0.0004882905787373634\n",
      "train loss:0.0002926559558448701\n",
      "train loss:0.0011839114568204599\n",
      "train loss:0.002836870332940125\n",
      "train loss:0.0001464288327102099\n",
      "train loss:7.553274201090424e-05\n",
      "train loss:0.0017820306832426885\n",
      "train loss:0.003293305240335798\n",
      "train loss:0.000947387944724821\n",
      "train loss:0.0011191221261439078\n",
      "train loss:0.0027614834239105246\n",
      "train loss:0.0033598665311696803\n",
      "train loss:8.301348882009422e-05\n",
      "train loss:0.0027521692851019946\n",
      "train loss:0.0007714415712419209\n",
      "train loss:0.00016175356223270331\n",
      "train loss:0.005876630316968026\n",
      "train loss:0.0007291414751761066\n",
      "train loss:0.00106115577991145\n",
      "train loss:0.0022400358480225233\n",
      "train loss:0.000584151247828641\n",
      "train loss:0.000341239053839131\n",
      "train loss:0.012532088715114806\n",
      "train loss:0.0020988653762696752\n",
      "train loss:0.002675320153125987\n",
      "train loss:0.0005490860743479168\n",
      "train loss:0.0002491311225813147\n",
      "train loss:0.00043561812073709944\n",
      "train loss:0.0024820018780722076\n",
      "train loss:0.0001839623049445107\n",
      "train loss:0.00158988066301647\n",
      "train loss:0.00312638996904263\n",
      "train loss:0.00017700315825583513\n",
      "train loss:0.0004189354551255454\n",
      "train loss:0.0007167425596454128\n",
      "train loss:0.00046487448599298673\n",
      "train loss:0.0018142165564000478\n",
      "train loss:0.0004555302902048014\n",
      "train loss:0.0026659486567504644\n",
      "train loss:0.00043726783244117315\n",
      "train loss:0.0004181286481550752\n",
      "train loss:0.0005754261593551753\n",
      "train loss:0.0010070719983157643\n",
      "train loss:0.0011268672775858472\n",
      "train loss:6.528709713398858e-05\n",
      "train loss:0.0001894298156131093\n",
      "train loss:0.0013005513343355287\n",
      "train loss:0.0005272464523466868\n",
      "train loss:0.0031094205386429036\n",
      "train loss:0.0031777054433898594\n",
      "train loss:0.00015689831369770438\n",
      "train loss:0.0005976607006188232\n",
      "train loss:0.0014220772593841835\n",
      "train loss:0.002492391737282805\n",
      "train loss:0.0024956060519707346\n",
      "train loss:0.00430017624204026\n",
      "train loss:0.0007425652950342218\n",
      "train loss:0.0005646695681538709\n",
      "train loss:0.0008125883140276909\n",
      "train loss:0.0032467819931913428\n",
      "train loss:0.0020476575476982644\n",
      "train loss:0.0008317111406490985\n",
      "train loss:0.0004114958220807149\n",
      "train loss:0.0029605017524847468\n",
      "train loss:0.0015942501546120792\n",
      "train loss:0.0013201279763116805\n",
      "train loss:8.760317750916016e-05\n",
      "train loss:0.0017226570282227558\n",
      "train loss:0.00028745379848588523\n",
      "train loss:0.00019056723319290324\n",
      "train loss:0.004639218143567895\n",
      "train loss:0.0020467862785575773\n",
      "train loss:0.0023084567956238736\n",
      "train loss:0.0011836795859802142\n",
      "train loss:0.00485634893754281\n",
      "train loss:0.005163729352954092\n",
      "train loss:0.0001679273031261802\n",
      "train loss:0.0016197304108781815\n",
      "train loss:0.0013601949976149194\n",
      "train loss:0.00017223399120263705\n",
      "train loss:0.0006447372119942019\n",
      "train loss:0.0025792639949338836\n",
      "train loss:0.0001388868570000489\n",
      "train loss:0.0002788409209004586\n",
      "train loss:0.0024295240492867433\n",
      "train loss:0.0015203395318438718\n",
      "train loss:0.0005155242660774672\n",
      "train loss:0.0010438971324377058\n",
      "train loss:0.0001390925161619298\n",
      "train loss:0.028551612184480216\n",
      "train loss:0.0028978537463937113\n",
      "train loss:0.003938051000251259\n",
      "train loss:0.0030016689371237227\n",
      "train loss:0.005679253070461161\n",
      "train loss:0.014168514742403257\n",
      "train loss:0.0007494282470439805\n",
      "train loss:0.0009581646204290168\n",
      "train loss:0.00010953835162295952\n",
      "train loss:0.0009250851848173986\n",
      "train loss:0.0013665497045862458\n",
      "train loss:0.00116832624664539\n",
      "train loss:0.0014244241022595017\n",
      "train loss:0.0017443350065043713\n",
      "train loss:0.0021426905266354757\n",
      "train loss:0.0014060996388230195\n",
      "train loss:0.0040329505813286835\n",
      "train loss:0.010581662535935616\n",
      "train loss:0.0005835693620198768\n",
      "train loss:7.910934125988902e-05\n",
      "train loss:0.0003416128745556611\n",
      "train loss:0.003301341671137127\n",
      "train loss:0.001980264511418858\n",
      "train loss:0.0021552479744890904\n",
      "train loss:0.0030371660252661793\n",
      "train loss:0.0007507870296784156\n",
      "train loss:0.002775154841137421\n",
      "train loss:0.0007871276327092258\n",
      "train loss:0.0008790629298629164\n",
      "train loss:0.0007350724229973096\n",
      "train loss:0.0004342671381062458\n",
      "train loss:0.0007129569790322579\n",
      "train loss:8.213084744192439e-05\n",
      "train loss:0.0005098279847184896\n",
      "train loss:0.026333191711705464\n",
      "train loss:0.0032354862549866413\n",
      "train loss:0.0004382887607257391\n",
      "train loss:0.00016447067391618008\n",
      "train loss:0.003713153124173281\n",
      "train loss:0.007373526170159627\n",
      "train loss:0.0011703270823279878\n",
      "train loss:0.017896756943896596\n",
      "train loss:0.004643266603038748\n",
      "train loss:0.0021432726454829517\n",
      "train loss:0.000253817665270065\n",
      "train loss:0.0033110857972963904\n",
      "train loss:0.0003039422525116154\n",
      "train loss:0.0006702015529486102\n",
      "train loss:0.000995539802781915\n",
      "train loss:0.004298369816130278\n",
      "train loss:0.0023719970875482163\n",
      "train loss:0.005858602145924258\n",
      "train loss:0.00017850607423981237\n",
      "train loss:0.003908136637534074\n",
      "train loss:0.0007159642477053747\n",
      "train loss:0.010241123412925001\n",
      "train loss:8.122013743139882e-05\n",
      "train loss:0.0038130372597355\n",
      "train loss:0.002253262550668313\n",
      "train loss:0.003762433971927912\n",
      "train loss:0.00789458276781195\n",
      "train loss:0.00625240627273262\n",
      "train loss:0.0006178487204067054\n",
      "train loss:0.0005861396861298795\n",
      "train loss:0.011341574536753201\n",
      "train loss:0.0007184529180938564\n",
      "train loss:0.0021196882083511902\n",
      "train loss:0.005413944869912423\n",
      "train loss:0.00038739345514294155\n",
      "train loss:0.0025481046921399957\n",
      "train loss:0.005195928878410865\n",
      "train loss:0.0030067236547293174\n",
      "train loss:0.0020719562505630102\n",
      "train loss:0.0009516604221445412\n",
      "train loss:0.0004023777004346202\n",
      "train loss:9.16927086006988e-05\n",
      "train loss:0.004709659021265432\n",
      "train loss:0.0026619916363797512\n",
      "train loss:0.005837477432633766\n",
      "train loss:0.0033049338137840726\n",
      "train loss:0.009466798029395032\n",
      "train loss:0.0008924364808000197\n",
      "train loss:0.0006827170315210167\n",
      "train loss:0.0005775271739290043\n",
      "train loss:0.0007184156109664144\n",
      "train loss:0.0032063362986534056\n",
      "train loss:0.0059579248585692225\n",
      "train loss:0.01002773763657629\n",
      "train loss:0.0005716431851269952\n",
      "train loss:0.0011100396211233792\n",
      "train loss:3.643261532879181e-05\n",
      "train loss:0.04649188852040037\n",
      "train loss:0.0017400440817008525\n",
      "train loss:0.0021539332756052433\n",
      "train loss:0.0002822428929245741\n",
      "train loss:0.0008392373441435615\n",
      "train loss:0.005659883916763816\n",
      "train loss:0.0028372476748768556\n",
      "train loss:0.003155987558842481\n",
      "train loss:4.928532992492198e-05\n",
      "train loss:0.0006207400307546836\n",
      "train loss:0.0002595305106100616\n",
      "train loss:0.0009195809987902983\n",
      "train loss:0.0005917535367955285\n",
      "train loss:0.0008308414492527711\n",
      "train loss:0.0011757138870067702\n",
      "train loss:0.0009240461626158213\n",
      "train loss:0.0028889114348058815\n",
      "train loss:0.0003182515850527929\n",
      "train loss:0.014386551929999276\n",
      "train loss:0.00034800975596243673\n",
      "train loss:0.006206128066934629\n",
      "train loss:0.000290917514335101\n",
      "train loss:0.0003617047551733902\n",
      "train loss:0.00027729530337319185\n",
      "train loss:0.0027711088700876746\n",
      "train loss:0.0030488122233869003\n",
      "train loss:0.0008447718244779965\n",
      "train loss:5.458061789793015e-05\n",
      "train loss:0.0003472227677114549\n",
      "train loss:0.001013569844889093\n",
      "train loss:0.005913852648485805\n",
      "train loss:0.0010248271899302296\n",
      "train loss:0.0023079884337196335\n",
      "train loss:0.0031094477226738726\n",
      "train loss:0.0011267876320920397\n",
      "train loss:0.09551856827137596\n",
      "train loss:0.005196814322379933\n",
      "train loss:0.0007565422419416809\n",
      "train loss:0.0011682904504009006\n",
      "train loss:0.0016732691116040888\n",
      "train loss:0.0014872943287197978\n",
      "train loss:0.0023710578255310526\n",
      "train loss:0.005896088554513087\n",
      "train loss:0.003915828853460974\n",
      "train loss:0.004191914350331024\n",
      "train loss:0.0003607080287663805\n",
      "train loss:0.00018645680190910985\n",
      "train loss:0.0007778578485205335\n",
      "train loss:0.0006419402955959854\n",
      "train loss:0.0006725907630637334\n",
      "train loss:0.0031041287333745888\n",
      "train loss:0.002235379187017934\n",
      "train loss:0.007797816927984401\n",
      "train loss:0.0001529875426657392\n",
      "train loss:0.0010751287947380326\n",
      "train loss:0.002399510243618614\n",
      "train loss:0.002316316649616399\n",
      "train loss:0.0015189349466229634\n",
      "train loss:0.0011139641501363158\n",
      "train loss:0.0065386735887461665\n",
      "train loss:0.0009200469234180477\n",
      "train loss:0.0010628019125548942\n",
      "train loss:0.0006454889947898444\n",
      "train loss:0.0050162212350612\n",
      "train loss:0.001940204615673955\n",
      "train loss:0.003862497598909499\n",
      "train loss:0.002961382719110038\n",
      "train loss:0.0003883843254374316\n",
      "train loss:0.0050899559151732525\n",
      "train loss:0.0010563183083111135\n",
      "train loss:0.0004468597066067119\n",
      "train loss:0.014729867891568873\n",
      "train loss:0.001340837935140598\n",
      "train loss:0.0052279879904662185\n",
      "train loss:0.005616750356541649\n",
      "train loss:0.0006603055015321385\n",
      "train loss:5.959469705679003e-05\n",
      "train loss:0.0018254159545917075\n",
      "train loss:0.00017058974583846056\n",
      "train loss:0.002174438259001466\n",
      "train loss:0.015179248851907919\n",
      "train loss:0.008765692554718443\n",
      "train loss:0.0022276104367963336\n",
      "train loss:0.0016279974128137196\n",
      "train loss:0.003711524175357692\n",
      "train loss:0.0001962150794901567\n",
      "train loss:0.0003474634548903246\n",
      "train loss:0.007190524271376365\n",
      "train loss:0.0006810295402034168\n",
      "train loss:0.015478586867440329\n",
      "train loss:0.004655215782711325\n",
      "train loss:0.0011080166838864946\n",
      "train loss:0.011324538046368097\n",
      "train loss:0.005298601738600129\n",
      "train loss:0.0024871229207094254\n",
      "train loss:0.0009113760875016625\n",
      "train loss:0.010128221804724913\n",
      "train loss:0.0019601661910027576\n",
      "train loss:0.002619415761322235\n",
      "train loss:5.6516929576671076e-05\n",
      "train loss:0.0020191590560011286\n",
      "train loss:0.003016047339707758\n",
      "train loss:0.0009852875277155811\n",
      "train loss:0.001089972396167835\n",
      "train loss:0.0008639304955483026\n",
      "train loss:0.005610778495282504\n",
      "train loss:0.0014301266898040874\n",
      "train loss:0.0069053688808080734\n",
      "train loss:0.0018332798822232033\n",
      "train loss:0.0010252183482251161\n",
      "train loss:0.012001089962630775\n",
      "train loss:0.0011749704568727291\n",
      "train loss:8.586451450704244e-05\n",
      "train loss:6.580011590279483e-06\n",
      "train loss:0.0012183467745206901\n",
      "train loss:0.0029127945129011207\n",
      "train loss:0.0004888881622745888\n",
      "train loss:0.004401048468078264\n",
      "train loss:0.0006983452896513908\n",
      "train loss:0.001032640453430283\n",
      "train loss:0.0003272347102640577\n",
      "train loss:0.0006433588840232252\n",
      "train loss:0.0019262686146718532\n",
      "train loss:0.0010784509717793283\n",
      "train loss:0.001102673014684549\n",
      "train loss:0.0005130939791004929\n",
      "train loss:0.00024811249517489516\n",
      "train loss:0.0012873169824725334\n",
      "train loss:0.0005797515483004509\n",
      "train loss:0.0013022458986530162\n",
      "train loss:0.003073181034781123\n",
      "train loss:0.0008349532404084331\n",
      "train loss:0.002570285786167047\n",
      "train loss:0.0012798977433394474\n",
      "train loss:0.0002785477901756833\n",
      "train loss:0.0011333316115390078\n",
      "train loss:0.00026118816052632095\n",
      "train loss:0.0005290212859742531\n",
      "train loss:0.001649863640817127\n",
      "train loss:0.0038030280825351536\n",
      "train loss:0.00029890453123798146\n",
      "train loss:0.0006928153001503157\n",
      "train loss:0.0011275310395237569\n",
      "train loss:0.00033065540879581427\n",
      "train loss:0.0017783697692428724\n",
      "train loss:0.00024226670697149126\n",
      "train loss:9.076121714154776e-05\n",
      "train loss:0.0012407225216661223\n",
      "train loss:0.00022705805321703108\n",
      "train loss:0.0024783003823819635\n",
      "train loss:0.002058790248116886\n",
      "train loss:0.0026011873633193217\n",
      "train loss:0.0017538214211285513\n",
      "train loss:0.001479418247182576\n",
      "train loss:0.000813570451937043\n",
      "train loss:0.0001249348850426439\n",
      "train loss:0.00036017376406760366\n",
      "train loss:0.0001390542948042008\n",
      "train loss:0.0019217187735949223\n",
      "train loss:0.0015752616372088691\n",
      "train loss:0.00042851587719190784\n",
      "train loss:0.0028729659855070817\n",
      "train loss:0.002260895026269497\n",
      "train loss:0.0010001942745411156\n",
      "train loss:0.018060551398243852\n",
      "train loss:0.002340966060657584\n",
      "train loss:0.009617550906815245\n",
      "train loss:0.00027906776264920055\n",
      "train loss:0.00043967628152199225\n",
      "train loss:0.0016827082184336112\n",
      "train loss:0.0020141400192345057\n",
      "train loss:0.0053429613103638755\n",
      "train loss:0.002973068366892888\n",
      "train loss:0.02045879397435254\n",
      "train loss:0.015637349003173335\n",
      "train loss:0.038087138599016156\n",
      "train loss:0.008133858385837283\n",
      "train loss:0.001576025412795429\n",
      "train loss:0.0022416117328588883\n",
      "train loss:0.00015889150968900108\n",
      "train loss:0.0007348245700310507\n",
      "train loss:0.00036985165805331484\n",
      "train loss:0.0025401510221572283\n",
      "train loss:0.0010436241043250642\n",
      "train loss:0.00024932172680813727\n",
      "train loss:0.0006519139908064766\n",
      "train loss:0.00250865294284892\n",
      "train loss:0.020751952210426514\n",
      "train loss:0.0013095371976792522\n",
      "train loss:0.0006256500541909643\n",
      "train loss:0.0010440458543797002\n",
      "train loss:0.00023742648071660335\n",
      "train loss:0.00393451406294351\n",
      "train loss:0.0007942821851754675\n",
      "train loss:0.0008038108772770314\n",
      "train loss:0.0003624865399638097\n",
      "train loss:0.0023450763672707396\n",
      "train loss:0.002223503439263137\n",
      "train loss:0.0025748718468638814\n",
      "train loss:0.0024453718552620214\n",
      "train loss:0.0009811894938884316\n",
      "train loss:0.005759142192901007\n",
      "train loss:0.001984221166568479\n",
      "train loss:0.005864670227749363\n",
      "train loss:0.00030891238366875334\n",
      "train loss:0.00013493731798752546\n",
      "train loss:0.0007856769722921827\n",
      "train loss:0.00046161680011139205\n",
      "train loss:0.0017854616791210928\n",
      "train loss:0.00020435529355946897\n",
      "train loss:0.030185701681177637\n",
      "train loss:0.0004839348868322516\n",
      "train loss:0.0010803240001243633\n",
      "train loss:0.020770535127728116\n",
      "train loss:0.004372401720200745\n",
      "train loss:0.009347100320291174\n",
      "train loss:0.0051931510402515434\n",
      "train loss:0.0073223054291167424\n",
      "train loss:0.006124939066423597\n",
      "train loss:0.0018783415940049114\n",
      "train loss:0.0023405109041036458\n",
      "train loss:0.0013889822464858384\n",
      "train loss:0.0004153093562431683\n",
      "train loss:0.0017219942659452041\n",
      "train loss:0.002264560130162255\n",
      "train loss:0.019303537761453465\n",
      "train loss:0.00170812068761037\n",
      "train loss:0.00035359275203403727\n",
      "train loss:0.0001292543819311679\n",
      "train loss:0.013622649001575894\n",
      "train loss:0.002736809127700453\n",
      "train loss:0.0006539169755048189\n",
      "train loss:0.0013701128796866772\n",
      "train loss:0.007238344986767601\n",
      "train loss:0.0002399425895356888\n",
      "train loss:0.0020695853116810136\n",
      "train loss:0.0008831131247536923\n",
      "train loss:0.001192418262399862\n",
      "train loss:0.03157300818731959\n",
      "train loss:0.0022591518409191434\n",
      "train loss:0.0027273497325935097\n",
      "train loss:0.004166120132513041\n",
      "train loss:0.0009102373711927327\n",
      "train loss:0.008334263526253239\n",
      "train loss:0.0004609332329909542\n",
      "train loss:6.222618138659027e-05\n",
      "train loss:0.0027217413940841227\n",
      "train loss:0.02457738622165079\n",
      "train loss:0.0013486429567833797\n",
      "train loss:0.0021295441196391277\n",
      "train loss:0.0014882389591643155\n",
      "train loss:0.0009894846813048495\n",
      "train loss:0.0005064544943820644\n",
      "train loss:0.0013344481695565188\n",
      "train loss:0.00040619787444142553\n",
      "train loss:0.000245306438655617\n",
      "train loss:0.0005376083516705675\n",
      "train loss:0.0010556859020500531\n",
      "train loss:0.011876587743527208\n",
      "train loss:0.0008574946393147344\n",
      "train loss:0.017583689298663684\n",
      "train loss:0.0003255598673355106\n",
      "train loss:0.022492626843822645\n",
      "train loss:0.002989449293279277\n",
      "train loss:0.0022131351869698495\n",
      "train loss:0.00036239565617873586\n",
      "train loss:0.0003367728499972828\n",
      "train loss:0.00041679048412712353\n",
      "train loss:0.0014145018447157384\n",
      "train loss:0.0002456664453576915\n",
      "train loss:0.0030059608633024805\n",
      "train loss:0.0035078902162868396\n",
      "train loss:0.00019980316464126156\n",
      "train loss:0.0006493048887245118\n",
      "train loss:0.0020595763517333656\n",
      "train loss:0.0013087871958665188\n",
      "train loss:0.00020405712793161861\n",
      "train loss:0.0005133542222422268\n",
      "train loss:0.00024376781830832883\n",
      "train loss:0.0019425090558164207\n",
      "train loss:0.0018361540842103232\n",
      "train loss:0.0001392304049355554\n",
      "train loss:0.0002885908816198535\n",
      "train loss:0.0003094547127831592\n",
      "train loss:0.007425499707830325\n",
      "train loss:0.002323182840566761\n",
      "train loss:0.00042408220014544725\n",
      "train loss:0.0015062117548620343\n",
      "train loss:0.0009630675944042738\n",
      "train loss:0.0022809575509871866\n",
      "train loss:0.00685282886681218\n",
      "train loss:0.0001333487162506758\n",
      "train loss:0.0015109379574058824\n",
      "train loss:0.004042338276522432\n",
      "train loss:0.0022181444220541404\n",
      "train loss:0.000143245351238237\n",
      "train loss:0.002884725276866364\n",
      "train loss:0.004723571208372359\n",
      "train loss:0.010132372022557673\n",
      "train loss:0.0005837149649530544\n",
      "train loss:0.0006831505748890786\n",
      "train loss:0.000640878323550738\n",
      "train loss:0.006885617727405757\n",
      "train loss:0.0001262056585530338\n",
      "train loss:0.0014437127879983983\n",
      "train loss:0.00027001096672246044\n",
      "train loss:0.0009666880409625669\n",
      "train loss:0.007382235392996524\n",
      "train loss:0.001317639206357431\n",
      "train loss:0.0015699018221050417\n",
      "train loss:0.005895674197718782\n",
      "train loss:0.0017832782718163389\n",
      "train loss:0.0003564065098186689\n",
      "train loss:0.001492657927941973\n",
      "train loss:0.0003224478813485185\n",
      "train loss:0.00018905756817350495\n",
      "train loss:0.0006966488014189947\n",
      "train loss:0.003508125378182062\n",
      "train loss:0.0015119311345902031\n",
      "train loss:0.0041491314902464375\n",
      "train loss:0.0037705837751986367\n",
      "train loss:0.0062461837905054465\n",
      "train loss:0.006061422630774904\n",
      "train loss:0.0014159308166161568\n",
      "train loss:0.0001925319032093291\n",
      "train loss:0.001629147307508034\n",
      "train loss:0.0017319209243207407\n",
      "train loss:8.254801944074583e-05\n",
      "train loss:0.0015967155122835208\n",
      "train loss:0.0010284367397858226\n",
      "train loss:0.020464868188864282\n",
      "train loss:0.003661460339557427\n",
      "train loss:0.003985223011867256\n",
      "train loss:0.0024465015602815124\n",
      "train loss:0.0007629559853375913\n",
      "train loss:0.0003422584366386645\n",
      "train loss:0.0002770227592620763\n",
      "train loss:0.00013880313303440536\n",
      "train loss:0.0007950946995559236\n",
      "train loss:0.0033085021258505826\n",
      "train loss:0.001023019342565644\n",
      "train loss:0.00020373640586381222\n",
      "train loss:0.0014759369695597226\n",
      "train loss:0.0008142044471050667\n",
      "train loss:0.0014287545671710403\n",
      "train loss:0.0012835531614975768\n",
      "train loss:0.0006482169501087758\n",
      "train loss:0.0015709858161201409\n",
      "train loss:0.00013153423516479808\n",
      "train loss:0.0005405570974909682\n",
      "train loss:0.00010872009595655997\n",
      "train loss:0.00026115935930376357\n",
      "train loss:0.002250028393349349\n",
      "train loss:0.0008855945512639842\n",
      "train loss:0.0024790671038451536\n",
      "=== epoch:17, train acc:0.997, test acc:0.988 ===\n",
      "train loss:0.0007217007603851064\n",
      "train loss:0.0008462926551850123\n",
      "train loss:0.007859497523695429\n",
      "train loss:0.0009137454911388445\n",
      "train loss:0.0004610480782149229\n",
      "train loss:0.0001233843081955235\n",
      "train loss:0.002748437602673673\n",
      "train loss:0.0013737062289266551\n",
      "train loss:0.00355473240534743\n",
      "train loss:0.0005548784835799753\n",
      "train loss:0.00969581274546038\n",
      "train loss:0.0012185089578750647\n",
      "train loss:0.00107890017204505\n",
      "train loss:0.0006711217254582993\n",
      "train loss:0.007641711910027169\n",
      "train loss:0.0005720996356636163\n",
      "train loss:0.0018032235224052064\n",
      "train loss:0.0005714760888874885\n",
      "train loss:0.0012852664099438486\n",
      "train loss:0.0052502874397727575\n",
      "train loss:0.0012764986717653515\n",
      "train loss:0.0022547333509021447\n",
      "train loss:0.004191862578668407\n",
      "train loss:0.0029487823171646366\n",
      "train loss:0.0044005570922710955\n",
      "train loss:0.006725169638691852\n",
      "train loss:0.0003944610248508973\n",
      "train loss:0.0006919288581505665\n",
      "train loss:0.0005082754790716466\n",
      "train loss:0.00048745252372941507\n",
      "train loss:0.0020441088326836007\n",
      "train loss:0.0005244329979254878\n",
      "train loss:0.00023062195852548134\n",
      "train loss:0.00010592461588824893\n",
      "train loss:0.001578455669395132\n",
      "train loss:0.002019475350082138\n",
      "train loss:0.003892181390126878\n",
      "train loss:0.009057939523373474\n",
      "train loss:0.0020888977370768437\n",
      "train loss:0.001176497198147519\n",
      "train loss:0.0011976945048359472\n",
      "train loss:0.003516197393616773\n",
      "train loss:0.0005633949913673462\n",
      "train loss:0.0017494338725045544\n",
      "train loss:0.0038582274201230723\n",
      "train loss:0.004310025700134364\n",
      "train loss:0.0012700221259418889\n",
      "train loss:0.00017867409086445233\n",
      "train loss:0.0025648666641960795\n",
      "train loss:0.00012303800074735445\n",
      "train loss:0.0005648629030340048\n",
      "train loss:0.003120821190941566\n",
      "train loss:0.002743725822725194\n",
      "train loss:0.002516800966317519\n",
      "train loss:0.0002989279845138354\n",
      "train loss:0.00031920311655361924\n",
      "train loss:0.002401990300890333\n",
      "train loss:0.006749241177852291\n",
      "train loss:0.0005343732703734252\n",
      "train loss:0.0002316120040216062\n",
      "train loss:0.0011651475538641714\n",
      "train loss:0.001513412277713591\n",
      "train loss:0.001115662926615377\n",
      "train loss:0.0003638999653021752\n",
      "train loss:0.000186223840130539\n",
      "train loss:0.011177768328129369\n",
      "train loss:0.00014503543826246336\n",
      "train loss:0.004887292932350788\n",
      "train loss:0.0007670091641257685\n",
      "train loss:0.0016575688271253228\n",
      "train loss:0.0015686453486802746\n",
      "train loss:0.001142436214835301\n",
      "train loss:0.0009064201410729915\n",
      "train loss:0.03482767130638979\n",
      "train loss:0.001289899296599476\n",
      "train loss:0.0033350210052446555\n",
      "train loss:0.00048341513643781407\n",
      "train loss:0.0009188669167670539\n",
      "train loss:0.0011877264140988434\n",
      "train loss:0.0001805694304400001\n",
      "train loss:0.0030310402860626107\n",
      "train loss:0.00012553883563743513\n",
      "train loss:0.0014039581443667615\n",
      "train loss:0.0009135875116012984\n",
      "train loss:0.0006814523547689188\n",
      "train loss:0.0005501383616573074\n",
      "train loss:0.003985768741179926\n",
      "train loss:0.0005022412412680495\n",
      "train loss:0.0010006933141364777\n",
      "train loss:0.004221346452880139\n",
      "train loss:0.0028136790682532953\n",
      "train loss:0.0002801181927116448\n",
      "train loss:0.0001393254078568765\n",
      "train loss:0.001470646019260375\n",
      "train loss:0.0008211611462120595\n",
      "train loss:0.004083628907595417\n",
      "train loss:0.00921452692203328\n",
      "train loss:0.0007878619456102369\n",
      "train loss:0.017324912393674875\n",
      "train loss:4.140139457139843e-05\n",
      "train loss:0.003071280942911371\n",
      "train loss:0.0007469385131101681\n",
      "train loss:0.00021972182920686006\n",
      "train loss:0.0038522865665853107\n",
      "train loss:0.00041909783272111575\n",
      "train loss:0.0005317541822720531\n",
      "train loss:0.00023952211187855892\n",
      "train loss:0.0025231125736155735\n",
      "train loss:0.0010857676731842784\n",
      "train loss:0.0014016267947529523\n",
      "train loss:0.0013446868672869364\n",
      "train loss:0.0015033468725334169\n",
      "train loss:9.876662152727594e-05\n",
      "train loss:0.00013713161421917964\n",
      "train loss:0.0012278321997470673\n",
      "train loss:3.944749730514668e-05\n",
      "train loss:0.0001266553207972224\n",
      "train loss:0.003911604067147376\n",
      "train loss:0.00045163774922330846\n",
      "train loss:0.0006124608661105937\n",
      "train loss:0.006086085815551723\n",
      "train loss:0.0007766234202931702\n",
      "train loss:0.0022588003570530733\n",
      "train loss:0.0004568517762525623\n",
      "train loss:0.004493412663766887\n",
      "train loss:0.0034483024076884007\n",
      "train loss:0.001418109336580923\n",
      "train loss:0.0028637169518187485\n",
      "train loss:0.0017739045329858757\n",
      "train loss:0.0033255428443730636\n",
      "train loss:3.100351945636302e-05\n",
      "train loss:0.0010884520680098273\n",
      "train loss:0.00031681090255374794\n",
      "train loss:0.00338495970930464\n",
      "train loss:0.00022371846313268935\n",
      "train loss:0.007815318721102145\n",
      "train loss:0.0030042497125210093\n",
      "train loss:0.0006310515489172441\n",
      "train loss:0.001738460865351017\n",
      "train loss:0.003110070160707668\n",
      "train loss:0.0002687814210390515\n",
      "train loss:0.002839440469204346\n",
      "train loss:0.0010761845033638651\n",
      "train loss:0.0017482885899307965\n",
      "train loss:0.0002184020773674163\n",
      "train loss:0.0008399004581687512\n",
      "train loss:0.0026752183991647057\n",
      "train loss:0.002626278007177784\n",
      "train loss:0.00027935099199418443\n",
      "train loss:0.0002236153052976009\n",
      "train loss:5.136313444640401e-05\n",
      "train loss:0.002097981698137901\n",
      "train loss:0.0030257486843055944\n",
      "train loss:0.0011085022812602555\n",
      "train loss:0.0003275722301277918\n",
      "train loss:0.002310171391147904\n",
      "train loss:0.00296233466548585\n",
      "train loss:0.007534297100020132\n",
      "train loss:0.0013823642535108446\n",
      "train loss:0.0015089885360432802\n",
      "train loss:0.006448662009469824\n",
      "train loss:0.0008005230984164203\n",
      "train loss:0.006730333546542097\n",
      "train loss:0.0013386373776128392\n",
      "train loss:0.0007674623618540011\n",
      "train loss:0.001607389573189219\n",
      "train loss:0.002686541418893919\n",
      "train loss:0.002821036997279294\n",
      "train loss:0.0008173258911631557\n",
      "train loss:0.002633422081307078\n",
      "train loss:0.0006234233602809401\n",
      "train loss:0.0011781208377101774\n",
      "train loss:0.00035085329372916906\n",
      "train loss:0.0020800272013538935\n",
      "train loss:0.0007288113875043073\n",
      "train loss:0.000270457411936284\n",
      "train loss:0.00423564946038411\n",
      "train loss:0.00011653853306125031\n",
      "train loss:0.0011616915640538287\n",
      "train loss:0.0004442741069457362\n",
      "train loss:0.00046783467120848054\n",
      "train loss:0.0036536913635901746\n",
      "train loss:0.01206043983848096\n",
      "train loss:0.0016670592908938373\n",
      "train loss:0.0022435252617207745\n",
      "train loss:0.0018086111727980165\n",
      "train loss:0.00027235109006766263\n",
      "train loss:0.0007242041170625984\n",
      "train loss:0.00037239570260530204\n",
      "train loss:0.0023396700073835316\n",
      "train loss:0.0532268150716979\n",
      "train loss:0.004553302390787099\n",
      "train loss:0.00122288238493015\n",
      "train loss:0.0013197867834012348\n",
      "train loss:0.0018621522578306526\n",
      "train loss:0.0035090535300951824\n",
      "train loss:0.0010348909487113427\n",
      "train loss:0.0006799064146144465\n",
      "train loss:4.201237549527576e-05\n",
      "train loss:0.0018334460214894377\n",
      "train loss:0.00022334011190555165\n",
      "train loss:0.0006417803294721917\n",
      "train loss:0.004620970737356417\n",
      "train loss:0.0020843847222468373\n",
      "train loss:0.0006678890491682025\n",
      "train loss:0.005784667822777465\n",
      "train loss:0.0005349624313070261\n",
      "train loss:0.00041926088128488347\n",
      "train loss:0.0006112505071079197\n",
      "train loss:0.002491101968112659\n",
      "train loss:0.0005203610920880991\n",
      "train loss:0.0007450901871842257\n",
      "train loss:0.001516890773934561\n",
      "train loss:0.000668747918214765\n",
      "train loss:0.0018852350206576124\n",
      "train loss:0.0005326209190018305\n",
      "train loss:0.002889182334996715\n",
      "train loss:0.0007479845531930772\n",
      "train loss:0.0009602208806687669\n",
      "train loss:0.05016563597706471\n",
      "train loss:0.0008926109922979929\n",
      "train loss:0.0012134217880662341\n",
      "train loss:0.00016649060575369114\n",
      "train loss:0.00020179743113343498\n",
      "train loss:0.0021049866695201824\n",
      "train loss:0.0005722835834709584\n",
      "train loss:0.0009373732317630186\n",
      "train loss:0.0010079125183131493\n",
      "train loss:0.00034247877576064616\n",
      "train loss:0.003643926099145335\n",
      "train loss:0.0052869514626371975\n",
      "train loss:0.0006061082102317687\n",
      "train loss:0.0005555455712120751\n",
      "train loss:0.0003840241196245705\n",
      "train loss:0.003602214375583011\n",
      "train loss:0.0030439478176324595\n",
      "train loss:0.00314355142544291\n",
      "train loss:0.0008654902184775863\n",
      "train loss:0.0013522469538574924\n",
      "train loss:0.004777822774029561\n",
      "train loss:0.0009412880938512735\n",
      "train loss:7.511878028129435e-05\n",
      "train loss:0.007656675012908045\n",
      "train loss:0.0017267598180012048\n",
      "train loss:0.002350803674404621\n",
      "train loss:0.0003073075545459691\n",
      "train loss:0.0012380387383003164\n",
      "train loss:0.0019159501012159482\n",
      "train loss:0.0020130292356868424\n",
      "train loss:0.0013170701812045655\n",
      "train loss:0.002345791423504858\n",
      "train loss:0.002671752480422134\n",
      "train loss:0.0023439652063341463\n",
      "train loss:0.001745782325217989\n",
      "train loss:0.0002559085849865137\n",
      "train loss:0.001324435651346324\n",
      "train loss:0.00017693021508905199\n",
      "train loss:0.0016243521882843938\n",
      "train loss:0.004725741238037253\n",
      "train loss:0.001862966265760566\n",
      "train loss:0.0005777282093035354\n",
      "train loss:0.0048840457194773545\n",
      "train loss:6.517190735407439e-05\n",
      "train loss:0.0003229216591307326\n",
      "train loss:0.005294258535413841\n",
      "train loss:0.00015803365304580523\n",
      "train loss:0.0016705210234248264\n",
      "train loss:0.001727431291162152\n",
      "train loss:0.0014704521084714895\n",
      "train loss:0.0013317427495010536\n",
      "train loss:0.001955890509170024\n",
      "train loss:0.009989672219054467\n",
      "train loss:0.0005600202599354506\n",
      "train loss:0.0045813479732633835\n",
      "train loss:0.00047333491420343274\n",
      "train loss:0.0021677334798340876\n",
      "train loss:0.014425544901228039\n",
      "train loss:0.00032162400250223503\n",
      "train loss:0.0019518562468808368\n",
      "train loss:0.0014018087175108876\n",
      "train loss:0.0001952182172165255\n",
      "train loss:0.0018337013260148727\n",
      "train loss:4.5834706289721494e-05\n",
      "train loss:0.004134275705779547\n",
      "train loss:0.00023536643167034247\n",
      "train loss:0.0029050095975296547\n",
      "train loss:0.001216589770493208\n",
      "train loss:0.0009413386068386343\n",
      "train loss:0.0006415803284214093\n",
      "train loss:0.00226364118250766\n",
      "train loss:0.010131383112499448\n",
      "train loss:0.001184011499829915\n",
      "train loss:0.0003707237162431726\n",
      "train loss:0.0053184545582455915\n",
      "train loss:0.0034693254549773077\n",
      "train loss:0.0006125695625672293\n",
      "train loss:8.887395197732098e-05\n",
      "train loss:0.0006241871676447538\n",
      "train loss:0.0001950312053224643\n",
      "train loss:0.0005278744061758922\n",
      "train loss:0.0002614365556534397\n",
      "train loss:0.00046163890097815856\n",
      "train loss:0.00036689061145259096\n",
      "train loss:0.0014737854789480876\n",
      "train loss:0.004499606751292671\n",
      "train loss:0.0011644611140125742\n",
      "train loss:0.005255641647582053\n",
      "train loss:0.0003715653832880769\n",
      "train loss:0.0002767108737265789\n",
      "train loss:0.0032699394344466583\n",
      "train loss:0.003133227885865349\n",
      "train loss:0.00011936475971708112\n",
      "train loss:0.003089906586035593\n",
      "train loss:0.004058105252593165\n",
      "train loss:0.00026058952729748794\n",
      "train loss:0.0003136759650111829\n",
      "train loss:0.0008394887187900439\n",
      "train loss:0.0005526108140713631\n",
      "train loss:0.0002812127108575225\n",
      "train loss:0.0026402654125650504\n",
      "train loss:0.0005519587125119868\n",
      "train loss:0.00014523291164326117\n",
      "train loss:0.0002306363370467124\n",
      "train loss:0.001646318396395456\n",
      "train loss:0.004126716521596395\n",
      "train loss:0.00119688441426667\n",
      "train loss:0.001725355670603825\n",
      "train loss:0.0004910544880438874\n",
      "train loss:0.0006572665484617294\n",
      "train loss:0.0006643212462068838\n",
      "train loss:0.00015016618939840633\n",
      "train loss:0.00010733036969768741\n",
      "train loss:0.00014553953999976222\n",
      "train loss:0.0005804009884560671\n",
      "train loss:0.00030325527767903547\n",
      "train loss:0.0003097521696676232\n",
      "train loss:0.0020290154907457746\n",
      "train loss:0.0024391135028902534\n",
      "train loss:0.0012871098317594563\n",
      "train loss:0.006340291002278544\n",
      "train loss:0.0029337571061313484\n",
      "train loss:0.0016723013856846894\n",
      "train loss:0.000443798530624746\n",
      "train loss:0.0023890191011029125\n",
      "train loss:0.0009640620918002383\n",
      "train loss:0.0007042678482713608\n",
      "train loss:0.0018264694941812069\n",
      "train loss:0.003163099160226376\n",
      "train loss:0.001190750055153442\n",
      "train loss:0.009804840280580712\n",
      "train loss:0.0024007180111150685\n",
      "train loss:0.002265705469307532\n",
      "train loss:0.0008775175274813372\n",
      "train loss:0.0004590855662829948\n",
      "train loss:0.0031355427244824097\n",
      "train loss:0.0012443155588985249\n",
      "train loss:0.001498932518664922\n",
      "train loss:0.0006035311864974228\n",
      "train loss:0.001673872374999222\n",
      "train loss:0.005811106627389217\n",
      "train loss:0.0007600397790543859\n",
      "train loss:0.0016161688308815106\n",
      "train loss:0.00045904981457701224\n",
      "train loss:0.0013777196227853286\n",
      "train loss:0.0018195556336368496\n",
      "train loss:0.0030874187775483215\n",
      "train loss:0.001315005261140137\n",
      "train loss:0.0003105442861935873\n",
      "train loss:0.0015822287315766085\n",
      "train loss:0.00023405502900499604\n",
      "train loss:0.0035476826805702646\n",
      "train loss:0.002139953110788044\n",
      "train loss:0.00264228455884254\n",
      "train loss:0.0019567550316589477\n",
      "train loss:0.00041483256700829835\n",
      "train loss:0.0009361625864845097\n",
      "train loss:0.0027529162713089933\n",
      "train loss:0.001920292857827192\n",
      "train loss:0.0004159042187604004\n",
      "train loss:0.00180417154603262\n",
      "train loss:0.00028739976954866506\n",
      "train loss:0.0022270619586438733\n",
      "train loss:0.0009727114702475859\n",
      "train loss:0.0006966967922831666\n",
      "train loss:0.0009676880317694721\n",
      "train loss:0.0002962996806491905\n",
      "train loss:0.0005395397469557473\n",
      "train loss:0.00021783865238270004\n",
      "train loss:0.00044708959022743557\n",
      "train loss:0.0027981887682188593\n",
      "train loss:0.0025534739940513145\n",
      "train loss:0.0010459352562091186\n",
      "train loss:0.0009723553345632641\n",
      "train loss:0.0004715504465517387\n",
      "train loss:0.000333718915076981\n",
      "train loss:0.0006161640205919062\n",
      "train loss:0.0011705943090288098\n",
      "train loss:3.4225482218697494e-05\n",
      "train loss:0.0008152515007849449\n",
      "train loss:0.00950185238290333\n",
      "train loss:0.00031436171498524796\n",
      "train loss:0.004484186038977807\n",
      "train loss:0.000977733167091873\n",
      "train loss:0.031634107377807266\n",
      "train loss:0.001272827077513156\n",
      "train loss:7.174942056879925e-05\n",
      "train loss:0.0030980137585580837\n",
      "train loss:0.0003251150165770443\n",
      "train loss:0.0014560576233528175\n",
      "train loss:0.0002267635388131459\n",
      "train loss:0.001215762458147107\n",
      "train loss:0.0010275488611523318\n",
      "train loss:0.0005041243866601435\n",
      "train loss:7.033830589570309e-05\n",
      "train loss:0.008716012861405099\n",
      "train loss:0.0002678025675444481\n",
      "train loss:0.05028699714948051\n",
      "train loss:0.00046747664514273576\n",
      "train loss:0.0018993085363830473\n",
      "train loss:2.9700214996171257e-05\n",
      "train loss:0.0012235158884018158\n",
      "train loss:1.814382782407679e-05\n",
      "train loss:0.002452805881589187\n",
      "train loss:0.003829797864328738\n",
      "train loss:0.0004962554535853839\n",
      "train loss:0.0032213436184247847\n",
      "train loss:0.0012080380630334147\n",
      "train loss:0.004168107960046508\n",
      "train loss:0.0016265681675755115\n",
      "train loss:0.000331912235639385\n",
      "train loss:0.0010799150342146713\n",
      "train loss:0.0010186392558427636\n",
      "train loss:0.0006110465263750523\n",
      "train loss:0.002506265136611409\n",
      "train loss:0.0025682616835809476\n",
      "train loss:0.00011936751751124203\n",
      "train loss:0.00034913056856303624\n",
      "train loss:0.002408948516749292\n",
      "train loss:0.001158531234862418\n",
      "train loss:0.002861830800552655\n",
      "train loss:0.0032701090859068725\n",
      "train loss:0.0004134189964030579\n",
      "train loss:0.0013165743312249826\n",
      "train loss:0.000540432748112025\n",
      "train loss:0.00014350182247357504\n",
      "train loss:0.002219688865991675\n",
      "train loss:0.0003146542182495394\n",
      "train loss:0.00025497550682548377\n",
      "train loss:7.400731524640638e-05\n",
      "train loss:0.0004893025354828469\n",
      "train loss:0.0014954326316186475\n",
      "train loss:0.0011528782512585784\n",
      "train loss:6.40445565017166e-05\n",
      "train loss:0.0008541894860463605\n",
      "train loss:0.001066528755689428\n",
      "train loss:0.0015346927842622925\n",
      "train loss:0.0011926621390312491\n",
      "train loss:0.00035846111470677096\n",
      "train loss:0.0025038885359267614\n",
      "train loss:0.0011688076944340936\n",
      "train loss:0.003966974920690008\n",
      "train loss:0.004509591493865943\n",
      "train loss:0.00034552930776154007\n",
      "train loss:0.000972919164723737\n",
      "train loss:0.0005215379408729961\n",
      "train loss:0.0019060265329178564\n",
      "train loss:0.00821316253212283\n",
      "train loss:7.514770167000379e-05\n",
      "train loss:2.871697763841198e-05\n",
      "train loss:0.000594951288365023\n",
      "train loss:0.0004729161097592122\n",
      "train loss:6.465681207787631e-05\n",
      "train loss:0.004360899018278245\n",
      "train loss:0.0032105919450146187\n",
      "train loss:0.0039872193578644475\n",
      "train loss:0.0008204449066999199\n",
      "train loss:0.0012411316311245581\n",
      "train loss:0.00169971494405915\n",
      "train loss:0.0009202936265197594\n",
      "train loss:0.003383147710080769\n",
      "train loss:0.0026384317980294265\n",
      "train loss:0.0004673329765318883\n",
      "train loss:0.0008187618345313756\n",
      "train loss:0.005575015599683391\n",
      "train loss:6.762091973482059e-05\n",
      "train loss:8.043506437762343e-05\n",
      "train loss:0.00038789583927540603\n",
      "train loss:0.00034678667023805453\n",
      "train loss:0.0012144662412205945\n",
      "train loss:0.0013279525988722546\n",
      "train loss:0.0018070765006445474\n",
      "train loss:0.0001607545219020551\n",
      "train loss:0.0007057426858269631\n",
      "train loss:0.0019099134553907755\n",
      "train loss:0.005764225355949836\n",
      "train loss:0.0031982690273751856\n",
      "train loss:0.0009568075046093387\n",
      "train loss:0.0005236438404219134\n",
      "train loss:0.0011425848519787066\n",
      "train loss:0.002718910504139008\n",
      "train loss:0.002632458419078504\n",
      "train loss:0.0005683439738987757\n",
      "train loss:0.0012187081444680998\n",
      "train loss:0.006315699810567808\n",
      "train loss:0.0014118451078772814\n",
      "train loss:0.0007021850548690091\n",
      "train loss:0.0008285933636719081\n",
      "train loss:0.00011029162803523285\n",
      "train loss:0.0025075664426167927\n",
      "train loss:0.0010448519938866553\n",
      "train loss:0.0020175087132356284\n",
      "train loss:0.00015571612685009537\n",
      "train loss:0.003332993004622129\n",
      "train loss:0.0003037155753846425\n",
      "train loss:0.05053873188148431\n",
      "train loss:6.473606658593709e-05\n",
      "train loss:0.0002907159784996553\n",
      "train loss:6.350186691060432e-05\n",
      "train loss:0.000266594842625425\n",
      "train loss:0.006190595159599477\n",
      "train loss:0.0004402963188097242\n",
      "train loss:0.005640646805972226\n",
      "train loss:0.0003441870004404951\n",
      "train loss:0.0013085770316334936\n",
      "train loss:0.0008638831623430718\n",
      "train loss:0.00019717092624407535\n",
      "train loss:0.0034547484965166356\n",
      "train loss:0.0013219674884745766\n",
      "train loss:0.002837516152750132\n",
      "train loss:0.004399021014121104\n",
      "train loss:0.0024012245078784645\n",
      "train loss:0.0003682025397182169\n",
      "train loss:0.012764492840195502\n",
      "train loss:0.00017689409666915394\n",
      "train loss:0.0020769569990442983\n",
      "train loss:0.0018908671639018277\n",
      "train loss:0.005356323795518947\n",
      "train loss:0.0032604779569110533\n",
      "train loss:0.001683131195480677\n",
      "train loss:0.00047114270487968665\n",
      "train loss:0.000273621231056189\n",
      "train loss:0.0032049890774476925\n",
      "train loss:0.00028452639685764907\n",
      "train loss:0.02584239906093254\n",
      "train loss:0.02942238583946071\n",
      "train loss:0.004490158412383213\n",
      "train loss:0.0008999948192084092\n",
      "train loss:0.004074065713751535\n",
      "train loss:0.0016367310048065501\n",
      "train loss:0.0026217163556547358\n",
      "train loss:0.00033927407756881626\n",
      "train loss:0.0008396583891477161\n",
      "train loss:0.0014079682170681528\n",
      "train loss:0.0013467810597888807\n",
      "train loss:0.0013075172688905995\n",
      "train loss:0.02600793313916337\n",
      "train loss:0.00010105149939873522\n",
      "train loss:0.005315812566160859\n",
      "train loss:0.004998249152236525\n",
      "train loss:0.0030366511551397613\n",
      "train loss:7.292686612170818e-05\n",
      "train loss:0.002631838480588593\n",
      "train loss:0.00091756400293922\n",
      "train loss:0.0036006942862300134\n",
      "train loss:0.0005618823823607696\n",
      "train loss:7.681224288817329e-05\n",
      "train loss:0.0009329749488655274\n",
      "train loss:0.00013304537472804255\n",
      "train loss:0.0001427183801561903\n",
      "train loss:0.0009354818950474058\n",
      "train loss:0.0009781548997669105\n",
      "train loss:0.013263592539639085\n",
      "train loss:0.0023581811953166565\n",
      "train loss:0.0016352522732913535\n",
      "train loss:0.00036169500310785716\n",
      "train loss:0.0006171702116261559\n",
      "train loss:0.0015322536949242665\n",
      "train loss:6.16218980820018e-05\n",
      "train loss:0.0005878676216360648\n",
      "train loss:8.75795686417807e-05\n",
      "train loss:0.001067394860720036\n",
      "train loss:0.0015180079430939977\n",
      "train loss:0.002959049856999348\n",
      "train loss:0.0001224338437480343\n",
      "train loss:0.003719165011749019\n",
      "train loss:0.006845400757878617\n",
      "train loss:0.001626036858490514\n",
      "train loss:0.00992857536575662\n",
      "train loss:0.002214627122799546\n",
      "train loss:0.0009154384920150001\n",
      "train loss:0.0005394060309824153\n",
      "train loss:0.0004171559783560997\n",
      "train loss:0.00021056972201381207\n",
      "train loss:0.0004912679471175788\n",
      "train loss:0.0019392290212539292\n",
      "train loss:0.0010055054658635472\n",
      "train loss:0.00063292706744611\n",
      "train loss:0.0021638350227572317\n",
      "train loss:0.0009417414031488461\n",
      "train loss:0.006141354259262373\n",
      "=== epoch:18, train acc:0.999, test acc:0.992 ===\n",
      "train loss:0.00040408705744891555\n",
      "train loss:0.0003987878352856679\n",
      "train loss:0.00026257985302444896\n",
      "train loss:0.0002235788441082063\n",
      "train loss:0.0024509806886022388\n",
      "train loss:0.0016287842462085298\n",
      "train loss:0.003136930837709702\n",
      "train loss:0.0015654705434010447\n",
      "train loss:0.0008559962743878502\n",
      "train loss:0.0015211972240546379\n",
      "train loss:0.00018285763554497784\n",
      "train loss:0.0023818569691013403\n",
      "train loss:0.0010321934691178076\n",
      "train loss:0.0013827839568090602\n",
      "train loss:0.02608842174857846\n",
      "train loss:4.90272098762606e-05\n",
      "train loss:0.0013458344795275743\n",
      "train loss:0.0010244579302252635\n",
      "train loss:0.0040258168565684045\n",
      "train loss:0.00125283702918243\n",
      "train loss:0.004069851124554747\n",
      "train loss:0.000486161494038908\n",
      "train loss:0.002735376964097095\n",
      "train loss:0.00040150962606170447\n",
      "train loss:0.0038518794372284144\n",
      "train loss:0.00430157491923109\n",
      "train loss:0.0011090044075217605\n",
      "train loss:0.008411724455561606\n",
      "train loss:0.0016138165179222534\n",
      "train loss:0.00021572662785417407\n",
      "train loss:0.00046456046236940224\n",
      "train loss:0.0020164268770488285\n",
      "train loss:0.000861989004821952\n",
      "train loss:0.0007867273121659448\n",
      "train loss:0.00026465864975217706\n",
      "train loss:0.00027153619763039974\n",
      "train loss:0.002920185108369062\n",
      "train loss:0.0018669151743588408\n",
      "train loss:0.0009561788701574114\n",
      "train loss:0.0005934214792909277\n",
      "train loss:0.001360767312545871\n",
      "train loss:0.0013788478779688034\n",
      "train loss:0.04626198096567049\n",
      "train loss:0.0007193817688029798\n",
      "train loss:0.0030897924814883328\n",
      "train loss:0.0004926222557449292\n",
      "train loss:0.0006761967429474094\n",
      "train loss:0.001552379790243734\n",
      "train loss:6.873085295817612e-05\n",
      "train loss:0.0004901108608554123\n",
      "train loss:0.004887791847819622\n",
      "train loss:0.0009744175021863983\n",
      "train loss:0.0007205849938540931\n",
      "train loss:0.0029691435958162293\n",
      "train loss:0.0007137383444856236\n",
      "train loss:0.005546118917518129\n",
      "train loss:0.0019008678382388823\n",
      "train loss:0.0005798264881737541\n",
      "train loss:0.0012011032899382496\n",
      "train loss:0.0013587860353678823\n",
      "train loss:0.0009732583368372963\n",
      "train loss:0.0033179740168946138\n",
      "train loss:0.004328037776255148\n",
      "train loss:0.00218928692474263\n",
      "train loss:0.00012782852964270504\n",
      "train loss:0.0020626334064000993\n",
      "train loss:0.0002483327867797242\n",
      "train loss:0.0003462476372926831\n",
      "train loss:0.005674091007673111\n",
      "train loss:0.00016312678423572257\n",
      "train loss:0.0005194271235203077\n",
      "train loss:0.0019832998064756016\n",
      "train loss:0.003001938507891348\n",
      "train loss:0.0001530627981747912\n",
      "train loss:0.0015023956732622904\n",
      "train loss:0.0009740523527754576\n",
      "train loss:0.011937107840419715\n",
      "train loss:0.0024789790101986163\n",
      "train loss:0.0020209588103645643\n",
      "train loss:0.0015645519869660676\n",
      "train loss:4.677834048513257e-05\n",
      "train loss:0.001816964092822269\n",
      "train loss:0.0015074164080703587\n",
      "train loss:3.62252907158809e-05\n",
      "train loss:0.0002613112334101054\n",
      "train loss:0.0003371132959131082\n",
      "train loss:0.0005442049136019372\n",
      "train loss:0.004314255673704499\n",
      "train loss:0.0006320809900112125\n",
      "train loss:9.198939196900923e-05\n",
      "train loss:7.416332737044574e-05\n",
      "train loss:0.00010617008217116081\n",
      "train loss:0.000446621177195878\n",
      "train loss:0.00030077475803669805\n",
      "train loss:0.0030669575828760538\n",
      "train loss:0.0001092155488547365\n",
      "train loss:0.003604916795158603\n",
      "train loss:0.000774985626588413\n",
      "train loss:0.0001937203623655227\n",
      "train loss:0.0035415746236259144\n",
      "train loss:0.0006774833692524741\n",
      "train loss:0.001460589997707953\n",
      "train loss:0.000537348984249015\n",
      "train loss:0.0006960613344365818\n",
      "train loss:0.0008947828616321901\n",
      "train loss:0.008197009823372488\n",
      "train loss:8.542284778494946e-05\n",
      "train loss:0.00027630234224370747\n",
      "train loss:0.0009207145118724662\n",
      "train loss:0.00016367144278457458\n",
      "train loss:0.004329778867154993\n",
      "train loss:0.00015681998056561977\n",
      "train loss:0.0007015049136909775\n",
      "train loss:0.009666760086857363\n",
      "train loss:0.0010597259672357532\n",
      "train loss:0.0021585757217158093\n",
      "train loss:0.0013533775089324101\n",
      "train loss:0.0003788045329779529\n",
      "train loss:0.0030266061152388157\n",
      "train loss:0.0008205242107174462\n",
      "train loss:0.0004163005081855374\n",
      "train loss:0.0015421747667084692\n",
      "train loss:0.004546764051751902\n",
      "train loss:0.0005172867309662613\n",
      "train loss:0.0006355581909878989\n",
      "train loss:0.0023463671713318058\n",
      "train loss:0.00021663748075738246\n",
      "train loss:0.0021907401277057722\n",
      "train loss:4.6106916778408047e-05\n",
      "train loss:0.002105615972119593\n",
      "train loss:0.0017088477148242273\n",
      "train loss:0.0008348985621493059\n",
      "train loss:0.002114002279910113\n",
      "train loss:0.002263106801429651\n",
      "train loss:0.0007586022726501254\n",
      "train loss:4.372261218325351e-05\n",
      "train loss:2.8653840838470023e-05\n",
      "train loss:0.00011578817947782798\n",
      "train loss:0.00021624306844248732\n",
      "train loss:5.3991292930219766e-05\n",
      "train loss:0.0003928344084072467\n",
      "train loss:0.00047395180559850676\n",
      "train loss:0.0029942515805379687\n",
      "train loss:0.000419059830458434\n",
      "train loss:0.0006017187183088635\n",
      "train loss:0.0001357218770363821\n",
      "train loss:0.00010730367031693997\n",
      "train loss:0.00038412239299888125\n",
      "train loss:0.011466588736353462\n",
      "train loss:0.0013138456349037133\n",
      "train loss:0.0002292309459193144\n",
      "train loss:0.0003642782767037189\n",
      "train loss:0.0004047310863320567\n",
      "train loss:0.002618514769201818\n",
      "train loss:0.0005056380452042971\n",
      "train loss:0.0008848811375429726\n",
      "train loss:0.0007474113321793307\n",
      "train loss:0.009402912116351456\n",
      "train loss:0.0016801378727570827\n",
      "train loss:0.00032473325255726515\n",
      "train loss:0.0005041409500825113\n",
      "train loss:0.0009511429944261262\n",
      "train loss:0.0006746685637681983\n",
      "train loss:0.0025748145972748935\n",
      "train loss:0.0016258539559609933\n",
      "train loss:0.0027705494414643826\n",
      "train loss:0.0012753424985237896\n",
      "train loss:0.00011449637720298546\n",
      "train loss:0.0024187044362696266\n",
      "train loss:0.00037600505058160526\n",
      "train loss:0.00021622750825827964\n",
      "train loss:0.0016646822739222365\n",
      "train loss:0.00042613344320114487\n",
      "train loss:0.0002926620324549638\n",
      "train loss:0.0001407324538130246\n",
      "train loss:0.0007284426692713369\n",
      "train loss:0.00027119480384967963\n",
      "train loss:0.003187594224352592\n",
      "train loss:2.0820857676852517e-05\n",
      "train loss:0.004031963863036647\n",
      "train loss:5.5036274490802734e-05\n",
      "train loss:0.0020481114550472334\n",
      "train loss:0.00042371153061558024\n",
      "train loss:0.0015787022751424184\n",
      "train loss:0.0011176660867820672\n",
      "train loss:0.00012819795779566432\n",
      "train loss:0.007624567336902012\n",
      "train loss:0.002132231262006724\n",
      "train loss:0.0002857019001240924\n",
      "train loss:0.002146521476445699\n",
      "train loss:0.0005205886900501025\n",
      "train loss:0.001426885009603855\n",
      "train loss:0.00034109906249400306\n",
      "train loss:0.00010833976562593564\n",
      "train loss:0.00010890487318571027\n",
      "train loss:0.0014317625092210406\n",
      "train loss:0.0001939949305619279\n",
      "train loss:0.0026960499686174404\n",
      "train loss:0.0003270885352693137\n",
      "train loss:0.00014106351114729206\n",
      "train loss:0.0013888234376770003\n",
      "train loss:0.0027274032201705196\n",
      "train loss:0.00010619538679598116\n",
      "train loss:0.0003376272303264116\n",
      "train loss:0.0019141366189873484\n",
      "train loss:0.0001626980861022579\n",
      "train loss:0.0003793613378501858\n",
      "train loss:0.0006697512777000438\n",
      "train loss:0.002959477269315065\n",
      "train loss:0.0031353783615355696\n",
      "train loss:4.489793573473251e-05\n",
      "train loss:0.0009958322749830779\n",
      "train loss:0.0001486249131044448\n",
      "train loss:0.0008533412156037913\n",
      "train loss:0.0011363437429856619\n",
      "train loss:0.0023289175419711245\n",
      "train loss:0.0029350404461007474\n",
      "train loss:0.0007654134287543783\n",
      "train loss:0.013241220453543853\n",
      "train loss:0.0007024640689387474\n",
      "train loss:0.00046139794669494157\n",
      "train loss:0.00013148887740165377\n",
      "train loss:0.0006520257624058464\n",
      "train loss:0.0011897761565201111\n",
      "train loss:0.00011984104559314366\n",
      "train loss:8.743067678271872e-05\n",
      "train loss:0.0022386662385093787\n",
      "train loss:0.0010658813004977379\n",
      "train loss:0.001797268943709505\n",
      "train loss:0.0013423856203913728\n",
      "train loss:0.00042936124121518407\n",
      "train loss:3.7326933867445256e-05\n",
      "train loss:0.00016218635163531517\n",
      "train loss:0.00013583931576575416\n",
      "train loss:0.003539119036978752\n",
      "train loss:0.0012461875299699166\n",
      "train loss:0.0010023084062380436\n",
      "train loss:0.002560851198615416\n",
      "train loss:0.0010114479066436243\n",
      "train loss:0.0020185481406357854\n",
      "train loss:0.00787210912842506\n",
      "train loss:0.00026286011892465643\n",
      "train loss:0.00011845757144841973\n",
      "train loss:0.0001413984735570693\n",
      "train loss:0.001009998803033408\n",
      "train loss:0.00037122425623891336\n",
      "train loss:0.00020876799490492703\n",
      "train loss:4.0959860651768005e-05\n",
      "train loss:0.00027738920825430986\n",
      "train loss:9.836141449925088e-05\n",
      "train loss:0.0015024572916599748\n",
      "train loss:5.811186155808555e-05\n",
      "train loss:0.009063147934347725\n",
      "train loss:0.0002611407404184353\n",
      "train loss:0.00015328337584897994\n",
      "train loss:0.0016073420867750855\n",
      "train loss:0.0032383872533828313\n",
      "train loss:0.0003798719050841677\n",
      "train loss:0.001790498251848695\n",
      "train loss:0.00010641340652037975\n",
      "train loss:3.677713097373026e-05\n",
      "train loss:0.0002923776010270481\n",
      "train loss:0.0006228353901366085\n",
      "train loss:0.0006983128481785042\n",
      "train loss:0.0010677998808442482\n",
      "train loss:0.002339018453888006\n",
      "train loss:5.333899228235623e-05\n",
      "train loss:0.0006366421313903126\n",
      "train loss:0.0013974925319174068\n",
      "train loss:4.45326543298681e-05\n",
      "train loss:0.0037306865011027457\n",
      "train loss:0.0062664533137691495\n",
      "train loss:0.001279317372823682\n",
      "train loss:0.0007736051218327143\n",
      "train loss:0.0017451320335099604\n",
      "train loss:0.0011354845243175427\n",
      "train loss:0.00014322530690465715\n",
      "train loss:0.002142345188837315\n",
      "train loss:0.00022835019310253404\n",
      "train loss:0.0001258084529530234\n",
      "train loss:5.0494946157344165e-05\n",
      "train loss:0.0005870512245007016\n",
      "train loss:0.0022222937781981745\n",
      "train loss:0.0002777135112495122\n",
      "train loss:0.0016057757712008057\n",
      "train loss:0.0031362330344960115\n",
      "train loss:0.0018363521275140732\n",
      "train loss:0.0006370045996742068\n",
      "train loss:0.0037150896009308797\n",
      "train loss:0.0002692562954095899\n",
      "train loss:0.00011151868984180855\n",
      "train loss:0.00015353266926355834\n",
      "train loss:0.0007553285959839756\n",
      "train loss:0.0003327458382506029\n",
      "train loss:0.0002906306322305089\n",
      "train loss:0.003999199558900448\n",
      "train loss:0.00016036280506563267\n",
      "train loss:0.0008327927432142207\n",
      "train loss:0.001222874668891343\n",
      "train loss:0.0007946803408295079\n",
      "train loss:0.0055953635168126655\n",
      "train loss:0.00351906257597682\n",
      "train loss:0.005927024578413813\n",
      "train loss:0.0006520073186910167\n",
      "train loss:0.0014828074425392765\n",
      "train loss:0.00022851802556227992\n",
      "train loss:0.004010445760534659\n",
      "train loss:0.0006685341871053424\n",
      "train loss:0.00023007348756389875\n",
      "train loss:0.001247347106463682\n",
      "train loss:0.0034635752619773114\n",
      "train loss:6.127581127301689e-05\n",
      "train loss:0.0014057203120612543\n",
      "train loss:0.00010189311395309775\n",
      "train loss:0.0005885578940336988\n",
      "train loss:0.001490548232208384\n",
      "train loss:0.0016539029469085345\n",
      "train loss:0.0007820338374085908\n",
      "train loss:0.0014384958122558584\n",
      "train loss:0.0031339492245682694\n",
      "train loss:0.001386495209623492\n",
      "train loss:9.780740879525963e-05\n",
      "train loss:0.0021742761522812685\n",
      "train loss:0.00847460610679394\n",
      "train loss:0.0008369008289302701\n",
      "train loss:0.0039483332670724935\n",
      "train loss:0.002119986545673011\n",
      "train loss:0.009497866421435573\n",
      "train loss:7.576553374809665e-05\n",
      "train loss:0.0010794812926591532\n",
      "train loss:0.0004966263453869945\n",
      "train loss:0.0019455481690835452\n",
      "train loss:0.0003975544490634912\n",
      "train loss:0.00010608370219296264\n",
      "train loss:0.0008083144222649516\n",
      "train loss:0.0015468148616782603\n",
      "train loss:0.000360531904825565\n",
      "train loss:0.0016985414961521875\n",
      "train loss:0.002902339583599504\n",
      "train loss:0.00023881209417812196\n",
      "train loss:0.000565509513085849\n",
      "train loss:0.010017673273678765\n",
      "train loss:0.0009004684207370117\n",
      "train loss:0.0004667452226936191\n",
      "train loss:0.0012647024678612195\n",
      "train loss:0.0013321276256208232\n",
      "train loss:0.0027619176386312035\n",
      "train loss:1.4786690018323707e-05\n",
      "train loss:0.0002593257290170674\n",
      "train loss:0.001099703587338328\n",
      "train loss:0.00022408466075825764\n",
      "train loss:0.0003126731518871533\n",
      "train loss:0.00014205009086400752\n",
      "train loss:5.0178260859636375e-05\n",
      "train loss:0.0006465008569155706\n",
      "train loss:0.004893945336381016\n",
      "train loss:0.00158197191763631\n",
      "train loss:0.021703160127109627\n",
      "train loss:0.000698448004349922\n",
      "train loss:0.0011586542909371158\n",
      "train loss:0.003943615267931977\n",
      "train loss:0.0027116111880149214\n",
      "train loss:0.00020234964971903497\n",
      "train loss:0.0012735194328151616\n",
      "train loss:0.002481555583184695\n",
      "train loss:0.0033126275059550893\n",
      "train loss:0.0013997644026869632\n",
      "train loss:6.49546099201756e-05\n",
      "train loss:0.001624377174040471\n",
      "train loss:0.003359865748385108\n",
      "train loss:0.00035865599621873634\n",
      "train loss:6.921020113208573e-05\n",
      "train loss:0.0018930419210088684\n",
      "train loss:0.0010658043099663651\n",
      "train loss:0.00023160652383534677\n",
      "train loss:8.50337857532847e-05\n",
      "train loss:0.0006719238268991347\n",
      "train loss:0.003205403317743028\n",
      "train loss:0.0016914745129695805\n",
      "train loss:3.054837391897961e-05\n",
      "train loss:0.0003243110692156483\n",
      "train loss:0.00028893730053968533\n",
      "train loss:0.003782663350092177\n",
      "train loss:0.0007134167073040046\n",
      "train loss:0.005929845065003746\n",
      "train loss:0.0007403134840285037\n",
      "train loss:2.1605214296831413e-05\n",
      "train loss:0.002327570784305892\n",
      "train loss:0.0006985233213323124\n",
      "train loss:0.0011534219813977601\n",
      "train loss:0.00043403671210834235\n",
      "train loss:0.0033545021084052163\n",
      "train loss:5.5398972417033696e-05\n",
      "train loss:0.006175046504243589\n",
      "train loss:0.0002068307929574073\n",
      "train loss:0.009712583380431537\n",
      "train loss:0.0002857275207327153\n",
      "train loss:0.004730244936055871\n",
      "train loss:0.00022631518482855596\n",
      "train loss:0.0005615858433828718\n",
      "train loss:0.0031965831355309426\n",
      "train loss:0.006842734190970628\n",
      "train loss:0.000591271791330407\n",
      "train loss:0.0002829893480940106\n",
      "train loss:0.00011900960622476902\n",
      "train loss:0.000249695663524449\n",
      "train loss:0.0002302492765085391\n",
      "train loss:0.0005304289461607286\n",
      "train loss:0.0030774878142912467\n",
      "train loss:0.00015332998071436658\n",
      "train loss:0.0016514837587982153\n",
      "train loss:0.0001886800386741671\n",
      "train loss:0.004426704905612566\n",
      "train loss:0.013645989973791644\n",
      "train loss:0.0035206753995797586\n",
      "train loss:0.0001849642961349091\n",
      "train loss:0.0003467680050710605\n",
      "train loss:0.0031932232402512445\n",
      "train loss:0.0017037909588389164\n",
      "train loss:0.004115620760207512\n",
      "train loss:0.0022431792585926925\n",
      "train loss:0.0024532598710890886\n",
      "train loss:8.63076548780216e-05\n",
      "train loss:5.296629813260584e-05\n",
      "train loss:0.001614065409588865\n",
      "train loss:0.0023796991377851326\n",
      "train loss:0.00343949163229061\n",
      "train loss:0.0004353109774676647\n",
      "train loss:0.002518004362840656\n",
      "train loss:0.000207782799448709\n",
      "train loss:0.0006131218272433982\n",
      "train loss:0.0006686892787241654\n",
      "train loss:0.00018148550447133813\n",
      "train loss:0.0011421171142323254\n",
      "train loss:0.0012537858586715472\n",
      "train loss:0.0025538626241080574\n",
      "train loss:0.0056944577635824585\n",
      "train loss:0.001695887261397701\n",
      "train loss:0.014881241077631146\n",
      "train loss:0.0016853483681505616\n",
      "train loss:0.0009907158775345883\n",
      "train loss:0.005289681873957067\n",
      "train loss:0.00026632176281657995\n",
      "train loss:0.00020799028126841922\n",
      "train loss:0.0016257514411144643\n",
      "train loss:5.070005339983366e-05\n",
      "train loss:0.00024204897263950726\n",
      "train loss:0.0006212817718053772\n",
      "train loss:0.0025361432280749776\n",
      "train loss:0.0019876510152086867\n",
      "train loss:0.0017466943497030968\n",
      "train loss:0.00017667216440482987\n",
      "train loss:0.00033402545474467594\n",
      "train loss:0.00017688163668333302\n",
      "train loss:0.0025121927431098795\n",
      "train loss:0.005685299655676417\n",
      "train loss:0.0003162876791588476\n",
      "train loss:0.0002974024838836452\n",
      "train loss:0.0008784091634764513\n",
      "train loss:0.0001784783637636024\n",
      "train loss:0.0006123173429839397\n",
      "train loss:0.004302618811638358\n",
      "train loss:0.0001032258711497519\n",
      "train loss:0.00016732013490882713\n",
      "train loss:0.007852773619719032\n",
      "train loss:0.000409571677537617\n",
      "train loss:0.004470799526351317\n",
      "train loss:0.00020516398824560743\n",
      "train loss:0.0007204368187465542\n",
      "train loss:0.00034687576331658915\n",
      "train loss:0.001404820790928651\n",
      "train loss:0.0005117726957296234\n",
      "train loss:0.0002291598518477862\n",
      "train loss:0.0012840512646010713\n",
      "train loss:0.00017441404836435957\n",
      "train loss:0.003934398822309895\n",
      "train loss:0.0005839632994718048\n",
      "train loss:0.001640270061697997\n",
      "train loss:0.0032000202937515192\n",
      "train loss:0.0025350780331944\n",
      "train loss:0.0006277248451281095\n",
      "train loss:0.0045553949779979855\n",
      "train loss:0.0003337589756102345\n",
      "train loss:0.0019988533769147157\n",
      "train loss:0.00018784701611319568\n",
      "train loss:0.0017150076900862776\n",
      "train loss:0.0010726983545563127\n",
      "train loss:1.3170817476406215e-05\n",
      "train loss:0.006639410201115914\n",
      "train loss:0.01540984208988356\n",
      "train loss:0.001436317727548486\n",
      "train loss:0.00018710277756082947\n",
      "train loss:0.0019059500964080664\n",
      "train loss:0.003053250150460809\n",
      "train loss:0.0019311586323794549\n",
      "train loss:0.00263607234062741\n",
      "train loss:0.0017217053329434805\n",
      "train loss:0.00010359206061990099\n",
      "train loss:0.00017614979728393013\n",
      "train loss:0.0005808593428334266\n",
      "train loss:0.00015045204022524358\n",
      "train loss:0.0024773556317028086\n",
      "train loss:0.002269321651352682\n",
      "train loss:0.00012402538614795779\n",
      "train loss:0.0005444408627872574\n",
      "train loss:0.01827259364865033\n",
      "train loss:7.82382234931199e-05\n",
      "train loss:0.00030057942754246664\n",
      "train loss:0.0007604786987927128\n",
      "train loss:0.0018974940591555607\n",
      "train loss:0.0009704159008413573\n",
      "train loss:0.000144801630730547\n",
      "train loss:9.400607994132803e-05\n",
      "train loss:0.00023041356543539526\n",
      "train loss:0.0023407176939478237\n",
      "train loss:0.0027947288896196048\n",
      "train loss:0.0001936944585325404\n",
      "train loss:0.003824227128590727\n",
      "train loss:0.0014982616398864306\n",
      "train loss:0.001194245065930086\n",
      "train loss:0.0016346529990319805\n",
      "train loss:0.00031859074604609876\n",
      "train loss:3.057919477031711e-05\n",
      "train loss:0.00013659710187062226\n",
      "train loss:0.0019797054475615577\n",
      "train loss:0.00015913631679374842\n",
      "train loss:0.001215470249202576\n",
      "train loss:0.001293230506539062\n",
      "train loss:0.0011246776944035463\n",
      "train loss:3.361016936843361e-05\n",
      "train loss:0.00019681280445472176\n",
      "train loss:0.0011293979587127273\n",
      "train loss:0.0012266571989369638\n",
      "train loss:0.002607237441205915\n",
      "train loss:0.0003214330212447535\n",
      "train loss:0.0022424281835886554\n",
      "train loss:0.0033131634293438257\n",
      "train loss:0.0007039476125843208\n",
      "train loss:0.0015008836177586372\n",
      "train loss:0.0011805714107139054\n",
      "train loss:0.0035336768084533606\n",
      "train loss:0.0008993338330664638\n",
      "train loss:0.0034235520634794524\n",
      "train loss:0.0006224429230156615\n",
      "train loss:0.0002796737744744214\n",
      "train loss:0.0016313300373716297\n",
      "train loss:0.000287555091790046\n",
      "train loss:0.0007139380259194453\n",
      "train loss:0.0011193035208145435\n",
      "train loss:0.00426797248825982\n",
      "train loss:3.165811104246762e-05\n",
      "train loss:0.00012103456404943745\n",
      "train loss:0.0004469820845575886\n",
      "train loss:0.000914028797618069\n",
      "train loss:9.412535852921096e-05\n",
      "train loss:0.0010829879577235163\n",
      "train loss:0.0004152895588394737\n",
      "train loss:0.0044323149526892605\n",
      "train loss:0.005508454885849565\n",
      "train loss:0.0023333024202387464\n",
      "train loss:0.0005826866519468913\n",
      "train loss:0.0019891914370725647\n",
      "train loss:0.0015225339559982506\n",
      "train loss:0.00028410256518070806\n",
      "train loss:0.0032961841331840386\n",
      "train loss:0.0017133048845807259\n",
      "train loss:0.002569672932329147\n",
      "train loss:0.002655373693186312\n",
      "train loss:6.449832656532206e-05\n",
      "train loss:0.00012190707565689588\n",
      "train loss:0.002214727120528377\n",
      "train loss:4.330328237414882e-05\n",
      "train loss:0.002470950489676813\n",
      "train loss:0.0011428919689195815\n",
      "train loss:0.0004484512222178539\n",
      "train loss:0.0015685441807154039\n",
      "train loss:0.00015646706872199926\n",
      "train loss:0.001977604290510566\n",
      "train loss:5.5193463914561536e-05\n",
      "train loss:0.0004091629252465585\n",
      "train loss:0.0012470891267944283\n",
      "train loss:0.001219112199571423\n",
      "train loss:0.00010021999899048712\n",
      "train loss:0.0008404083442634481\n",
      "train loss:0.0020463102437349705\n",
      "train loss:0.01551049221162366\n",
      "train loss:0.0005733600236178119\n",
      "train loss:0.0028145666644372493\n",
      "train loss:0.00022605154251519082\n",
      "train loss:0.0026567602841737013\n",
      "train loss:0.0007566806318702214\n",
      "train loss:0.00032370751201689206\n",
      "train loss:0.000321576362900672\n",
      "train loss:0.00033617284929754113\n",
      "train loss:0.000807430095950588\n",
      "train loss:0.001274618239837284\n",
      "train loss:0.0006858994048250122\n",
      "train loss:3.572672275290249e-05\n",
      "train loss:0.0012446420199154827\n",
      "train loss:0.0031400141049230603\n",
      "=== epoch:19, train acc:0.999, test acc:0.987 ===\n",
      "train loss:0.002140413741912883\n",
      "train loss:0.0006591076165297656\n",
      "train loss:0.0004422088179657225\n",
      "train loss:0.0017232990658940638\n",
      "train loss:0.00039989227826063236\n",
      "train loss:0.0035965344515562865\n",
      "train loss:0.0001685838802476278\n",
      "train loss:0.0006985305566661813\n",
      "train loss:6.76605491396943e-05\n",
      "train loss:5.065908604866802e-05\n",
      "train loss:0.00025957440789202603\n",
      "train loss:0.00042625160238287925\n",
      "train loss:0.0001443531798546669\n",
      "train loss:0.0010824367425709173\n",
      "train loss:0.0009626864817760129\n",
      "train loss:0.0030743079564639346\n",
      "train loss:0.000777459041009326\n",
      "train loss:0.00044291299537252296\n",
      "train loss:0.0005971719481083453\n",
      "train loss:0.0006340698758347775\n",
      "train loss:0.00019019232502098818\n",
      "train loss:0.0021457491973976527\n",
      "train loss:0.00021381110734527604\n",
      "train loss:0.0004884693435177578\n",
      "train loss:0.00017121162518783577\n",
      "train loss:0.0007532245725344423\n",
      "train loss:0.0008160791351979148\n",
      "train loss:0.0005165575653758907\n",
      "train loss:0.0009463641276040035\n",
      "train loss:0.0006023781931771001\n",
      "train loss:0.0002740371275476827\n",
      "train loss:0.0003934530310892839\n",
      "train loss:0.0007993919772328682\n",
      "train loss:0.00037576183190216847\n",
      "train loss:0.00040575272395492926\n",
      "train loss:0.0017960198191709444\n",
      "train loss:0.002128362634221745\n",
      "train loss:0.0011003465652925928\n",
      "train loss:0.0003467176260097189\n",
      "train loss:0.0005314455335635592\n",
      "train loss:0.00026412436030055287\n",
      "train loss:0.0007487446356202454\n",
      "train loss:0.004537060116190406\n",
      "train loss:0.0002757905963725477\n",
      "train loss:0.0007955392492811101\n",
      "train loss:0.0011941040819820736\n",
      "train loss:0.00011352314816731559\n",
      "train loss:0.0012322049354247225\n",
      "train loss:0.00040120122088720106\n",
      "train loss:0.00015579623270544704\n",
      "train loss:0.0002052975025743217\n",
      "train loss:0.003264130316972698\n",
      "train loss:0.0013852081620539583\n",
      "train loss:0.0007601555986625347\n",
      "train loss:0.0003892119949305761\n",
      "train loss:0.0016780968280851313\n",
      "train loss:0.0011370376279041658\n",
      "train loss:0.00012655520183346952\n",
      "train loss:0.0006351860858371295\n",
      "train loss:0.00026314659913130654\n",
      "train loss:0.00021831166931284674\n",
      "train loss:0.0019660326727570323\n",
      "train loss:0.0012862564576386628\n",
      "train loss:0.0017222003559592641\n",
      "train loss:0.0012622513719251444\n",
      "train loss:0.0017920097462828485\n",
      "train loss:0.00047323444945020015\n",
      "train loss:0.002717076190219653\n",
      "train loss:0.0005503639368422181\n",
      "train loss:0.0010263399675253292\n",
      "train loss:0.0016698807517659278\n",
      "train loss:1.601051854213995e-05\n",
      "train loss:0.0021448909043349615\n",
      "train loss:0.0030241918275719644\n",
      "train loss:0.0008374270585966157\n",
      "train loss:0.0004195396957456538\n",
      "train loss:0.000199681018549562\n",
      "train loss:0.0003088481924609996\n",
      "train loss:0.000864543372114389\n",
      "train loss:0.003600370515909388\n",
      "train loss:0.0019872852174814925\n",
      "train loss:0.00022687419272110588\n",
      "train loss:0.001731788212135544\n",
      "train loss:0.002804141968303626\n",
      "train loss:0.0027426344443076366\n",
      "train loss:0.00023555911892556188\n",
      "train loss:4.2225349260176616e-05\n",
      "train loss:0.00035245627461585134\n",
      "train loss:0.0013780144545801247\n",
      "train loss:0.0005516377261366579\n",
      "train loss:0.0009106314153317243\n",
      "train loss:0.0004274281658349716\n",
      "train loss:0.02436882712999442\n",
      "train loss:0.0022792555693740514\n",
      "train loss:0.0008761692991413325\n",
      "train loss:0.00044964505929678144\n",
      "train loss:0.006544361561033768\n",
      "train loss:0.0014149344180124823\n",
      "train loss:0.00011931703955597543\n",
      "train loss:0.0006472323327605806\n",
      "train loss:0.001457100368528071\n",
      "train loss:0.0009371456652066118\n",
      "train loss:0.001101694039907369\n",
      "train loss:0.0002615650798423369\n",
      "train loss:0.00030036182814222864\n",
      "train loss:0.007224682011784005\n",
      "train loss:0.002504976160792856\n",
      "train loss:0.0005941716016978281\n",
      "train loss:0.00020178960501730596\n",
      "train loss:4.881436137938037e-05\n",
      "train loss:1.4302532335290914e-05\n",
      "train loss:0.0010560969820705372\n",
      "train loss:0.00032311214820191886\n",
      "train loss:0.00016351961105502352\n",
      "train loss:0.0016308392200207237\n",
      "train loss:0.0005764767975775823\n",
      "train loss:0.0027784136466346824\n",
      "train loss:0.0019073236344722645\n",
      "train loss:0.0021862848755447497\n",
      "train loss:0.005329785882741266\n",
      "train loss:0.00025151260608350507\n",
      "train loss:0.003213622145031381\n",
      "train loss:0.0004974002508377608\n",
      "train loss:4.833922765731784e-05\n",
      "train loss:0.001201540281559437\n",
      "train loss:6.170503148801003e-05\n",
      "train loss:0.002295919829271651\n",
      "train loss:0.0013258849052031228\n",
      "train loss:0.0004206950617590872\n",
      "train loss:0.0003246691253684722\n",
      "train loss:0.0007011393504246991\n",
      "train loss:0.03423645498956167\n",
      "train loss:0.00043644790537158626\n",
      "train loss:0.0011596150024266343\n",
      "train loss:0.0002830942287192217\n",
      "train loss:0.0006177099543023069\n",
      "train loss:0.0003170977126589903\n",
      "train loss:0.00019263359464162047\n",
      "train loss:0.0010738406509919003\n",
      "train loss:0.0006764531937577916\n",
      "train loss:4.6947089796106846e-05\n",
      "train loss:0.0007774186628905956\n",
      "train loss:0.00027047332113756063\n",
      "train loss:0.002383714401387167\n",
      "train loss:0.0006035071720084615\n",
      "train loss:0.0008831056368360407\n",
      "train loss:0.00012913810072839632\n",
      "train loss:0.0014751512677467563\n",
      "train loss:0.0001947494402944963\n",
      "train loss:0.0003482846687910052\n",
      "train loss:0.0001555308152480296\n",
      "train loss:0.0012031826263781333\n",
      "train loss:0.00011004827654208028\n",
      "train loss:0.0010128860406061332\n",
      "train loss:0.0066040969368198225\n",
      "train loss:0.0010493695929463247\n",
      "train loss:0.0005169722929858178\n",
      "train loss:0.010466980950125302\n",
      "train loss:0.0002899622623815195\n",
      "train loss:0.002380500673896392\n",
      "train loss:0.003882865536923128\n",
      "train loss:0.0006802907396370381\n",
      "train loss:0.006784804432802921\n",
      "train loss:0.00031097206436585473\n",
      "train loss:0.0018308985053295643\n",
      "train loss:0.001415457013296934\n",
      "train loss:2.645870218538594e-05\n",
      "train loss:0.0013867792518466465\n",
      "train loss:0.0001768977955427468\n",
      "train loss:0.005167562054448703\n",
      "train loss:0.00018220983590461337\n",
      "train loss:0.003014634030255665\n",
      "train loss:0.005590282798223427\n",
      "train loss:0.0038176988019387936\n",
      "train loss:0.0012891599724314674\n",
      "train loss:0.0002770306470224868\n",
      "train loss:0.00038635973247233085\n",
      "train loss:0.00028530157941561554\n",
      "train loss:0.001238452883379027\n",
      "train loss:0.0012045286630517594\n",
      "train loss:0.004060334183999752\n",
      "train loss:0.001453011674569309\n",
      "train loss:0.0012515685190809117\n",
      "train loss:0.0022967254198578094\n",
      "train loss:0.00026778864563178515\n",
      "train loss:0.0028908373324522158\n",
      "train loss:0.0005447253321059129\n",
      "train loss:0.0008915082319834415\n",
      "train loss:0.05757019016749073\n",
      "train loss:0.0006056378841141808\n",
      "train loss:0.00013484179423840233\n",
      "train loss:0.0015200665731194296\n",
      "train loss:0.008534986142933187\n",
      "train loss:0.0030620117718495117\n",
      "train loss:0.01724797170606922\n",
      "train loss:0.003292511898324044\n",
      "train loss:0.00034212002273091487\n",
      "train loss:0.0001496546319075477\n",
      "train loss:0.0021162104361088976\n",
      "train loss:0.0003829026152163183\n",
      "train loss:0.0034509260452477908\n",
      "train loss:0.01114529011445143\n",
      "train loss:0.001015313526049421\n",
      "train loss:0.0020665448417923273\n",
      "train loss:0.03862506141432224\n",
      "train loss:0.013842868696410634\n",
      "train loss:0.0009060628118225234\n",
      "train loss:0.0032033803115992555\n",
      "train loss:0.0040165215256621305\n",
      "train loss:0.0002933172334070413\n",
      "train loss:0.005671324066416209\n",
      "train loss:0.008617335437518962\n",
      "train loss:0.0001144697338865712\n",
      "train loss:0.010981991491884413\n",
      "train loss:0.005076663845026212\n",
      "train loss:0.0035499998183030827\n",
      "train loss:0.0014281495071168959\n",
      "train loss:0.00028988041414806806\n",
      "train loss:0.0020519033017240545\n",
      "train loss:0.0020573088308604728\n",
      "train loss:0.0017513895358740417\n",
      "train loss:0.058451875286983965\n",
      "train loss:0.0026575254835756435\n",
      "train loss:0.004223527503921722\n",
      "train loss:0.004342291998725574\n",
      "train loss:0.002094948788905288\n",
      "train loss:0.0014651548718440162\n",
      "train loss:9.625421722518897e-05\n",
      "train loss:0.0018155574926525828\n",
      "train loss:0.007989325893867525\n",
      "train loss:0.010649514536631875\n",
      "train loss:0.0015749511983561876\n",
      "train loss:0.0060059770865170635\n",
      "train loss:0.0016144846774629033\n",
      "train loss:0.0024382813027641653\n",
      "train loss:0.023529950279410943\n",
      "train loss:0.0016862174775663294\n",
      "train loss:0.0004204400083441975\n",
      "train loss:0.004431945418513008\n",
      "train loss:9.560958134645383e-05\n",
      "train loss:0.00018732355881435225\n",
      "train loss:0.0021558620549682076\n",
      "train loss:0.0004557635146932978\n",
      "train loss:0.0004696220961111558\n",
      "train loss:8.290806390027406e-05\n",
      "train loss:0.0012427708573743569\n",
      "train loss:0.004413866996281582\n",
      "train loss:0.006327194193227893\n",
      "train loss:0.00036289522151944636\n",
      "train loss:0.0013413033981902014\n",
      "train loss:0.003354237740490437\n",
      "train loss:0.002441929259828518\n",
      "train loss:0.0017799819703075489\n",
      "train loss:0.0013062241503304223\n",
      "train loss:0.0004978403673457888\n",
      "train loss:0.0052236576418618055\n",
      "train loss:0.0011511885555299747\n",
      "train loss:0.0006640240208121538\n",
      "train loss:0.0008955581947775038\n",
      "train loss:0.0011162063109894553\n",
      "train loss:0.006261599147907418\n",
      "train loss:0.00011144146231658874\n",
      "train loss:0.00015731634098844094\n",
      "train loss:0.0015230471791869973\n",
      "train loss:0.0019166830671542164\n",
      "train loss:0.0007655698782134673\n",
      "train loss:0.001255134179664987\n",
      "train loss:0.0006856429084808294\n",
      "train loss:0.001671626434276502\n",
      "train loss:0.0046380884737454355\n",
      "train loss:0.004276091215293456\n",
      "train loss:0.002302806009027343\n",
      "train loss:0.0017198391812799634\n",
      "train loss:0.002595255802566116\n",
      "train loss:0.0023723704815325724\n",
      "train loss:0.0005966975954328172\n",
      "train loss:0.011224046165976592\n",
      "train loss:4.399381551193866e-05\n",
      "train loss:0.0007764795776477629\n",
      "train loss:0.002955058855788024\n",
      "train loss:0.00015171180679820397\n",
      "train loss:0.000104429245786511\n",
      "train loss:9.482408307502415e-05\n",
      "train loss:0.0020170160969784955\n",
      "train loss:0.00429618423110477\n",
      "train loss:0.0029440318400933752\n",
      "train loss:0.0006550360177048993\n",
      "train loss:0.00014596052575864364\n",
      "train loss:0.004533962932605488\n",
      "train loss:0.0005002724821332365\n",
      "train loss:0.00034928187821214006\n",
      "train loss:0.0028651246603217774\n",
      "train loss:0.0023106051958713524\n",
      "train loss:0.0006125393374764259\n",
      "train loss:0.000523466979845361\n",
      "train loss:0.0006157679252846551\n",
      "train loss:0.0032990455234812865\n",
      "train loss:3.264694270387893e-05\n",
      "train loss:0.0024133207059639393\n",
      "train loss:0.0007286574474887093\n",
      "train loss:0.0011287659904421665\n",
      "train loss:0.0011207886974037686\n",
      "train loss:0.0026258072411219115\n",
      "train loss:0.0013731865847639675\n",
      "train loss:0.0015367597820073948\n",
      "train loss:0.00140669254134339\n",
      "train loss:0.0009752449625107429\n",
      "train loss:0.0007340830384199781\n",
      "train loss:3.276923736879558e-05\n",
      "train loss:0.000511630858160602\n",
      "train loss:0.0043213848373439235\n",
      "train loss:0.00036452352967109426\n",
      "train loss:0.00040496454238028056\n",
      "train loss:0.0004383217554960147\n",
      "train loss:0.003802315473024775\n",
      "train loss:0.0001830443549336442\n",
      "train loss:0.00034854147163579174\n",
      "train loss:0.019052737905650333\n",
      "train loss:0.00035429889896727894\n",
      "train loss:0.0006026332433743676\n",
      "train loss:0.00013172260058451403\n",
      "train loss:0.0015929476922670683\n",
      "train loss:0.0013322673205349545\n",
      "train loss:0.0004966161335283005\n",
      "train loss:0.0007995226805953996\n",
      "train loss:0.0018862799210814212\n",
      "train loss:0.0013508802391309833\n",
      "train loss:0.0002940220946391134\n",
      "train loss:0.001843587058862044\n",
      "train loss:0.003427057313905876\n",
      "train loss:0.0007104010016280205\n",
      "train loss:0.0005357372765087208\n",
      "train loss:0.0007365060013601419\n",
      "train loss:0.0021818547793206384\n",
      "train loss:0.0006686254190352734\n",
      "train loss:0.0004095190502619609\n",
      "train loss:0.0007099905158610565\n",
      "train loss:0.0004692768463815884\n",
      "train loss:0.0002723011989183694\n",
      "train loss:0.0038410757921032153\n",
      "train loss:0.0021836352552015294\n",
      "train loss:0.002669292826875359\n",
      "train loss:0.001644056885211735\n",
      "train loss:0.0006042527515586969\n",
      "train loss:0.00147254240080451\n",
      "train loss:0.002260039110624078\n",
      "train loss:0.00022167968302925524\n",
      "train loss:0.0011168575015193247\n",
      "train loss:1.5824845098906832e-05\n",
      "train loss:0.009834653757062838\n",
      "train loss:0.00016315429735253852\n",
      "train loss:0.00013296117728409785\n",
      "train loss:0.0022827050906237055\n",
      "train loss:0.0011066735654745236\n",
      "train loss:0.0016583656013939006\n",
      "train loss:0.00019594352033659702\n",
      "train loss:0.001710275748457821\n",
      "train loss:0.0011594258038992579\n",
      "train loss:0.0013646417180118928\n",
      "train loss:2.9564345892681762e-05\n",
      "train loss:0.00017838464486539933\n",
      "train loss:0.0024612824629754253\n",
      "train loss:0.001292936772458004\n",
      "train loss:0.0005076324272406834\n",
      "train loss:0.0015307630837987174\n",
      "train loss:0.0004340626838030267\n",
      "train loss:0.00967344524951269\n",
      "train loss:0.0004389584130648724\n",
      "train loss:0.0004234358650318971\n",
      "train loss:0.00329683509743556\n",
      "train loss:9.694765093541342e-05\n",
      "train loss:0.00015461125164599227\n",
      "train loss:0.0016277488710421508\n",
      "train loss:8.423932136108011e-05\n",
      "train loss:0.0007974271374248919\n",
      "train loss:0.0036170401634887145\n",
      "train loss:0.0048815535642841865\n",
      "train loss:0.0004261399710465049\n",
      "train loss:0.0019717388222484835\n",
      "train loss:0.0016622750219834048\n",
      "train loss:0.0001696234299396249\n",
      "train loss:0.00044862136434889574\n",
      "train loss:0.001973521131010259\n",
      "train loss:3.231714672862053e-05\n",
      "train loss:0.00021415434151229838\n",
      "train loss:0.0005252445780323346\n",
      "train loss:0.0017180364988890434\n",
      "train loss:0.0005203081135394173\n",
      "train loss:0.005838451344614842\n",
      "train loss:0.002883996086200188\n",
      "train loss:0.0007936736921407294\n",
      "train loss:0.0067762328177800365\n",
      "train loss:0.0019598951351378286\n",
      "train loss:0.0009327505772911494\n",
      "train loss:0.000491147203715731\n",
      "train loss:0.00016056557916029228\n",
      "train loss:0.0007071665997395002\n",
      "train loss:0.00042404685776060676\n",
      "train loss:0.0003095477948977104\n",
      "train loss:0.0006817466685868735\n",
      "train loss:0.001119326887923273\n",
      "train loss:0.002569230422970939\n",
      "train loss:3.404973465137966e-05\n",
      "train loss:0.0006813053830427251\n",
      "train loss:0.004493109623304234\n",
      "train loss:0.0031367511783506026\n",
      "train loss:0.0054527201976819015\n",
      "train loss:0.0002810069307011972\n",
      "train loss:0.0006193367855912295\n",
      "train loss:0.004249596155902219\n",
      "train loss:0.004017014976312326\n",
      "train loss:0.0007939209043617991\n",
      "train loss:0.0008499511140212332\n",
      "train loss:0.000739924402691953\n",
      "train loss:0.00010348291309163767\n",
      "train loss:0.0005644886929586164\n",
      "train loss:0.0015089015452028236\n",
      "train loss:0.0007354145450101433\n",
      "train loss:0.00030531157352100397\n",
      "train loss:0.0023574641792755043\n",
      "train loss:0.0019152243921210373\n",
      "train loss:0.0009983677043263514\n",
      "train loss:0.0013318453627773467\n",
      "train loss:6.505919205393371e-05\n",
      "train loss:0.0006166591062577954\n",
      "train loss:0.00024569198174831787\n",
      "train loss:7.018215484651103e-05\n",
      "train loss:0.00019119076671596322\n",
      "train loss:0.0025671697743688635\n",
      "train loss:0.004147402214299562\n",
      "train loss:2.057557779983447e-05\n",
      "train loss:6.206429750922672e-05\n",
      "train loss:0.004288883878841362\n",
      "train loss:0.0005411594237111285\n",
      "train loss:0.0004319573383736621\n",
      "train loss:0.00016158006627073836\n",
      "train loss:0.0014509554688630396\n",
      "train loss:0.004506099045248461\n",
      "train loss:0.0049025353435129745\n",
      "train loss:9.196715923938033e-05\n",
      "train loss:0.0005938193692199566\n",
      "train loss:0.0017077141169817927\n",
      "train loss:0.0009601913552083762\n",
      "train loss:0.0006023456747949317\n",
      "train loss:0.0009690556105579062\n",
      "train loss:0.0020412955455493043\n",
      "train loss:3.289962730345399e-05\n",
      "train loss:0.0008729510977999574\n",
      "train loss:0.004703131443733261\n",
      "train loss:0.0025703461101406\n",
      "train loss:0.00031879872593622895\n",
      "train loss:0.00017576926784626005\n",
      "train loss:0.00115544181099206\n",
      "train loss:0.0034422102504817332\n",
      "train loss:0.0006500566132147287\n",
      "train loss:9.014843908803642e-05\n",
      "train loss:0.002774167726462543\n",
      "train loss:0.0003741820627249924\n",
      "train loss:0.001144307709709834\n",
      "train loss:0.001737025613019402\n",
      "train loss:0.00045228956572225954\n",
      "train loss:0.0004919050711333313\n",
      "train loss:0.0003302878101795054\n",
      "train loss:0.0009081383708282568\n",
      "train loss:0.00026173852049422077\n",
      "train loss:0.002577174139194193\n",
      "train loss:0.0016393597742181698\n",
      "train loss:0.000197639198968487\n",
      "train loss:0.0011015830155793249\n",
      "train loss:0.002754852356218501\n",
      "train loss:7.180614750090115e-05\n",
      "train loss:0.00023265874932563825\n",
      "train loss:9.907486242616261e-05\n",
      "train loss:0.0004292517722166019\n",
      "train loss:0.0003236068183190486\n",
      "train loss:0.0017460467814272393\n",
      "train loss:0.0002205522566993184\n",
      "train loss:6.379502037036186e-05\n",
      "train loss:0.0005353129299103945\n",
      "train loss:0.0036021118319620456\n",
      "train loss:0.0004863138679139762\n",
      "train loss:0.00014139201175330477\n",
      "train loss:0.00011620313345618971\n",
      "train loss:0.0002962098513812652\n",
      "train loss:0.000179151169606485\n",
      "train loss:0.0026983380448993605\n",
      "train loss:0.00038510196926990197\n",
      "train loss:0.0005029035323910807\n",
      "train loss:0.017189856996607755\n",
      "train loss:0.001390778766554302\n",
      "train loss:0.0007683925473263079\n",
      "train loss:0.0003032372960500216\n",
      "train loss:5.7205679380856746e-05\n",
      "train loss:0.0014885554981991383\n",
      "train loss:0.009726825547360677\n",
      "train loss:0.0002008614829748626\n",
      "train loss:0.0031035513879610127\n",
      "train loss:0.00019380918426665104\n",
      "train loss:0.00036608518311207763\n",
      "train loss:0.0005945409690245104\n",
      "train loss:0.00033778893375712544\n",
      "train loss:0.0013568756401368886\n",
      "train loss:0.0028297699571669965\n",
      "train loss:0.00017567270288315522\n",
      "train loss:0.002060449079182158\n",
      "train loss:0.0009154129935807056\n",
      "train loss:0.003657783009818078\n",
      "train loss:0.00016865370337731969\n",
      "train loss:0.0005553979011242865\n",
      "train loss:0.00043983654850359517\n",
      "train loss:0.005860656245759376\n",
      "train loss:0.0028422356548874973\n",
      "train loss:0.00025269105600685327\n",
      "train loss:0.00025297290372131096\n",
      "train loss:0.0009114167291685854\n",
      "train loss:0.00033230351434219415\n",
      "train loss:5.3335370740719195e-05\n",
      "train loss:0.0008767097891229478\n",
      "train loss:0.0008209463237248194\n",
      "train loss:4.8202826877110914e-05\n",
      "train loss:0.0016546644305655389\n",
      "train loss:0.0031365087399883025\n",
      "train loss:8.484350863186217e-05\n",
      "train loss:0.000177787151980233\n",
      "train loss:0.00013500789450310665\n",
      "train loss:0.0007836753382826976\n",
      "train loss:0.0009951164762172003\n",
      "train loss:0.0011017308436543857\n",
      "train loss:0.006175681198977889\n",
      "train loss:8.267478233270886e-05\n",
      "train loss:0.0002043460837158295\n",
      "train loss:0.0005618069406008656\n",
      "train loss:0.0018033794369618683\n",
      "train loss:0.0021117082796913004\n",
      "train loss:0.0010433939501371073\n",
      "train loss:0.0005407419667408927\n",
      "train loss:0.006780787518552242\n",
      "train loss:7.792468002608877e-05\n",
      "train loss:0.012107334088707858\n",
      "train loss:0.00033439634130076406\n",
      "train loss:0.002240383355845757\n",
      "train loss:0.0038562358691498723\n",
      "train loss:0.00013051476509120995\n",
      "train loss:0.0017900242827769898\n",
      "train loss:0.00014867931631930436\n",
      "train loss:0.00037462127659135524\n",
      "train loss:0.004018985582146362\n",
      "train loss:0.0002809761857879271\n",
      "train loss:0.001418456530538874\n",
      "train loss:0.0014071163400787713\n",
      "train loss:0.0006135610332477924\n",
      "train loss:0.0015975157287060617\n",
      "train loss:0.0009564501986136128\n",
      "train loss:0.0011439374475522206\n",
      "train loss:0.002656596610961987\n",
      "train loss:0.0006298659478946823\n",
      "train loss:0.002066057580360518\n",
      "train loss:0.007882741702194392\n",
      "train loss:5.035014651473406e-05\n",
      "train loss:0.001856820377426621\n",
      "train loss:0.0001150610708773367\n",
      "train loss:0.0002754001911913571\n",
      "train loss:0.00020212425224459352\n",
      "train loss:0.00023576387110920316\n",
      "train loss:0.0009094852653960684\n",
      "train loss:0.0011905289630884692\n",
      "train loss:0.0015800817953921156\n",
      "train loss:0.00019020365488015687\n",
      "train loss:0.00023675382721027746\n",
      "train loss:0.00043962446304034717\n",
      "train loss:0.002331279695155706\n",
      "train loss:0.008523209866498517\n",
      "train loss:0.004580916312728661\n",
      "train loss:0.0021257727333619443\n",
      "train loss:0.005378375295947373\n",
      "train loss:0.05157635584313868\n",
      "train loss:0.001536239782796645\n",
      "train loss:0.0011431853954930002\n",
      "train loss:0.00262190390234924\n",
      "train loss:0.0002946371520581314\n",
      "train loss:0.0005110501871213924\n",
      "train loss:0.009444616289324635\n",
      "train loss:0.0010567906900747312\n",
      "train loss:0.0024043836460286767\n",
      "train loss:0.00464570768902604\n",
      "train loss:0.0066619114694173535\n",
      "train loss:0.003029686355850783\n",
      "train loss:0.00018339251484362652\n",
      "train loss:0.0015690567623845855\n",
      "train loss:0.0003818521973374811\n",
      "train loss:0.004075812062447578\n",
      "train loss:0.007707018746624407\n",
      "train loss:0.0006580237680243956\n",
      "train loss:0.0011926392560379375\n",
      "train loss:0.0007332555848469725\n",
      "train loss:0.0007596743019132606\n",
      "train loss:0.0008694403642044954\n",
      "train loss:0.0026008900364995267\n",
      "train loss:0.010046918182517461\n",
      "train loss:0.0009485971730099235\n",
      "=== epoch:20, train acc:0.997, test acc:0.987 ===\n",
      "train loss:0.0029466648048819184\n",
      "train loss:0.0032561895592100078\n",
      "train loss:0.0006035438508788853\n",
      "train loss:0.0010623426716889976\n",
      "train loss:3.947737871670982e-05\n",
      "train loss:0.00041124437382780834\n",
      "train loss:0.0006780152419995222\n",
      "train loss:0.0018137882053212212\n",
      "train loss:0.002098083465384832\n",
      "train loss:0.005281123917334525\n",
      "train loss:0.0015450788225620051\n",
      "train loss:0.002427577340061971\n",
      "train loss:0.0006395923923759137\n",
      "train loss:0.001739260086520346\n",
      "train loss:0.0009040808935952835\n",
      "train loss:0.0005469624097943272\n",
      "train loss:0.004305627041714779\n",
      "train loss:0.004298983948313459\n",
      "train loss:0.00033827338912048073\n",
      "train loss:6.31843021260701e-05\n",
      "train loss:0.0038223135366441717\n",
      "train loss:0.0018044768186374967\n",
      "train loss:0.0008558037795055739\n",
      "train loss:0.003090968276336159\n",
      "train loss:0.0014816687945005294\n",
      "train loss:3.7776611243463687e-05\n",
      "train loss:0.009576880123833753\n",
      "train loss:0.0015468241739437078\n",
      "train loss:0.0010630840668159611\n",
      "train loss:0.0001262195677542456\n",
      "train loss:0.0003782635320558711\n",
      "train loss:0.0005563081812874904\n",
      "train loss:9.203096234536396e-05\n",
      "train loss:0.0003386819498729929\n",
      "train loss:0.00461093564553163\n",
      "train loss:0.003052776741145509\n",
      "train loss:0.00016444812536033204\n",
      "train loss:0.0008382867418858461\n",
      "train loss:0.002309187289637111\n",
      "train loss:0.0007278652628244796\n",
      "train loss:0.001037036763569351\n",
      "train loss:0.0038272005753828835\n",
      "train loss:0.0033469619280501518\n",
      "train loss:0.0012192046106985046\n",
      "train loss:0.0010154278591745082\n",
      "train loss:0.0007863047008871966\n",
      "train loss:0.0005753125250774378\n",
      "train loss:0.00015495820272019622\n",
      "train loss:0.0019680510678821636\n",
      "train loss:0.0004784483731508275\n",
      "train loss:0.0003061492386738927\n",
      "train loss:0.0007926924289084127\n",
      "train loss:0.00034818835365534105\n",
      "train loss:0.0004073658860561432\n",
      "train loss:0.00025393595299460375\n",
      "train loss:1.2847848767950753e-05\n",
      "train loss:7.276284522804992e-05\n",
      "train loss:0.000314517867611918\n",
      "train loss:0.005388471457449937\n",
      "train loss:0.0009357952151046841\n",
      "train loss:0.0001566897667074272\n",
      "train loss:0.0014184790885865021\n",
      "train loss:0.0015483339279815664\n",
      "train loss:0.0005651984394664999\n",
      "train loss:0.0011613134836911986\n",
      "train loss:0.003327434098414154\n",
      "train loss:0.0007654043611298783\n",
      "train loss:0.00015868057250822967\n",
      "train loss:0.0007120286435264096\n",
      "train loss:0.0007255645396691082\n",
      "train loss:0.0002494924144902893\n",
      "train loss:0.00044320104358138937\n",
      "train loss:0.0020845562800390106\n",
      "train loss:8.559827038436243e-05\n",
      "train loss:0.0003442423319284519\n",
      "train loss:4.887496397364059e-05\n",
      "train loss:0.000268477105690818\n",
      "train loss:0.00014302809322131978\n",
      "train loss:0.00047127175251948086\n",
      "train loss:0.00023365345261499055\n",
      "train loss:0.00037548080751542874\n",
      "train loss:0.0003829742164920676\n",
      "train loss:0.0009265273704704416\n",
      "train loss:9.836286001045489e-05\n",
      "train loss:3.394989596791346e-05\n",
      "train loss:7.112580135528207e-05\n",
      "train loss:7.907827887927345e-05\n",
      "train loss:7.586813765523533e-05\n",
      "train loss:0.0003247264949975537\n",
      "train loss:0.0008095521480139438\n",
      "train loss:0.0003517693576919108\n",
      "train loss:0.0001134407437150623\n",
      "train loss:0.0023196661698339192\n",
      "train loss:0.00027167952850426746\n",
      "train loss:0.000209560383383857\n",
      "train loss:0.00030646770836484236\n",
      "train loss:0.0007106606209964986\n",
      "train loss:0.005589287038803555\n",
      "train loss:0.000257477303041763\n",
      "train loss:0.001902321387652351\n",
      "train loss:0.0005139891365203291\n",
      "train loss:0.0018310491854753735\n",
      "train loss:0.000855180706898784\n",
      "train loss:0.00012780616278690898\n",
      "train loss:0.0017409884058528088\n",
      "train loss:0.00014614807740957202\n",
      "train loss:8.186603668833583e-05\n",
      "train loss:0.0004890139011676269\n",
      "train loss:0.0013550599205703726\n",
      "train loss:0.00036068394317860996\n",
      "train loss:0.025250281449278482\n",
      "train loss:0.00021822152657089222\n",
      "train loss:0.0004471529679230852\n",
      "train loss:0.0014846881716170943\n",
      "train loss:9.521218128982061e-05\n",
      "train loss:0.000414793452073992\n",
      "train loss:0.0011055118285364471\n",
      "train loss:2.010044710635427e-05\n",
      "train loss:6.514919562625373e-05\n",
      "train loss:0.0022878329692511993\n",
      "train loss:0.0004239375668836691\n",
      "train loss:0.0006848003523855825\n",
      "train loss:0.0006536133859871597\n",
      "train loss:0.0005305475531704385\n",
      "train loss:0.0014875225038486445\n",
      "train loss:0.000807457250837408\n",
      "train loss:0.001261496511073345\n",
      "train loss:0.00039115809423194355\n",
      "train loss:0.0021440519017203916\n",
      "train loss:0.0014207824537702854\n",
      "train loss:0.001521365326786689\n",
      "train loss:0.0010196307589772613\n",
      "train loss:0.002373153605435683\n",
      "train loss:0.0012786978149319323\n",
      "train loss:1.8766322131242148e-05\n",
      "train loss:0.000144756973545839\n",
      "train loss:0.0018789014133481956\n",
      "train loss:0.0002046006629426223\n",
      "train loss:6.15708859349939e-05\n",
      "train loss:0.008265892934922935\n",
      "train loss:4.40608401519048e-05\n",
      "train loss:0.00639322716284914\n",
      "train loss:0.0007347463169901963\n",
      "train loss:0.005112954468433393\n",
      "train loss:0.0004584919963446253\n",
      "train loss:0.0013879359937561833\n",
      "train loss:2.549646910196973e-05\n",
      "train loss:0.0007174776503382998\n",
      "train loss:4.805308545545645e-05\n",
      "train loss:0.0038191727167063206\n",
      "train loss:0.0002174142057022045\n",
      "train loss:0.004763234949975366\n",
      "train loss:0.0023918384123707576\n",
      "train loss:0.0003199998085824588\n",
      "train loss:0.0028495112665174777\n",
      "train loss:0.00022451084761483222\n",
      "train loss:0.0008467091751951304\n",
      "train loss:0.00024434722596675904\n",
      "train loss:0.0004630535415922707\n",
      "train loss:0.0004730642123508367\n",
      "train loss:0.00045918693649762575\n",
      "train loss:0.0011064161231463079\n",
      "train loss:0.0009736983940973154\n",
      "train loss:0.0016134343702344908\n",
      "train loss:0.004380969104137557\n",
      "train loss:0.0007322538162100634\n",
      "train loss:0.00048798313848081876\n",
      "train loss:3.0142982185414702e-05\n",
      "train loss:0.0007792896033701843\n",
      "train loss:4.329017966982993e-05\n",
      "train loss:4.1246746057944865e-05\n",
      "train loss:0.0018544069565465066\n",
      "train loss:0.00025522422568797826\n",
      "train loss:0.0017524046784825203\n",
      "train loss:0.0006911291774965517\n",
      "train loss:6.804398435723674e-05\n",
      "train loss:0.0012880538667478436\n",
      "train loss:0.0003373272563323258\n",
      "train loss:0.002986435373456599\n",
      "train loss:0.0011441716877951388\n",
      "train loss:9.245223279305733e-05\n",
      "train loss:0.0011615892632146588\n",
      "train loss:0.0003046690524895625\n",
      "train loss:0.0012051780397298382\n",
      "train loss:0.006599842517050214\n",
      "train loss:0.0010909024938614768\n",
      "train loss:7.12068169256587e-05\n",
      "train loss:0.0009827349272672314\n",
      "train loss:0.0006720611833374816\n",
      "train loss:0.0006549250233111934\n",
      "train loss:0.0015757872741250102\n",
      "train loss:0.007561830332788788\n",
      "train loss:0.0010801455577960093\n",
      "train loss:4.549963617141431e-05\n",
      "train loss:0.00018149384486548448\n",
      "train loss:0.0012968939833116187\n",
      "train loss:0.0011484522617415372\n",
      "train loss:0.00045252738864603743\n",
      "train loss:0.001601868300160492\n",
      "train loss:0.001038239267352182\n",
      "train loss:0.0011879883444427582\n",
      "train loss:0.008075948473488663\n",
      "train loss:0.0018787813382079247\n",
      "train loss:0.004168664682343386\n",
      "train loss:0.0014569200599290077\n",
      "train loss:0.0008002312906799968\n",
      "train loss:0.000104877168957654\n",
      "train loss:0.0020860013202099194\n",
      "train loss:0.002345938354715758\n",
      "train loss:0.0003987557184633023\n",
      "train loss:0.007869084695068933\n",
      "train loss:0.00027280131915325307\n",
      "train loss:0.0007689116115763542\n",
      "train loss:8.265127246227983e-05\n",
      "train loss:0.00014031742042396384\n",
      "train loss:0.00017220682656757212\n",
      "train loss:0.00028679348058969735\n",
      "train loss:0.001249338031457262\n",
      "train loss:0.0007088800792877918\n",
      "train loss:0.009818806127514868\n",
      "train loss:0.01328352170438228\n",
      "train loss:9.179674375531954e-05\n",
      "train loss:0.0005591777999192817\n",
      "train loss:0.0028149095886188867\n",
      "train loss:0.008281637593395019\n",
      "train loss:0.0024873930397702106\n",
      "train loss:0.0005947929901621714\n",
      "train loss:0.0003054567318043391\n",
      "train loss:0.00046890643256908076\n",
      "train loss:0.0008753597133971036\n",
      "train loss:0.0006635887570060627\n",
      "train loss:0.00023993923480892783\n",
      "train loss:0.00021830204449998895\n",
      "train loss:0.0005543539420901919\n",
      "train loss:0.0018272878057229555\n",
      "train loss:8.217826032400314e-05\n",
      "train loss:0.0008900142230996017\n",
      "train loss:0.00010639277702853904\n",
      "train loss:0.0006748667800393674\n",
      "train loss:0.0002719299558173613\n",
      "train loss:0.0029102223313446866\n",
      "train loss:0.0008608679801727537\n",
      "train loss:0.00017009376296492137\n",
      "train loss:0.0022348538978026545\n",
      "train loss:0.004412076682070982\n",
      "train loss:0.00259593628719631\n",
      "train loss:7.390178405354246e-05\n",
      "train loss:0.0042304519641808295\n",
      "train loss:0.00030970005306917085\n",
      "train loss:0.000610508453683363\n",
      "train loss:0.00018189895726354398\n",
      "train loss:0.0006553717857760413\n",
      "train loss:0.0003120321980106413\n",
      "train loss:1.824725406472809e-05\n",
      "train loss:0.00013433831226143452\n",
      "train loss:0.0001509332990784119\n",
      "train loss:7.875054393891198e-05\n",
      "train loss:0.00022040644294297462\n",
      "train loss:5.8204083848020805e-05\n",
      "train loss:0.0007028771928359566\n",
      "train loss:0.0013262053786040164\n",
      "train loss:0.00024591231668392755\n",
      "train loss:0.003203498999175214\n",
      "train loss:0.001376952936931533\n",
      "train loss:0.00019983415323480646\n",
      "train loss:0.0011951732652154832\n",
      "train loss:0.00026552853301888924\n",
      "train loss:0.00012510642738500287\n",
      "train loss:0.00025876828716631556\n",
      "train loss:0.0014058208679138561\n",
      "train loss:0.0002522839968593588\n",
      "train loss:2.5890505172248327e-05\n",
      "train loss:0.0009127858690989209\n",
      "train loss:0.00038840593251633827\n",
      "train loss:0.000355803358916351\n",
      "train loss:0.00034206265501904206\n",
      "train loss:0.00022386074426354718\n",
      "train loss:0.0013751804316978395\n",
      "train loss:0.012572510648851532\n",
      "train loss:0.00030284444705413273\n",
      "train loss:5.05360101790809e-05\n",
      "train loss:0.00021826723367405075\n",
      "train loss:0.0010524476756016334\n",
      "train loss:0.0010122956791317574\n",
      "train loss:0.0009867915168814225\n",
      "train loss:0.0011753097678495871\n",
      "train loss:0.00021173054272802534\n",
      "train loss:0.0008780475504570424\n",
      "train loss:0.0006612168070507275\n",
      "train loss:4.774348132949763e-05\n",
      "train loss:0.00018264492717615766\n",
      "train loss:0.0008261670461788268\n",
      "train loss:0.00032415407677824636\n",
      "train loss:0.0011877378783643861\n",
      "train loss:0.0006523079473194063\n",
      "train loss:3.094821098741647e-05\n",
      "train loss:0.0001340362811034328\n",
      "train loss:0.0016552830964576264\n",
      "train loss:0.011420584599284517\n",
      "train loss:0.00025030715225145847\n",
      "train loss:0.0010874485826993265\n",
      "train loss:0.002044657150952579\n",
      "train loss:0.0004049967961213055\n",
      "train loss:0.0027162533826870184\n",
      "train loss:0.00020595155426884648\n",
      "train loss:0.006661433035830935\n",
      "train loss:4.697137611783222e-05\n",
      "train loss:0.0019912126584582133\n",
      "train loss:0.01187059292682382\n",
      "train loss:0.0033782952802667294\n",
      "train loss:0.0002509973510708389\n",
      "train loss:5.008077194918051e-05\n",
      "train loss:0.00017342652461521834\n",
      "train loss:0.001843234109935854\n",
      "train loss:0.002019909204698785\n",
      "train loss:0.0013614611090746957\n",
      "train loss:0.0026929880615927052\n",
      "train loss:0.004561078528313788\n",
      "train loss:0.00013301504109239134\n",
      "train loss:0.0001165227896192439\n",
      "train loss:0.0016226854644116457\n",
      "train loss:0.0025288422348798\n",
      "train loss:0.001821231395107874\n",
      "train loss:0.0002448206431770087\n",
      "train loss:0.0009399444064307364\n",
      "train loss:1.065595335709887e-05\n",
      "train loss:0.0010068271338979757\n",
      "train loss:0.002014867818089844\n",
      "train loss:0.00022679822749128688\n",
      "train loss:0.002316847400654514\n",
      "train loss:0.0012915791061913473\n",
      "train loss:0.0004066225319700131\n",
      "train loss:0.0009924394624029114\n",
      "train loss:0.0012471092310322002\n",
      "train loss:0.0002633352103456693\n",
      "train loss:0.0005052746500362707\n",
      "train loss:0.00014768668770420089\n",
      "train loss:0.003411732929959928\n",
      "train loss:0.00040983630566349756\n",
      "train loss:0.0001274764473358235\n",
      "train loss:0.0012031059091875895\n",
      "train loss:0.0002180293885654846\n",
      "train loss:0.0011249832785541413\n",
      "train loss:6.825793646170634e-05\n",
      "train loss:0.0008830566868663704\n",
      "train loss:0.0009346754664883306\n",
      "train loss:0.0026797395872965478\n",
      "train loss:8.016827965188082e-05\n",
      "train loss:0.0005911547992211118\n",
      "train loss:0.001779543181210655\n",
      "train loss:0.008987414970290895\n",
      "train loss:0.0008159693208202311\n",
      "train loss:0.00013024885789560895\n",
      "train loss:0.002884564196595892\n",
      "train loss:0.0015193932660128194\n",
      "train loss:0.003228973323285912\n",
      "train loss:0.001850541911233902\n",
      "train loss:0.0014581716250417758\n",
      "train loss:0.0008172935383907614\n",
      "train loss:4.704190993984512e-05\n",
      "train loss:0.003318547095393377\n",
      "train loss:0.0010493432198460193\n",
      "train loss:0.0007401060086292377\n",
      "train loss:0.0008738132309862886\n",
      "train loss:0.0008526773887293146\n",
      "train loss:0.002872130256278142\n",
      "train loss:0.00012059989248077631\n",
      "train loss:0.00027302612739394056\n",
      "train loss:0.00024891923914136546\n",
      "train loss:0.0003731669929227393\n",
      "train loss:0.0016773514026448251\n",
      "train loss:0.0005339664488924363\n",
      "train loss:0.0002488468883255787\n",
      "train loss:0.001353839721365537\n",
      "train loss:0.0006436144240303147\n",
      "train loss:0.00032109684844862493\n",
      "train loss:0.02623041969009764\n",
      "train loss:0.0026289187744908777\n",
      "train loss:0.00012789501346429676\n",
      "train loss:8.147620538620033e-05\n",
      "train loss:0.001076538934190827\n",
      "train loss:0.0017226894999089364\n",
      "train loss:0.003556741483816868\n",
      "train loss:0.0012657424448842534\n",
      "train loss:0.0023455343601771006\n",
      "train loss:0.0029393718094691207\n",
      "train loss:0.0007457169408246323\n",
      "train loss:0.0013645851670706286\n",
      "train loss:0.0011486527164705624\n",
      "train loss:0.0019972990053547693\n",
      "train loss:0.00047073719569585545\n",
      "train loss:0.037318835891212376\n",
      "train loss:0.001205932847166903\n",
      "train loss:0.00010261270038360818\n",
      "train loss:6.88649837435302e-05\n",
      "train loss:0.0075220129584817495\n",
      "train loss:0.0005498793461748222\n",
      "train loss:0.002003666390591449\n",
      "train loss:0.0012609216525904402\n",
      "train loss:0.0014176058232219367\n",
      "train loss:7.156448889393705e-05\n",
      "train loss:0.0009334157818331878\n",
      "train loss:0.0036280902529317936\n",
      "train loss:0.00021884555859023\n",
      "train loss:0.0010901904630258258\n",
      "train loss:8.189266605862847e-05\n",
      "train loss:0.0004967267105974934\n",
      "train loss:3.2180220800025446e-05\n",
      "train loss:4.121832346649384e-05\n",
      "train loss:5.6146279597592204e-05\n",
      "train loss:0.00017849850996996984\n",
      "train loss:0.0031213694691847095\n",
      "train loss:0.0021427327107265476\n",
      "train loss:0.0023824093344378947\n",
      "train loss:0.0004376265448914014\n",
      "train loss:0.0002615919559213093\n",
      "train loss:0.00021865110679033048\n",
      "train loss:0.002649084881674601\n",
      "train loss:0.0009659986428236298\n",
      "train loss:0.0005936237446470577\n",
      "train loss:0.00021025558920498253\n",
      "train loss:0.002010216302895308\n",
      "train loss:0.0006305273272740247\n",
      "train loss:0.00032723183522790077\n",
      "train loss:0.00015308649191155858\n",
      "train loss:0.0017627476426676544\n",
      "train loss:0.00043385932683336217\n",
      "train loss:0.0006869609700516795\n",
      "train loss:0.000574812949426318\n",
      "train loss:0.0001038268285452279\n",
      "train loss:0.0004951482716932587\n",
      "train loss:5.203137653876855e-05\n",
      "train loss:0.0052705063892731045\n",
      "train loss:0.0011301653293721548\n",
      "train loss:0.00016956591732764542\n",
      "train loss:0.00022386135348999786\n",
      "train loss:0.0007562577205490938\n",
      "train loss:0.0023368016023256042\n",
      "train loss:0.012835552095476209\n",
      "train loss:0.006102523581233465\n",
      "train loss:0.001819930744352135\n",
      "train loss:0.0003220116913064528\n",
      "train loss:0.0017835690338175202\n",
      "train loss:0.0038040941770882473\n",
      "train loss:0.001265934642499883\n",
      "train loss:0.007471954873932176\n",
      "train loss:0.0016349614899730505\n",
      "train loss:0.002702346561356207\n",
      "train loss:0.0005532609018248748\n",
      "train loss:0.001217413370543986\n",
      "train loss:0.0014827581887636773\n",
      "train loss:0.0013790382995270065\n",
      "train loss:0.0005039117764823147\n",
      "train loss:0.002300418624865283\n",
      "train loss:0.003862818685600459\n",
      "train loss:1.4730901986046774e-05\n",
      "train loss:0.00015691732171832992\n",
      "train loss:0.016784504305261107\n",
      "train loss:0.0002669165874710963\n",
      "train loss:7.522011106464374e-05\n",
      "train loss:0.00017527374998368238\n",
      "train loss:0.001223648720292856\n",
      "train loss:0.0001205763000931759\n",
      "train loss:0.0006582005475220469\n",
      "train loss:0.0023111955578513105\n",
      "train loss:0.00036999132407227934\n",
      "train loss:0.0028371960810309226\n",
      "train loss:0.0020471846680675575\n",
      "train loss:0.00010525967863433359\n",
      "train loss:0.0014310709280190318\n",
      "train loss:0.0014715058801204092\n",
      "train loss:0.0006982951208594491\n",
      "train loss:0.0005629493957034764\n",
      "train loss:0.0010278936785994323\n",
      "train loss:0.001330124633383181\n",
      "train loss:0.0009448291192271682\n",
      "train loss:0.0015211121243471676\n",
      "train loss:0.00020091346949527842\n",
      "train loss:0.0003883529499245024\n",
      "train loss:0.00028347934684012926\n",
      "train loss:4.894066442558435e-05\n",
      "train loss:0.00029783164499436036\n",
      "train loss:0.0010291641068298437\n",
      "train loss:0.0008593172122076509\n",
      "train loss:0.0011767466677410155\n",
      "train loss:0.0003345660051077338\n",
      "train loss:0.00032093195200088133\n",
      "train loss:0.01294896874412651\n",
      "train loss:0.001509246139874813\n",
      "train loss:0.0001007292687310529\n",
      "train loss:0.0002311990501244545\n",
      "train loss:4.446960992544402e-06\n",
      "train loss:7.446665686241392e-05\n",
      "train loss:0.004258514524354674\n",
      "train loss:0.0022353673935360265\n",
      "train loss:0.0004775283555148524\n",
      "train loss:0.0001857302275734072\n",
      "train loss:0.00441384257894463\n",
      "train loss:0.002507946253231994\n",
      "train loss:0.0006102748876293037\n",
      "train loss:8.589518395815133e-05\n",
      "train loss:0.0001977966844102369\n",
      "train loss:0.001421448227469844\n",
      "train loss:0.00012276552415030676\n",
      "train loss:0.0015395601579314196\n",
      "train loss:0.006126733297878559\n",
      "train loss:0.006609303952611116\n",
      "train loss:0.0021369783104839522\n",
      "train loss:0.0010880461264734596\n",
      "train loss:0.000979003902086824\n",
      "train loss:4.892807671466026e-05\n",
      "train loss:4.697570631652e-05\n",
      "train loss:0.0011268451422318864\n",
      "train loss:0.00021253513565718367\n",
      "train loss:0.0022210002202971164\n",
      "train loss:0.006040535142862278\n",
      "train loss:5.897040085445598e-05\n",
      "train loss:0.001675653777784577\n",
      "train loss:0.0023590443531161524\n",
      "train loss:3.2651528737991415e-05\n",
      "train loss:0.0005191140939654308\n",
      "train loss:0.0024704339007391137\n",
      "train loss:0.0002295050067090785\n",
      "train loss:0.001965921944132648\n",
      "train loss:0.0004716044079837301\n",
      "train loss:0.001023640004512299\n",
      "train loss:9.743945533448806e-05\n",
      "train loss:0.00011274392505913314\n",
      "train loss:0.0003289568951129846\n",
      "train loss:0.0008531529767131166\n",
      "train loss:0.00016831651696301323\n",
      "train loss:0.0011549793902820463\n",
      "train loss:0.00010930442068796624\n",
      "train loss:0.00019043708513844087\n",
      "train loss:0.004942083650352322\n",
      "train loss:0.0021566150170580006\n",
      "train loss:0.0009470488881622787\n",
      "train loss:0.0004946909460751905\n",
      "train loss:4.3267119010407904e-05\n",
      "train loss:0.0004693669229609972\n",
      "train loss:0.0004220269472846358\n",
      "train loss:0.0006580316811931089\n",
      "train loss:0.00025138879332881856\n",
      "train loss:0.0004304915857381775\n",
      "train loss:0.00030874957197049834\n",
      "train loss:0.000258809956620931\n",
      "train loss:3.518672322909673e-05\n",
      "train loss:0.00011574893275743304\n",
      "train loss:9.783556266737903e-05\n",
      "train loss:0.0029999801268185463\n",
      "train loss:0.0003688777855076073\n",
      "train loss:0.002271117623566802\n",
      "train loss:0.0003111818066969752\n",
      "train loss:0.0017326235136702873\n",
      "train loss:0.00010567043333731992\n",
      "train loss:0.00326945267392496\n",
      "train loss:0.001184444396448278\n",
      "train loss:0.0005158465779450997\n",
      "train loss:0.00047861482495617996\n",
      "train loss:0.0026200033883723418\n",
      "train loss:0.0003097808750001152\n",
      "train loss:0.002988188328251895\n",
      "train loss:0.00015041211968442542\n",
      "train loss:7.404126312989399e-05\n",
      "train loss:7.117912040107286e-05\n",
      "train loss:8.00771209157691e-05\n",
      "train loss:0.009619428251027295\n",
      "train loss:0.0004136033590476631\n",
      "train loss:0.0003669746473972136\n",
      "train loss:0.00026381986377332754\n",
      "train loss:0.003385703396477087\n",
      "train loss:4.1330100851691316e-05\n",
      "train loss:0.0029520852807745977\n",
      "train loss:0.0007687089542212569\n",
      "train loss:0.0005062601197498225\n",
      "train loss:0.002165308852737587\n",
      "train loss:0.004563871591678917\n",
      "train loss:0.00014492860671323597\n",
      "train loss:0.00067542538405464\n",
      "train loss:0.00013620617833989487\n",
      "train loss:0.0005555410785268406\n",
      "train loss:1.9749013592324833e-05\n",
      "train loss:6.0410447668776566e-05\n",
      "train loss:0.0002452581619069183\n",
      "train loss:0.0002815768837140627\n",
      "train loss:0.00014995515876022996\n",
      "train loss:3.043382067748308e-05\n",
      "train loss:7.680511490891407e-05\n",
      "train loss:0.0006705694060579151\n",
      "train loss:0.00038433512622421513\n",
      "train loss:0.00032443797236292814\n",
      "train loss:4.0211549845315856e-05\n",
      "train loss:0.0005002092442811387\n",
      "train loss:0.0005324127776341979\n",
      "train loss:9.392477316052662e-06\n",
      "train loss:6.392427740374209e-05\n",
      "train loss:0.00013159304883489815\n",
      "train loss:0.0005204669891402599\n",
      "train loss:0.000517817270326267\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9889\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CNN, MNIST 학습\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DLS_git_clone.dataset.mnist import load_mnist\n",
    "from DLS_git_clone.ch07.simple_convnet import SimpleConvNet\n",
    "from DLS_git_clone.common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs, mini_batch_size=100, optimizer='Adam', optimizer_param={'lr':0.001}, evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "network.save_params('params.pkl')\n",
    "print('Saved Network Parameters!')\n",
    "\n",
    "markers = {'train':'o', 'test':'s'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **7.6** CNN 시각화하기\n",
    "> \n",
    "> CNN을 구성하는 Conv layer는 입력으로 받은 이미지 데이터에서 무엇을 보는 것일까?  \n",
    "> \n",
    "> 이를 알아보기 위해 Conv layer를 시각화  \n",
    ">>  \n",
    ">> ##### 1번째 층의 가중치 시각화\n",
    ">>  \n",
    ">> MNIST로 간단한 CNN 학습을 해보았는데, 1번째 층의 Conv layer의 가중치는 shape이 (30, 1, 5, 5)였음 (필터 30개, 채널 1개, $5 \\times 5$크기)  \n",
    ">> \n",
    ">> 필터의 크기가 $5 \\times 5$이고 채널이 1개라는 것은 이 필터를 1채널의 회색조 이미지로 시각화할 수 있다는 뜻  \n",
    ">> \n",
    ">> 1층의 Conv layer 필터를 이미지로 나타낼 예정, 여기에서는 학습 전과 후의 가중치를 비교해볼 텐데, 그 결과는 그림 7-24처럼 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Conv layer filter 시각화\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DLS_git_clone.ch07.simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN/nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "network = SimpleConvNet()\n",
    "\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ![pic/filter_visualization](pic/filter_visualization.png)  \n",
    ">> \n",
    ">> 위 그림과 같이 학습 전 필터는 무작위로 초기화되고 있어 흑백의 정도에 규칙성이 없음  \n",
    ">> \n",
    ">> 한편, 학습을 마친 필터는 규칙성 있는 이미지가 되었음  \n",
    ">> \n",
    ">> 흰색에서 검은색으로 점차 변화하는 필터와 덩어리(blob)가 진 필터 등, 규칙을 띄는 필터로 바뀜  \n",
    ">> \n",
    ">> 그림 7-24의 오른쪽같이 규칙성 있는 필터는 무엇을 보고 있는 것일까?  \n",
    ">> \n",
    ">> 그것은 edge(색상이 바뀐 경계선)와 blob(국소적으로 덩어리진 영역) 등을 보고 있음  \n",
    ">> &nbsp; &nbsp; &rarr; 왼쪽 절반이 흰색이고 오른쪽 절반이 검은색인 필터는 그림 7-25와 같이 세로 방향의 edge에 반응하는 필터  \n",
    ">> \n",
    ">> ![pic/response_filter](pic/response_filter.png)  \n",
    ">> \n",
    ">> 위 그림은 학습된 필터 2개를 선택하여 입력 이미지에 합성곱 처리를 한 결과로, '필터 1'은 세로 edge에 반응하며 '필터 2'은 가로 edge에 반응하는 것을 알 수 있음  \n",
    ">> \n",
    ">> 이처럼 Conv layer의 필터는 edge나 blob등의 원시적인 정보를 추출할 수 있음  \n",
    ">> \n",
    ">> 이러한 원시적인 정보가 뒷단 layer에 전달된다는 것이 앞에서 구현한 CNN에서 일어나는 일  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### 층 깊이에 따른 추출 정보 변화\n",
    ">> 앞의 결과는 1번째 층의 Conv layer를 대상으로 한 것  \n",
    ">> \n",
    ">> 1번째 층의 Conv layer에서는 edge나 blob등의 저수준 정보가 추출된다 치고, 그럼 겹겹이 쌓인 CNN의 각 layer에서는 어떤 정보가 추출될까?  \n",
    ">> \n",
    ">> 딥러닝 시각화에 관한 연구에 따르면, layer가 깊어질수록 추출되는 정보 (정확히는 강하게 반응하는 뉴런)는 더 추상화된다는 것을 알 수 있음  \n",
    ">> \n",
    ">> 그림 7-26은 일반 사물 인식(자동차나 개 등)을 수행한 8층의 CNN  \n",
    ">> \n",
    ">> 이 네트워크 구조는 AlexNet이라 하는데, Conv layer와 Pooling layer를 여러겹 쌓고, 마지막으로 fully-connected layer를 거쳐 결과를 출력하는 구조  \n",
    ">> \n",
    ">> 그림 7-26에서 블록으로 나타낸 것은 중간 데이터이며, 그 중간 데이터에 Conv 연산을 연속해서 적용함  \n",
    ">> \n",
    ">> ![pic/alexnet_8layer](pic/alexnet_8layer.png)  \n",
    ">> \n",
    ">> 딥러닝의 흥미로운 점은 위 그림과 같이 Conv layer를 여러겹 쌓으면, 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다는 것  \n",
    ">> \n",
    ">> 처음 층은 단순한 edge에 반응하고, 이어서 texture에 반응하고, 더 복잡한 사물의 일부에 반응하도록 변화함  \n",
    ">> \n",
    ">> 즉, layer가 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 '고급'정보로 변화해 감  \n",
    ">> &nbsp; &nbsp; &rarr; 사물의 '의미'를 이해하도록 변화하는 것  \n",
    "\n",
    "> ### **7.7** 대표적인 CNN\n",
    "> 지금까지 제안된 CNN 네트워크의 구성은 다양함  \n",
    "> \n",
    "> 그중에서도 특히 중요한 네트워크 2개를 소개  \n",
    "> * LeNet (CNN의 원조)  \n",
    "> * AlexNet (딥러닝이 주목받도록 이끈 네트워크)  \n",
    ">> \n",
    ">> ##### LeNet  \n",
    ">> LeNet은 손글씨 숫자를 인식하는 네트워크로 1998년에 제안됨  \n",
    ">> \n",
    ">> 그림 7-27과 같이 Conv layer와 pooling layer(정확히는 단순히 '원소를 줄이기'만 하는 subsampling layer)을 반복하고, 마지막으로 fully-connected layer를 거치면서 결과를 출력  \n",
    ">> \n",
    ">> ![pic/lenet_structure](pic/lenet_structure.png)  \n",
    ">> \n",
    ">> LeNet과 '현재의 CNN'을 비교하면 여러 부분에서 차이가 있음  \n",
    ">> * activation function \n",
    ">> &nbsp; &nbsp; &nbsp; LeNet은 sigmoid function을 사용하는 데 반해, 현재는 주로 ReLU를 사용함  \n",
    ">> * LeNet은 subsampling을 하여 중간 데이터의 크기를 줄이지만, 현재는 max pooling이 주류  \n",
    ">> \n",
    ">> 이처럼 LeNet과 현재의 CNN은 얼마간 차이가 있지만 큰 차이는 아님  \n",
    ">> \n",
    ">> ##### AlexNet\n",
    ">> 2012년에 발표된 AlexNet은 딥러닝 열풍을 일으키는 데 큰 역할을 함  \n",
    ">> \n",
    ">> 그림 7-28에서 보듯 그 구성은 기본적으로 LeNet과 크게 다르지 않음  \n",
    ">> \n",
    ">> ![pic/alexnet_structure](pic/alexnet_structure.png)  \n",
    ">> \n",
    ">> AlexNet은 Conv layer와 Pooling layer를 거듭하여 마지막으로 fully-connected layer를 거쳐 결과를 출력함  \n",
    ">> \n",
    ">> LeNet에서 큰 구조는 바뀌지 않았지만, AlexNet에서는 다음과 같은 변화가 있음  \n",
    ">> * activation function으로 ReLU를 이용  \n",
    ">> * LRN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층을 이용  \n",
    ">> * dropout을 사용  \n",
    ">> \n",
    ">> 이상에서 보듯 네트워크 구성 면에서는 LeNet과 AlexNet에 큰 차이는 없음  \n",
    ">> \n",
    ">> 빅데이터와 GPU, 이것이 딥러닝 발전의 큰 원동력  \n",
    ">> \n",
    ">> 딥러닝(심층 신경망)에는 대부분 수많은 매개변수가 쓰임  \n",
    ">> \n",
    ">> 그래서 학습하려면 엄청난 양의 계산을 해야 함  \n",
    ">> \n",
    ">> 또한 그 매개변수를 fitting(적합)시키는 데이터도 대량으로 필요함  \n",
    ">> \n",
    ">> 빅데이터와 GPU가 이런 문제에 해결책을 제시했다고 할 수 있음  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
