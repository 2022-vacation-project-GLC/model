{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: 학습 관련 기술들\n",
    ">  \n",
    "> ### ** 6.1** 매개변수 갱신  \n",
    "> * 신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것  \n",
    "> &nbsp; &nbsp; &rarr; 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것을 최적화라고 함\n",
    "> * 지금까지 최적의 매개변수 값을 찾는 단서로 매개변수의 기울기(미분)을 이용함  \n",
    "> 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 반복해서 점점 최적의 값에 다가감  \n",
    "> &nbsp; &nbsp; &rarr; 확률적 경사 하강법 (SGD)\n",
    "> * SGD는 매개변수 공간을 무작정 찾는 것보다 '똑똑한' 방법이지만, 문제에 따라서는 SGD보다 똑똑한 방법도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### 모험가 이야기\n",
    ">> ![adventure](adventure.png)\n",
    ">> * 위와 같은 어려운 상황에서 중요한 단서가 되는 것이 땅의 '기울기'\n",
    ">> * 모험가는 주위 경치는 볼 수 없지만 지금 서 있는 땅의 기울기는 알 수 있음\n",
    ">> * 지금 서 있는 장소에서 가장 크게 기울어진 방향으로 가는 것이 SGD의 전략  \n",
    ">> 이를 반복하면 언젠가 '깊은 곳'에 찾아갈 수 있을 것\n",
    ">>  \n",
    ">> ##### 확률적 경사 하강법 (SGD)\n",
    ">> SGD 복습  \n",
    ">> * $ W \\larr W- \\eta \\frac{\\sigma L}{\\sigma W} $  \n",
    ">>      * $W$: 갱신할 가중치 매개변수  \n",
    ">>      * $\\frac{\\sigma L}{\\sigma W}$: $W$에 대한 손실함수의 기울기  \n",
    ">>      * $\\eta$: 학습률 (0.01이나 0.001과 같은 값을 미리 정해서 사용)  \n",
    ">>      * $\\larr$: 우변의 값으로 좌변의 값을 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD 구현\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr*grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> * 초기화 때 받는 인수 lr: learning rate(학습률)  \n",
    ">> 이 학습률을 인스턴스 변수로 유지  \n",
    ">> * `update(params, grads)` 메서드: SGD 과정에서 반복해서 호출\n",
    ">>      * `params, grads`: 딕셔너리 변수  \n",
    ">> `params['W1'], grads['W1']` 등과 같이 각각 가중치 매개변수와 기울기를 저장하고 있음\n",
    ">>  \n",
    ">> SGD 클래스를 사용하면 신경망 매개변수의 진행을 다음과 같이 수행할 수 있음  \n",
    ">> ```\n",
    ">> network = TwoLayerNet(...)  \n",
    ">> optimizer = SGD()  \n",
    ">>  \n",
    ">> for i in range(10000):  \n",
    ">> ...  \n",
    ">> x_batch, t_batch = get_mini_batch(...) # 미니배치  \n",
    ">> grads = network.gradient(x_batch, t_batch)  \n",
    ">> params = network.params  \n",
    ">> optimizer.update(params, grads)  \n",
    ">> ...\n",
    ">> ```\n",
    ">> * optimizer: 최적화를 행하는 자, 매개변수 갱신을 수행\n",
    ">>  \n",
    ">> * 이처럼 최적화를 담당하는 클래스를 분리해 구현하면 기능을 모듈화하기 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### SGD의 단점\n",
    ">> * 문제에 따라서 비효율적인 경우가 있음  \n",
    ">> ![disad_sgd](disad_sgd.png)\n",
    ">> ![gradient_disad](gradient_disad.png)\n",
    ">>  \n",
    ">> SGD 적용\n",
    ">> * 탐색 시작 (초깃값) $(x,y) = (-7.0, 2.0)$  \n",
    ">> ![disad_application_of_sgd](disad_application_of_sgd.png)\n",
    ">>  \n",
    ">> * SGD는 그림 6-3과 같은 심하게 굽이진 움직임을 보여줌  \n",
    ">> &nbsp; &nbsp; &rarr; 상당히 비효율적인 움직임  \n",
    ">> &nbsp; &nbsp; &rarr; SGD의 단점은 비등방성 함수(anisotropy function)에서는 탐색 경로가 비효율적이라는 것  \n",
    ">> &nbsp; &nbsp; &nbsp; &nbsp; (anisotropy function: 방향에 따라 성질, 즉 여기에서는 기울기가 달라지는 함수)\n",
    ">>  \n",
    ">> * SGD가 지그재그로 탐색하는 근본 원인은 기울어진 방향이 본래의 최솟값과 다른 방향을 가리키기 때문\n",
    ">> * SGD의 이러한 단점을 개선해주는 기법: Momentum, AdaGrad, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### Momentum\n",
    ">> * Momentum: 운동량, 물리와 관계가 있음  \n",
    ">>  \n",
    ">> * $ v \\larr \\alpha v - \\eta \\frac{\\sigma L}{\\sigma W} $ &nbsp; &nbsp; &nbsp; [식 6.3]\n",
    ">> * $ W \\larr W+v $ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [식 6.4]\n",
    ">>      * $W$: 갱신할 가중치 매개변수  \n",
    ">>      * $\\frac{\\sigma L}{\\sigma W}$: $W$에 대한 손실 함수의 기울기\n",
    ">>      * $\\eta$: 학습률\n",
    ">>      * $v$: 물리에서 말하는 속도\n",
    ">> * 식 6.3은 기울기 방향으로 힘을 받아 물체가 가속되는 물리 법칙을 나타냄  \n",
    ">> * 식 6.3의 $\\alpha v$ 항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 함  \n",
    ">> 물리에서의 마찰, 공기 저항에 해당  \n",
    ">> * 모멘텀은 공이 그릇의 바닥을 구르는 듯한 움직임을 보여줌  \n",
    ">> ![momentum_image](momentum_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Momentum 구현\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key]-self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> * 인스턴스 변수 v: 물체의 속도, 초기화 때는 아무 값도 담지 않음, update()가 처음 호출될 때 매개변수와 같은 구조의 데이터를 딕셔너리 변수로 저장\n",
    ">>  \n",
    ">> * 나머지 부분은 식 6.3과 식 6.4를 코드로 옮긴 것\n",
    ">>  \n",
    ">> ![optimize_momentum](optimize_momentum.png)\n",
    ">>  \n",
    ">> * 그림에서 보듯 모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직임\n",
    ">>  \n",
    ">> * SGD와 비교하면 '지그재그 정도'가 덜함\n",
    ">> * 이는 x축의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하기 때문\n",
    ">> * y축의 힘은 크지만 위아래로 번갈아 받아서 상충하여 y축 방향의 속도는 안정적이지 않음\n",
    ">> * 전체적으로는 SGD보다 x축 방향으로 빠르게 다가가 지그재그 움직임이 줄어듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### AdaGrad\n",
    ">> 신경망 학습에서는 학습률 (수식에서는 $\\eta$로 표기) 값이 중요  \n",
    ">> \n",
    ">> 이 값이 너무 작으면 학습 시간이 너무 길어지고, 너무 크면 발산하여 학습이 제대로 이루어지지 않음  \n",
    ">> \n",
    ">> 학습률을 정하는 효과적 기술로 학습률 감소(learning rate decay)가 있음\n",
    ">> * 학습을 진행하면서 학습률을 점차 줄여가는 방법\n",
    ">> \n",
    ">> * 처음에는 크게 학습하다가 조금씩 작게 학습\n",
    ">> * 실제 신경망 학습에 자주 쓰임\n",
    ">> * 학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 것   \n",
    ">> &nbsp; &nbsp; &rarr; 이를 더욱 발전시킨 것이 AdaGrad  \n",
    ">> &nbsp; &nbsp; &rarr; AdaGrad는 '각각의' 매개변수에 '맞춤형' 값을 만들어줌\n",
    ">>  \n",
    ">> AdaGrad는 개별 매개변수에 적응적으로(adaptive) 학습률을 조정하면서 학습을 진행\n",
    ">>  \n",
    ">> * $ h \\larr h + \\frac{\\sigma L}{\\sigma W} \\odot \\frac{\\sigma L}{\\sigma W} $ &nbsp; &nbsp; &nbsp; &nbsp; [식 6.5]\n",
    ">> * $ W \\larr W - \\eta \\frac{1}{\\sqrt{h}} \\frac{\\sigma L}{\\sigma W} $ &nbsp; &nbsp; &nbsp; &nbsp; [식 6.6 ]\n",
    ">>      * $W$: 갱신할 가중치 매개변수\n",
    ">>      * $\\frac{\\sigma L}{\\sigma W}$: $W$에 대한 손실 함수의 기울기\n",
    ">>      * $\\eta$: 학습률\n",
    ">>      * $h$: 기존 기울기 값을 제곱하여 계속 더해줌\n",
    ">>      * $\\odot$: 행렬의 원소별 곱셈\n",
    ">>      * 매개변수를 갱신할 때 $\\frac{1}{\\sqrt{h}}$ 을 곱해 학습률을 조정\n",
    ">>      &nbsp; &nbsp; &rarr; 매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아짐\n",
    ">>      &nbsp; &nbsp; &rarr; 학습률 감소가 매개변수의 원소마다 다르게 적용됨\n",
    ">>  \n",
    "\n",
    ">> AdaGrad는 과거의 기울기를 제곱하여 계속 더해감  \n",
    ">>  \n",
    ">> &nbsp; &nbsp; &rarr; 학습을 진행할수록 갱신 강도가 약해짐  \n",
    ">> \n",
    ">> 실제로 무한히 계속 학습한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게 됨  \n",
    ">> \n",
    ">> 이러한 문제를 개선한 기법으로 RMSProp이 있음  \n",
    ">> \n",
    ">> RMSProp은 과거의 모든 기울기를 균일하게 더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영  \n",
    ">> &nbsp; &nbsp; &rarr; 이를 지수이동평균 (Exponential Moving Average, EMA)라 하여, 과거 기울기의 반영 규모를 기하급수적으로 감소시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad 구현\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key]*grads[key]\n",
    "            params[key] -= self.lr*grads[key]/np.sqrt(self.h[key]+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> 위 구현에서 주의할 것은 `1e-7`이라는 작은 값을 더하는 부분\n",
    ">> \n",
    ">> 이 작은 값은 `self.h[key]`에 0이 담겨 있다 해도 0으로 나누는 사태를 막아줌 (zero divission error 예방)\n",
    ">>\n",
    ">> 대부분의 딥러닝 프레임 워크에서는 이 값도 인수로 설정할 수 있음\n",
    ">>  \n",
    ">> ![optimize_adagrad](optimize_adagrad.png)\n",
    ">> \n",
    ">> 위 그림을 보면 최솟값을 향해 효율적으로 움직이는 것을 알 수 있음\n",
    ">> \n",
    ">> y축 방향은 기울기가 커서 처음에는 크게 움직이지만, 그 큰 움직임에 비례해 갱신 정도도 큰 폭으로 작아지도록 조정\n",
    ">> &nbsp; &nbsp; &rarr; y축 방향으로 갱신 강도가 빠르게 약해지고, 지그재그 움직임이 줄어듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### Adam\n",
    ">> 모멘텀은 공이 그릇 바닥을 구르는 듯한 움직임을 보였음\n",
    ">>\n",
    ">> AdaGrad는 매개변수의 원소마다 적응적으로 갱신 정도를 조정했음\n",
    ">> \n",
    ">> &nbsp; &nbsp; &rarr; 이 두 기법을 융합하면 어떻게 될까?\n",
    ">> &nbsp; &nbsp; &rarr; Adam\n",
    ">> \n",
    ">> Adam\n",
    ">> * Momentum과 AdaGrad를 융합한 듯한 기법\n",
    ">>  \n",
    ">> * 하이퍼파라미터의 '편향 보정'이 진행됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam 구현\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ![disad_sgd](disad_sgd.png)\n",
    ">> \n",
    ">> Adam 갱신 과정도 그릇 바닥을 구르듯 움직임  \n",
    ">> \n",
    ">> 모멘텀과 비슷한 패턴, 모멘텀 때보다 공의 좌우 흔들림이 적음  \n",
    ">> &nbsp; &nbsp; &rarr; 학습의 갱신 강도를 적응적으로 조정해서 얻는 혜택\n",
    "\n",
    ">> Adam은 하이퍼파라미터를 3개 설정\n",
    ">> * 학습률 (`lr`)\n",
    ">> * 일차 모멘텀용 계수 (`beta1`)\n",
    ">> * 이차 모멘텀용 계수 (`beta1`)\n",
    ">> * 이정도의 값이면 많은 경우에 좋은 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### 어느 갱신 방법을 이용할 것인가\n",
    ">>  \n",
    ">> ![comparison_optimizer](comparison_optimizer.png)\n",
    ">>  \n",
    ">> optimizer 선택은 풀어야 할 문제가 무엇이냐에 따라 달라지므로 주의해야 함\n",
    ">> \n",
    ">> 당연하지만 (학습률 등의) 하이퍼파라미터를 어떻게 설정하느냐에 따라서도 결과가 바뀜\n",
    ">> \n",
    ">> 모든 문제에서 항상 뛰어난 기법은 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### MNIST 데이터셋으로 본 갱신 방법 비교\n",
    ">> \n",
    ">> ![comparison_mnist](comparison_mnist.png)\n",
    ">>  \n",
    ">> * 각 층이 100개의 뉴런으로 구성된 5층 신경망  \n",
    ">> * 활성화 함수: ReLU\n",
    ">> * 하이퍼파라미터인 학습률과 신경망의 구조(층 깊이 등)에 따라 결과가 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **6.2** 가중치의 초깃값\n",
    "> 신경망 학습에서 특히 중요한 것이 가중치의 초깃값  \n",
    "> 가중치의 초깃값을 무엇으로 설정하느냐가 신경망 학습의 성패를 가르기도 함\n",
    ">>  \n",
    ">> ##### 초깃값을 0으로 하면?\n",
    ">> 가중치 감소(weight decay): over-fitting을 억제해 범용 성능을 높이는 기법\n",
    ">> * 가중치 매개변수의 값이 작아지도록 학습하는 방법\n",
    ">> * 가중치 값을 작게 하여 over-fitting이 일어나지 않게 함\n",
    ">> * 가중치 값을 작게 하려면 초깃값도 최대한 작은 값에서 시작하는 것이 정공법\n",
    ">> * 지금까지 가중치의 초깃값은 `0.01*np.random.randn(10,100)`처럼 정규분포에서 생성되는 값을 0.01배 한 작은  값(표준편차가 0.01인 정규분포)을 사용함\n",
    ">>  \n",
    ">> 그렇다면 가중치 초깃값을 모두 0으로 설정하면 어떨까?  \n",
    ">> &nbsp; &nbsp; &rarr; 좋지 않음  \n",
    ">> &nbsp; &nbsp; &rarr; 가중치 초깃값을 0으로 하면 학습이 올바로 이루어지지 않음  \n",
    ">>  \n",
    ">> 초깃값을 모두 0으로 설정하면 안되는 이유 (정확히는 가중치를 균일한 값으로 설정하면 안되는 이유)  \n",
    ">> * 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문\n",
    ">> * 2층 신경망에서 첫 번째와 두 번째 층의 가중치가 0이라고 가정  \n",
    ">> &nbsp; &nbsp; &rarr; 순전파 때는 입력층의 가중치가 0이기 때문에 두 번째 층의 뉴런에 모두 같은 값이 전달  \n",
    ">> &nbsp; &nbsp; &rarr; 두 번째 층의 뉴런에 같은 값이 입력된다는 것은 역전파 때 두 번째 층의 가중치가 모두 똑같이 갱신된다는 것  \n",
    ">> &nbsp; &nbsp; &rarr; 가중치들은 같은 초깃값에서 시작하고 갱신을 거쳐도 여전히 같은 값을 유지  \n",
    ">> &nbsp; &nbsp; &rarr; 이는 가중치를 여러 개 갖는 의미를 사라지게 함 \n",
    ">>  \n",
    ">> 이렇게 가중치가 고르게 되는 상황을 막으려면 (정확히는 가중치의 대칭적인 구조를 무너뜨리려면) 초깃값을 무작위로 설정해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### 은닉층의 활성화값 분포\n",
    ">>  \n",
    ">> 은닉층의 활성화값(활성화 함수의 출력 데이터, 활성화값)의 분포를 관찰하면 중요한 정보를 얻을 수 있음  \n",
    ">>  \n",
    ">> 가중치의 초깃값에 따라 은닉층의 활성화값들이 어떻게 변화하는지 알아볼 예정\n",
    ">>  \n",
    ">> 구체적으로는 활성화 함수로 Sigmoid function을 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그릴 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVS0lEQVR4nO3dfZBldX3n8ffHGUbYIIJhMktmKIfEiWY0K8o4zJaJa8TAgNnAVikFqzKxiLMbIKtbu7WOqa3FRd3Vqs1iqCDJRCYMGoUpNTILmNkplE2ZEpgmGB4kSIeHnZnw0Do8GQQCfveP+2tzd/r29O3pnr798H5V3epzfud3zv2d7+2+n3seujtVhSRJLxv0ACRJs4OBIEkCDARJUmMgSJIAA0GS1BgIkiTAQPiJJA8leeegxzGbWJPerMtYSSrJawY9jtlkLtZkXgdCkouSDCV5PslVgx7PoCV5eZIrkzyc5Jkk30ly+qDHNRsk+UKSR5I8neR7SX5r0GOaLZKsSvJcki8MeiyDluTmVosftsd9gx7TdJrXgQD8HfAJYMugB9JLksUz/JSLgd3AvwBeCfxnYFuSlTM8jnENoCaj/juwsqqOAn4D+ESSkwY0ljEGWBeAy4FdA3z+npIsGtBTX1RVR7bHawc0hp6mWpN5HQhV9dWq+hrwg8msl2Rtkm8nebJ9avyDJEvassuT/N5+/bcn+fdt+meTfCXJSJIHk/y7rn4fS/Ll9mn0aeA3p7qPk1FVf19VH6uqh6rqx1V1PfAgMOEb33ytyaiquqeqnh+dbY+fn2i9+V6XJOcATwI3TWKddyW5ox1t7U7ysa5lNyT5nf3635nkX7Xp1yXZmWRfkvuSnN3V76okVyS5McnfA786xd2bMXOmJlU17x90jhKumqDPQ8A72/RJwDo6n6hXAvcCH27L1tI58nhZmz8WeBZYRidgbwf+C7AE+DngAeC01vdjwD8AZ7W+Rwy4LsuA54DXWZMC+GwbdwF/BRy5kOsCHAV8D1jRxvOFA/Qt4DVt+u3AL7Vx/zPgMeCstuxs4Nau9d5I5wPbEuCn6BzBfqDV803A94HVre9VwFPAW9u2Dx9ATW4GRtq4/hJ4+3yqybw+QjhYVXV7Vd1SVS9W1UPAH9E5zUJV3UbnBTildT8HuLmqHgPeAiytqkuq6oWqegD449Zn1Ler6mvV+YT+o5nap/0lOQz4U2BrVf3NRP0XQk2q6gLgFcCvAF8Fnj/wGvO+Lh8HrqyqPZNZqapurqq72rjvBL5EqwmwHfiFJKva/PuBa6vqBeDXgYeq6k9aPe8AvgK8p2vz11XVX7ZtPzeVnTtIH6ET3suBzcD/SjLhkeRcqcmCDIQkX++6KPTeHst/Icn1SR5th+v/jc6nu1Fbgfe16fcBn2/TrwZ+tp0+eDLJk8Dv0vlEOGr3dO/PZCV5GZ0xvwBc1NoWdE1GVdVLVfUtOp+Kf3uh1iXJicA7gUt7LLunqya/0mP5yUm+2U6FPQX8W1pN2hvWtcD72vfhufz/NTl5v5q8F/inXZsf6PdKVd1aVc9U1fNVtZXOUcIZ86Umg7xQNTBVNdGdNVcAdwDnVtUzST4MvLtr+ReAu5O8EfhF4GutfTfwYFWtYnwD/fOySQJcSeeN54yq+gdY2DUZx2Lg5xdwXd5O5xTY/+18y3AksCjJ6qp6/QTrfhH4A+D0qnouyWcYG5KfB74FPFtV327tu4H/U1W/doBtz7bvlQIyX2oyr48QkixOcjiwiM438+Hp726NVwBPAz9M8jrgt7sXtkPoXXRewK90Hc7fBjyT5CNJjkiyKMkbkrxl2nZq6q6g88b0Lyd5GmLe1iTJzyQ5J8mRbXyn0fmU1s+F1Plal810Lqqf2B5/CNwAnNbHuq8A9rU3vrXAv+5e2N7sfgz8Hv/4SRjgejqnTt6f5LD2eEuSX5zqzkyHJEcnOW30faQdMb4N+PM+Vp8TNZnXgUDntsofAZvoHK7/qLVN5D/SecGeoXNe99oefbbSuUj0kxevql6ic87vRDp373wf+BydWzwHLsmrgX9DZ3yPHuhUSA/zsiZN0Xkj3wM8AfwPOheGt/ex7rysS1U9W1WPjj6AHwLPVdVIH6tfAFyS5Bk6F8239ehzNZ2a/OR3G6rqGeBUOtdR/g54FPg08PIp7cz0OYzODSqjF5V/h86F4e/1se6cqEmqZtsR2NyQ5G10XrhXl0UErMl4rMtYSc4DNlbVLw96LLPFbKjJfD9COCTSuUPnQ8Dn/AHvsCa9WZexkvwTOp+YNw96LLPFbKmJgTBJ7dzdk8BxwGcGOphZwpr0Zl3GatdnRujch//FAQ9nVphNNfGUkSQJ8AhBktTM2d9DOPbYY2vlypWDHsYhdfvtt3+/qpb2238h1AQmVxdrMpY16W0h1GWimszZQFi5ciVDQ0ODHsYhleThyfRfCDWBydXFmoxlTXpbCHWZqCaeMpIkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQB8yAQVm66gZWbbhj0MGYVa9KbNRnL75WxFnJN+gqEJA8luSvJd5IMtbZXJdmZ5P729ZjWniSXJRlOcmeSN3dtZ0Prf3+SDV3tJ7XtD7d1M907Kkk6sMkcIfxqVZ1YVWva/CbgpvZPwm9q8wCnA6vaYyOd/+FLklcBFwMnA2uBi0dDpPX5YNd66w96jyRJB2Uqp4zOpPO/Ymlfz+pqv7o6bgGOTnIcnX/OvbOq9lXVE8BOYH1bdlRV3dL+o9TVXduSJM2QfgOhgP+d5PYkG1vbsqp6pE0/Cixr08uB3V3r7mltB2rf06N9jCQbkwwlGRoZ6ed/fUuS+tXvn7/+5aram+RngJ1J/qZ7YVVVkkP+r9eqajPtf46uWbPGf/UmSdOoryOEqtrbvj4O/BmdawCPtdM9tK+Pt+57geO7Vl/R2g7UvqJHuyRpBk0YCEl+KskrRqeBU4G7ge3A6J1CG4Dr2vR24Lx2t9E64Kl2amkHcGqSY9rF5FOBHW3Z00nWtbuLzuvaliRphvRzymgZ8GftTtDFwBer6s+T7AK2JTkfeBg4u/W/ETgDGAaeBT4AUFX7knwc2NX6XVJV+9r0BcBVwBHA19tDkjSDJgyEqnoAeGOP9h8Ap/RoL+DCcba1BdjSo30IeEMf45UkHSJz/jeVJUnTw0CQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBEwiEJIsSnJHkuvb/AlJbk0ynOTaJEta+8vb/HBbvrJrGx9t7fclOa2rfX1rG06yaRr3T5LUp8kcIXwIuLdr/tPApVX1GuAJ4PzWfj7wRGu/tPUjyWrgHOD1wHrgsy1kFgGXA6cDq4FzW19J0gzqKxCSrADeBXyuzQd4B/Dl1mUrcFabPrPN05af0vqfCVxTVc9X1YPAMLC2PYar6oGqegG4pvWVJM2gfo8QPgP8J+DHbf6ngSer6sU2vwdY3qaXA7sB2vKnWv+ftO+3znjtYyTZmGQoydDIyEifQ5ck9WPCQEjy68DjVXX7DIzngKpqc1Wtqao1S5cuHfRwJGleWdxHn7cCv5HkDOBw4Cjg94GjkyxuRwErgL2t/17geGBPksXAK4EfdLWP6l5nvHZJ0gyZ8Aihqj5aVSuqaiWdi8LfqKr3At8E3t26bQCua9Pb2zxt+Teqqlr7Oe0upBOAVcBtwC5gVbtraUl7ju3TsneSpL71c4Qwno8A1yT5BHAHcGVrvxL4fJJhYB+dN3iq6p4k24DvAi8CF1bVSwBJLgJ2AIuALVV1zxTGJUk6CJMKhKq6Gbi5TT9A5w6h/fs8B7xnnPU/CXyyR/uNwI2TGYskaXr5m8qSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSgD4CIcnhSW5L8tdJ7knyX1v7CUluTTKc5NokS1r7y9v8cFu+smtbH23t9yU5rat9fWsbTrLpEOynJGkC/RwhPA+8o6reCJwIrE+yDvg0cGlVvQZ4Aji/9T8feKK1X9r6kWQ1cA7wemA98Nkki5IsAi4HTgdWA+e2vpKkGTRhIFTHD9vsYe1RwDuAL7f2rcBZbfrMNk9bfkqStPZrqur5qnoQGAbWtsdwVT1QVS8A17S+kqQZ1Nc1hPZJ/jvA48BO4G+BJ6vqxdZlD7C8TS8HdgO05U8BP93dvt8647X3GsfGJENJhkZGRvoZuiSpT30FQlW9VFUnAivofKJ/3aEc1AHGsbmq1lTVmqVLlw5iCJI0b03qLqOqehL4JvDPgaOTLG6LVgB72/Re4HiAtvyVwA+62/dbZ7x2SdIM6ucuo6VJjm7TRwC/BtxLJxje3bptAK5r09vbPG35N6qqWvs57S6kE4BVwG3ALmBVu2tpCZ0Lz9unYd8kSZOweOIuHAdsbXcDvQzYVlXXJ/kucE2STwB3AFe2/lcCn08yDOyj8wZPVd2TZBvwXeBF4MKqegkgyUXADmARsKWq7pm2PZQk9WXCQKiqO4E39Wh/gM71hP3bnwPeM862Pgl8skf7jcCNfYxXknSI+JvKkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUTBgISY5P8s0k301yT5IPtfZXJdmZ5P729ZjWniSXJRlOcmeSN3dta0Prf3+SDV3tJyW5q61zWZIcip2VJI2vnyOEF4H/UFWrgXXAhUlWA5uAm6pqFXBTmwc4HVjVHhuBK6ATIMDFwMnAWuDi0RBpfT7Ytd76qe+aJGkyJgyEqnqkqv6qTT8D3AssB84EtrZuW4Gz2vSZwNXVcQtwdJLjgNOAnVW1r6qeAHYC69uyo6rqlqoq4OqubUmSZsikriEkWQm8CbgVWFZVj7RFjwLL2vRyYHfXanta24Ha9/Ro7/X8G5MMJRkaGRmZzNAlSRPoOxCSHAl8BfhwVT3dvax9sq9pHtsYVbW5qtZU1ZqlS5ce6qeTpAWlr0BIchidMPjTqvpqa36sne6hfX28te8Fju9afUVrO1D7ih7tkqQZ1M9dRgGuBO6tqv/ZtWg7MHqn0Abguq7289rdRuuAp9qppR3AqUmOaReTTwV2tGVPJ1nXnuu8rm1JkmbI4j76vBV4P3BXku+0tt8FPgVsS3I+8DBwdlt2I3AGMAw8C3wAoKr2Jfk4sKv1u6Sq9rXpC4CrgCOAr7eHJGkGTRgIVfUtYLzfCzilR/8CLhxnW1uALT3ah4A3TDQWSdKh428qS5IAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQL6CIQkW5I8nuTurrZXJdmZ5P729ZjWniSXJRlOcmeSN3ets6H1vz/Jhq72k5Lc1da5LEmmeyclSRPr5wjhKmD9fm2bgJuqahVwU5sHOB1Y1R4bgSugEyDAxcDJwFrg4tEQaX0+2LXe/s8lSZoBEwZCVf0FsG+/5jOBrW16K3BWV/vV1XELcHSS44DTgJ1Vta+qngB2AuvbsqOq6paqKuDqrm1JkmbQwV5DWFZVj7TpR4FlbXo5sLur357WdqD2PT3ae0qyMclQkqGRkZGDHLokqZcpX1Run+xrGsbSz3Ntrqo1VbVm6dKlM/GUkrRgHGwgPNZO99C+Pt7a9wLHd/Vb0doO1L6iR7skaYYdbCBsB0bvFNoAXNfVfl6722gd8FQ7tbQDODXJMe1i8qnAjrbs6STr2t1F53VtS5I0gxZP1CHJl4C3A8cm2UPnbqFPAduSnA88DJzdut8InAEMA88CHwCoqn1JPg7sav0uqarRC9UX0LmT6Qjg6+0hSZphEwZCVZ07zqJTevQt4MJxtrMF2NKjfQh4w0TjkCQdWv6msiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1Cwe9AA0c1ZuuoGHPvWuQQ9j1lq56QaAnjU60DLNT6Ov+Wx0qL4P500gzNYXb7wXbibenHvVZDbVaXT/Z/LNtp/9P1CfmarfTAfPTH9fGKxTM9nXq996z5pASLIe+H1gEfC5qvrUgIc0LQ7mzWWh/LDsv/8HqtVCqcmofn7g53JNDlUAzeWazAaz4hpCkkXA5cDpwGrg3CSrBzsqSVpYZkUgAGuB4ap6oKpeAK4BzhzwmCRpQUlVDXoMJHk3sL6qfqvNvx84uaou2q/fRmBjm30tcB9wLPD9GRzuTBjdp1dX1dJ+V0oyAjzM/K4JTKIuXTXZfxvzgTUZ66BqAgvm5+eANZk11xD6UVWbgc3dbUmGqmrNgIZ0SBzsPo2+0NbkH3V/88+3uliTsaayP/78zJ5TRnuB47vmV7Q2SdIMmS2BsAtYleSEJEuAc4DtAx6TJC0os+KUUVW9mOQiYAed2063VNU9fa6+eeIuc85U98maHLptzCbWZCxr0ltf+zQrLipLkgZvtpwykiQNmIEgSQLmeCAkWZ/kviTDSTYNejxTlWRLkseT3D2FbViTsduwJr23M2/qYk16m3RdqmpOPuhcfP5b4OeAJcBfA6sHPa4p7tPbgDcDd1sTa3KoajIf62JNpqcuc/kIYd79uYuq+gtg3xQ2YU3Gsia9zau6WJPeJluXuRwIy4HdXfN7WttCZk3Gsia9WZexFnxN5nIgSJKm0VwOBP/cxVjWZCxr0pt1GWvB12QuB4J/7mIsazKWNenNuoy14GsyZwOhql4ERv/cxb3Atur/z13MSkm+BHwbeG2SPUnOn8z61mQsa9LbfKuLNeltsnXxT1dIkoA5fIQgSZpeBoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktT8P2ZCocnSo20FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 구현\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.random.randn(1000, 100)\n",
    "node_num = 100\n",
    "hidden_layer_size = 5\n",
    "activations = {}\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "    \n",
    "    # W = np.random.randn(node_num, node_num)*1\n",
    "    # W = np.random.randn(node_num, node_num)*0.01\n",
    "    # W = np.random.randn(node_num, node_num)*np.sqrt(1.0/node_num)\n",
    "    W = np.random.randn(node_num, node_num)*np.sqrt(2.0/node_num)\n",
    "\n",
    "    a = np.dot(x, W)\n",
    "    #z = sigmoid(a)\n",
    "    z = ReLU(a)\n",
    "    activations[i] = z\n",
    "\n",
    "for i,a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1)+\"-layer\")\n",
    "\n",
    "    if i != 0:\n",
    "        plt.yticks([], [])\n",
    "        \n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ![standard_deviation_1](standard_deviation_1.png)\n",
    ">>  \n",
    ">> * 층: 5개\n",
    ">> * 각 층의 뉴런: 100개\n",
    ">> * 입력 데이터: 1000개의 무작위 정규분포 데이터\n",
    ">> * 활성화 함수: Sigmoid function\n",
    ">> * 가중치 분포에 주의해야 함\n",
    ">> * 위 구현에서는 표준편차가 1인 정규분포를 이용함, 이 분포된 정도(표준편차)를 바꿔가며 활성화값들의 분포가 어떻게 변화하는지 관찰하는 것이 이 실험의 목적\n",
    ">> * 각 층의 활성화값들이 0과 1에 치우쳐 분포\n",
    ">> * 위 구현에서 사용한 Sigmoid function은 그 출력이 0에 가까워지면 그 미분은 0에 다가감  \n",
    ">> &nbsp; &nbsp; &rarr; 데이터가 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라짐  \n",
    ">> &nbsp; &nbsp; &rarr; 기울기 소실(gradient vanishing) 문제, 층을 깊게 하는 딥러닝에서는 기울기 소실은 더 심각한 문제가 될 수 있음\n",
    ">>  \n",
    "\n",
    ">> ![standard_deviation_0.01](standard_deviation_0.01.png)\n",
    ">> * 표준편차를 0.01로 한 정규분포의 경우 각 층의 활성화값 분포\n",
    ">> * 0.5 부근에 집중되어 있음  \n",
    ">> &nbsp; &nbsp; &rarr; 0과 1로 치우치지는 않아 기울기 소실 문제는 일어나지 않지만, 활성화값들이 치우쳤다는 것은 표현력 관점에서는 큰 문제가 있음  \n",
    ">> &nbsp; &nbsp; &rarr; 이 상황에서는 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의미가 없어짐\n",
    "\n",
    ">> 따라서 활성화값들이 치우치면 표현력을 제한한다는 관점에서 문제가 됨  \n",
    ">> 각 층의 활성화값들은 적당히 고루 분포되어야 함  \n",
    ">> 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 진행되기 때문  \n",
    ">> 반대로 치우친 데이터가 흐르면 기울기 소실이나 표현력 제한 문제에 빠져 학습이 잘 이루어지지 않음\n",
    "\n",
    ">> Xavier 초깃값\n",
    ">> * Xavier Glorot와 Yoshua Bengio의 논문에서 권장하는 가중치 초깃값\n",
    ">> * 일반적인 딥러닝 프레임워크들이 표준적으로 이용\n",
    ">> * 논문은 각 층의 활성화값들을 광범위하게 분포시킬 목적으로 가중치의 적절한 분포를 찾고자 함\n",
    ">> * 앞 계층의 노드가 $n$ 개라면 표준편차가 $\\frac{1}{\\sqrt{n}}$ 인 분포를 사용하면 된다는 결론을 이끌어 냄\n",
    ">>  \n",
    ">> ![xavier](/pic/xavier.png)\n",
    ">>   \n",
    ">> Xavier 초깃값을 사용하면 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 펴짐  \n",
    ">>  \n",
    ">> ![standard_deviation_xavier](pic/standard_deviation_xavier.png)\n",
    ">>  \n",
    ">> * 층이 깊어지면서 형태가 다소 일그러지지만, 앞에서 본 방식보다는 확실히 넓게 분포됨\n",
    ">> * 각 층에 흐르는 데이터는 적당히 퍼져 있으므로, Sigmoid function의 표현력도 제한받지 않고 학습이 효율적으로 이루어질 것 \n",
    ">> * 오른쪽으로 갈수록 일그러지는 것은 sigmoid function 대신 tanh function을 이용하면 개선됨\n",
    ">> * tanh function도 sigmoid function과 같은 'S'자 모양 곡선 함수\n",
    ">> * tanh function은 원점(0, 0)에서 대칭인 S곡선인 반면, sigmoid function은 (x,y) = (0, 0.5)에서 대칭인 S곡선\n",
    ">> * 활성화 함수용으로는 원점에서 대칭인 함수가 바람직하다고 알려져 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### ReLU를 사용할 때의 가중치 초깃값\n",
    ">> Xavier 초깃값은 활성화 함수가 선형인 것을 전제로 이끈 결과  \n",
    ">> Sigmoid function과 tanh function 함수는 좌우 대칭이라 중앙 부근이 선형인 함수로 볼 수 있음  \n",
    ">> &nbsp; &nbsp; &rarr; Xavier 초깃값이 적당함  \n",
    ">>  \n",
    ">> 반면 ReLU를 이용할 때는 ReLU에 특화된 초깃값을 이용하라고 권장  \n",
    ">> 이 특화된 초깃값을 찾아낸 Kaiming He의 이름을 따 He 초깃값이라 함\n",
    ">>\n",
    ">> He 초깃값\n",
    ">> * 앞 계층의 노드가 $n$ 개일 때, $\\sqrt{\\frac{2}{n}}$ 인 정규분포를 사용\n",
    ">> * Xaiver 초깃값은 $\\sqrt{\\frac{2}{n}}$  \n",
    ">> &nbsp; &nbsp; &rarr; ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 (직감적으로) 해석할 수 있음\n",
    ">>  \n",
    ">> ![activations_spread_relu](pic/activations_spread_relu.png)\n",
    ">> \n",
    ">> 표준편차 = 0.01\n",
    ">> * 각 층의 활성화값들은 아주 작은 값들\n",
    ">> * 신경망에 아주 작은 데이터가 흐른다는 것은 역전파 때 가중치의 기울기 역시 작아진다는 뜻\n",
    ">> &nbsp; &nbsp; &rarr; 이는 중대한 문제이며, 실제로도 학습이 거의 이뤄지지 않을 것\n",
    ">>  \n",
    ">> Xavier 초깃값\n",
    ">> * 층이 깊어지면서 치우침이 조금씩 커짐\n",
    ">> * 실제로 층이 깊어지면 활성화값들의 치우침도 커지고, 학습할 때 '기울기 소실' 문제를 일으킴\n",
    ">>  \n",
    ">> He 초깃값\n",
    ">> * 모든 층에서 균일하게 분포\n",
    ">> * 층이 깊어져도 분포가 균일하게 유지\n",
    ">> &nbsp; &nbsp; &rarr; 역전파 때도 적절한 값이 나올 것으로 기대\n",
    "\n",
    ">> 이상의 실험 결과를 바탕으로\n",
    ">> * 활성화 함수: ReLU &rarr; He 초깃값\n",
    ">>  \n",
    ">> * 활성화 함수: Sigmoid, tanh (S자 모양 곡선) &rarr; Xavier 초깃값\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ##### MNIST 데이터셋으로 본 가중치 초깃값 비교\n",
    ">> 실제 데이터를 가지고 가중치의 초깃값을 주는 방법이 신경망 학습에 얼마나 영향을 주는지 알아볼 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========iteration:0===========\n",
      "std=0.01:2.3024906187624805\n",
      "Xavier:2.320218334259964\n",
      "He:2.467704672824771\n",
      "===========iteration:100===========\n",
      "std=0.01:2.302407607924717\n",
      "Xavier:2.2693165464693013\n",
      "He:1.6659291668038774\n",
      "===========iteration:200===========\n",
      "std=0.01:2.299268871180098\n",
      "Xavier:2.159679372363867\n",
      "He:0.8323954915906572\n",
      "===========iteration:300===========\n",
      "std=0.01:2.3027872971932233\n",
      "Xavier:1.9232582139518815\n",
      "He:0.5989196121360577\n",
      "===========iteration:400===========\n",
      "std=0.01:2.2995279093215624\n",
      "Xavier:1.4276477205260458\n",
      "He:0.47989956054138677\n",
      "===========iteration:500===========\n",
      "std=0.01:2.3028288613831824\n",
      "Xavier:0.9761506033763876\n",
      "He:0.3337547618581814\n",
      "===========iteration:600===========\n",
      "std=0.01:2.2999969447321544\n",
      "Xavier:0.7853394580098316\n",
      "He:0.31972473635706944\n",
      "===========iteration:700===========\n",
      "std=0.01:2.3044278206739097\n",
      "Xavier:0.592332132318945\n",
      "He:0.2646844760132683\n",
      "===========iteration:800===========\n",
      "std=0.01:2.3023929527358264\n",
      "Xavier:0.4979003745233773\n",
      "He:0.2798816103540105\n",
      "===========iteration:900===========\n",
      "std=0.01:2.3013175896502176\n",
      "Xavier:0.45521546419383696\n",
      "He:0.25493300556264514\n",
      "===========iteration:1000===========\n",
      "std=0.01:2.3022560578062974\n",
      "Xavier:0.4858697806742655\n",
      "He:0.3078981382767091\n",
      "===========iteration:1100===========\n",
      "std=0.01:2.3047357667034993\n",
      "Xavier:0.46517648046414306\n",
      "He:0.36454417783662624\n",
      "===========iteration:1200===========\n",
      "std=0.01:2.295450087610799\n",
      "Xavier:0.38639836510084885\n",
      "He:0.26405387589823204\n",
      "===========iteration:1300===========\n",
      "std=0.01:2.3001163900194714\n",
      "Xavier:0.3307150317271274\n",
      "He:0.22236578580087135\n",
      "===========iteration:1400===========\n",
      "std=0.01:2.2952446849291994\n",
      "Xavier:0.3269474648605637\n",
      "He:0.19079047451259531\n",
      "===========iteration:1500===========\n",
      "std=0.01:2.3011279994534655\n",
      "Xavier:0.21439319251794003\n",
      "He:0.1410045251978393\n",
      "===========iteration:1600===========\n",
      "std=0.01:2.3049828659933778\n",
      "Xavier:0.31059790276502786\n",
      "He:0.23208733067206375\n",
      "===========iteration:1700===========\n",
      "std=0.01:2.3025112456678554\n",
      "Xavier:0.27426322444170714\n",
      "He:0.23032245560748693\n",
      "===========iteration:1800===========\n",
      "std=0.01:2.2985687590127473\n",
      "Xavier:0.2338455998162073\n",
      "He:0.14597071969379116\n",
      "===========iteration:1900===========\n",
      "std=0.01:2.296319299222109\n",
      "Xavier:0.24274426070734134\n",
      "He:0.18371180724928945\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 활성화 함수: ReLU\n",
    "# 0.01, Xavier, He\n",
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "# 0. MNIST 데이터 읽기==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "\n",
    "# 1. 실험용 설정==========\n",
    "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key, weight_type in weight_init_types.items():\n",
    "    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "                                  output_size=10, weight_init_std=weight_type)\n",
    "    train_loss[key] = []\n",
    "\n",
    "\n",
    "# 2. 훈련 시작==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in weight_init_types.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizer.update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in weight_init_types.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "\n",
    "# 3. 그래프 그리기==========\n",
    "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
    "x = np.arange(max_iterations)\n",
    "for key in weight_init_types.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> * 활성화 함수: ReLU\n",
    ">>  \n",
    ">> * `std=0.01`일 때는 학습이 전혀 이뤄지지 않음  \n",
    ">> &nbsp; &nbsp; &rarr; 활성화 값의 분포에서 본 것처럼 순전파 때 너무 작은 값(0 근처로 밀집한 데이터)이 흐르기 때문  \n",
    ">> &nbsp; &nbsp; &rarr; 역전파 때의 기울기도 작아져 가중치가 거의 갱신되지 않음  \n",
    ">> * Xavier와 He의 경우는 학습이 순조롭게 이뤄지고 있음\n",
    ">> &nbsp; &nbsp; &rarr; 학습 진도는 He가 더 빠름\n",
    "\n",
    ">> 기중치의 초깃값은 신경망 학습에 아주 중요한 포인트  \n",
    ">>  \n",
    ">> 가중치의 초깃값에 따라 신경망 학습의 성패가 갈리는 경우가 많음  \n",
    ">> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **6.3** 배치 정규화\n",
    ">>  \n",
    ">> 지금까지 각 층의 활성화값 분포를 관찰해보며, 가중치의 초깃값을 적절히 설정하면 각 층의 활성화값 분포가 적당히 퍼지면서 학습이 원활하게 수행됨을 배움\n",
    ">>  \n",
    ">> 그렇다면 각 층이 활성화를 적당히 퍼뜨리도록 '강제'하면 어떨까?  \n",
    ">> &nbsp; &nbsp; &rarr; 배치 정규화(Batch Normalization)의 아이디어"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
